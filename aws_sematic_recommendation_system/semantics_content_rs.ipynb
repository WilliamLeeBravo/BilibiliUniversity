{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "therapeutic-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rural-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')['data']\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')['data']\n",
    "\n",
    "NUM_TOPICS = 30\n",
    "NUM_NEIGHBORS = 10\n",
    "BUCKET = 'sagemaker-xl'\n",
    "PREFIX = '20newsgroups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smoking-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocss the data\n",
    "for i in range(len(newsgroups_train)):\n",
    "    newsgroups_train[i] = strip_newsgroup_header(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_quoting(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_footer(newsgroups_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "digital-stuart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mathematical-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "taken-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fitting-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "turned-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(t) >= 2 and re.match(\"[a-z].*\", t)\n",
    "                and re.match(token_pattern, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "later-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "damaged-malpractice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and counting, this may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 2000\n",
    "\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "\n",
    "vectors = vectorizer.fit_transform(newsgroups_train)\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "\n",
    "print('vocab size:', len(vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "designed-talent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time elapsed: 99.45s\n"
     ]
    }
   ],
   "source": [
    "# random shuffle\n",
    "idx = np.arange(vectors.shape[0])\n",
    "newidx = np.random.permutation(idx)  # the label fed into the KNN model\n",
    "\n",
    "# store the permutations\n",
    "vectors = vectors[newidx]\n",
    "\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stuck-sight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "# convert vectors to a sparse representation\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "understanding-rental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9051, 2000) (2263, 2000)\n"
     ]
    }
   ],
   "source": [
    "n_train = int(0.8*vectors.shape[0])\n",
    "\n",
    "train_vectors = vectors[:n_train, :]\n",
    "val_vectors = vectors[n_train:, :]\n",
    "\n",
    "print(train_vectors.shape,val_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tutorial-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "scientific-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set location s3://sagemaker-xl/20newsgroups/train\n",
      "Validation set location s3://sagemaker-xl/20newsgroups/val\n",
      "Trained model will be saved at s3://sagemaker-xl/20newsgroups/output\n"
     ]
    }
   ],
   "source": [
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "\n",
    "print('Training set location', s3_train_data)\n",
    "print('Validation set location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "gentle-career",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data to s3://sagemaker-xl/20newsgroups/train/train_part0.pbr\n",
      "Uploaded data to s3://sagemaker-xl/20newsgroups/train/train_part1.pbr\n",
      "Uploaded data to s3://sagemaker-xl/20newsgroups/train/train_part2.pbr\n",
      "Uploaded data to s3://sagemaker-xl/20newsgroups/train/train_part3.pbr\n",
      "Uploaded data to s3://sagemaker-xl/20newsgroups/train/train_part4.pbr\n",
      "Uploaded data to s3://sagemaker-xl/20newsgroups/train/train_part5.pbr\n",
      "Uploaded data to s3://sagemaker-xl/20newsgroups/train/train_part6.pbr\n",
      "Uploaded data to s3://sagemaker-xl/20newsgroups/train/train_part7.pbr\n",
      "Uploaded data to s3://sagemaker-xl/20newsgroups/val/val_part0.pbr\n"
     ]
    }
   ],
   "source": [
    "# convert raw vectors into RecordIO format\n",
    "# and use the n_parts parameter\n",
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=2):\n",
    "    import io\n",
    "    import boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "    \n",
    "    chunk_size = sparray.shape[0] // n_parts\n",
    "    \n",
    "    for i in range(n_parts):\n",
    "        # calculate start and end indices\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size\n",
    "        \n",
    "        if i + 1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "        \n",
    "        # convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # upload to amazon s3\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        \n",
    "        print('Uploaded data to s3://{}'.format(os.path.join(bucket, fname)))\n",
    "\n",
    "split_convert_upload(train_vectors, bucket=bucket, prefix=train_prefix, fname_template='train_part{}.pbr', n_parts=8)\n",
    "split_convert_upload(val_vectors, bucket=bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "amazing-medicaid",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "latin-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(container, role, train_instance_count=2,\n",
    "                                   train_instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_path, sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "everyday-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm.set_hyperparameters(num_topics=NUM_TOPICS, feature_dim=vocab_size, mini_batch_size=128, \n",
    "                        epochs=100, num_patience_epochs=5, tolerance=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sufficient-lucas",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-15 06:42:25 Starting - Starting the training job...\n",
      "2021-02-15 06:42:47 Starting - Launching requested ML instancesProfilerReport-1613371344: InProgress\n",
      "......\n",
      "2021-02-15 06:43:53 Starting - Preparing the instances for training.........\n",
      "2021-02-15 06:45:19 Downloading - Downloading input data...\n",
      "2021-02-15 06:45:50 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:47 INFO 140391001810112] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:47 INFO 140391001810112] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'30', u'epochs': u'100', u'feature_dim': u'2000', u'mini_batch_size': u'128', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:47 INFO 140391001810112] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'2000', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'100', u'mini_batch_size': u'128', u'num_topics': u'30', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:47 INFO 140391001810112] nvidia-smi took: 0.0252029895782 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/05462b6a-1f57-41b2-917c-0dd72d3b6599', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/05462b6a-1f57-41b2-917c-0dd72d3b6599', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2021-02-15-06-42-24-836', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-246-193.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/dda2181e-24f2-437c-bb48-b99fd40b3e08', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:900019131056:training-job/ntm-2021-02-15-06-42-24-836', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/05462b6a-1f57-41b2-917c-0dd72d3b6599', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/05462b6a-1f57-41b2-917c-0dd72d3b6599', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.246.193', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2021-02-15-06-42-24-836', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-246-193.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/dda2181e-24f2-437c-bb48-b99fd40b3e08', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:900019131056:training-job/ntm-2021-02-15-06-42-24-836', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/05462b6a-1f57-41b2-917c-0dd72d3b6599', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/05462b6a-1f57-41b2-917c-0dd72d3b6599', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2021-02-15-06-42-24-836', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-246-193.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/dda2181e-24f2-437c-bb48-b99fd40b3e08', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:900019131056:training-job/ntm-2021-02-15-06-42-24-836', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/05462b6a-1f57-41b2-917c-0dd72d3b6599', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/05462b6a-1f57-41b2-917c-0dd72d3b6599', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.246.193', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2021-02-15-06-42-24-836', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-246-193.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/dda2181e-24f2-437c-bb48-b99fd40b3e08', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:900019131056:training-job/ntm-2021-02-15-06-42-24-836', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/05462b6a-1f57-41b2-917c-0dd72d3b6599', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/05462b6a-1f57-41b2-917c-0dd72d3b6599', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.246.193', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2021-02-15-06-42-24-836', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-246-193.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/dda2181e-24f2-437c-bb48-b99fd40b3e08', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:900019131056:training-job/ntm-2021-02-15-06-42-24-836', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34mProcess 34 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 35 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] Using default worker.\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:45:51.223] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] Initializing\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] None\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] vocab.txt\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] Vocab file is not provided\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:51 INFO 140391001810112] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1613371554.813257, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1613371554.813221}\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:45:54.813] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 3609, \"num_examples\": 1, \"num_bytes\": 32024}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:54 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:54 INFO 140391001810112] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[35mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[35mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'30', u'epochs': u'100', u'feature_dim': u'2000', u'mini_batch_size': u'128', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'2000', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'100', u'mini_batch_size': u'128', u'num_topics': u'30', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] nvidia-smi took: 0.0251839160919 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] Launching parameter server for role server\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/3fb755a4-cbe9-432e-84da-10d06175fd55', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/3fb755a4-cbe9-432e-84da-10d06175fd55', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2021-02-15-06-42-24-836', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-195-170.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/254bb14a-5752-4b3c-a089-3b2a12be258c', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:900019131056:training-job/ntm-2021-02-15-06-42-24-836', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/3fb755a4-cbe9-432e-84da-10d06175fd55', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/3fb755a4-cbe9-432e-84da-10d06175fd55', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.246.193', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2021-02-15-06-42-24-836', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-195-170.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/254bb14a-5752-4b3c-a089-3b2a12be258c', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:900019131056:training-job/ntm-2021-02-15-06-42-24-836', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/3fb755a4-cbe9-432e-84da-10d06175fd55', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/3fb755a4-cbe9-432e-84da-10d06175fd55', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.246.193', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2021-02-15-06-42-24-836', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-195-170.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/254bb14a-5752-4b3c-a089-3b2a12be258c', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:900019131056:training-job/ntm-2021-02-15-06-42-24-836', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[35mProcess 34 is a shell:server.\u001b[0m\n",
      "\u001b[35mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] Using default worker.\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:45:54.380] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] Initializing\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] None\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] vocab.txt\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] Vocab file is not provided\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] Create Store: dist_async\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1613371554.814184, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1613371554.814149}\n",
      "\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:45:54.814] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 450, \"num_examples\": 1, \"num_bytes\": 28628}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:54 INFO 140255820981440] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:45:56.020] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1205, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] # Finished training epoch 1 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] Loss (name: value) total: 6.97715918885\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] Loss (name: value) kld: 0.0287854088205\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] Loss (name: value) recons: 6.94837368859\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] Loss (name: value) logppx: 6.97715918885\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=6.97715918885\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] #progress_metric: host=algo-2, completed 1 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Total Records Seen\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1613371556.029157, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1613371554.814586}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3726.77011104 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:56 INFO 140255820981440] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:45:56.048] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1234, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] # Finished training epoch 1 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] Loss (name: value) total: 6.94910968675\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] Loss (name: value) kld: 0.0161275992949\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] Loss (name: value) recons: 6.932982107\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] Loss (name: value) logppx: 6.94910968675\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=6.94910968675\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Total Records Seen\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1613371556.053614, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1613371554.813619}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3647.98095325 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:56 INFO 140391001810112] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:45:57.252] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 1221, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] # Finished training epoch 2 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] Loss (name: value) total: 6.88300011555\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] Loss (name: value) kld: 0.00265770036802\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] Loss (name: value) recons: 6.88034239742\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] Loss (name: value) logppx: 6.88300011555\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=6.88300011555\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}, \"Total Records Seen\": {\"count\": 1, \"max\": 9054, \"sum\": 9054.0, \"min\": 9054}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1613371557.257075, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1613371556.029463}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3687.11512876 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:57 INFO 140255820981440] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:45:57.332] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 1278, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] # Finished training epoch 2 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] Loss (name: value) total: 6.86113454898\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] Loss (name: value) kld: 0.00270291361246\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] Loss (name: value) recons: 6.85843163729\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] Loss (name: value) logppx: 6.86113454898\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=6.86113454898\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}, \"Total Records Seen\": {\"count\": 1, \"max\": 9048, \"sum\": 9048.0, \"min\": 9048}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1613371557.336863, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1613371556.053914}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3525.37434788 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:57 INFO 140391001810112] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:45:58.435] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1177, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] # Finished training epoch 3 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] Loss (name: value) total: 6.87850756778\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] Loss (name: value) kld: 0.00559792888493\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] Loss (name: value) recons: 6.87290967835\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] Loss (name: value) logppx: 6.87850756778\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=6.87850756778\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] #progress_metric: host=algo-2, completed 3 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}, \"Total Records Seen\": {\"count\": 1, \"max\": 13581, \"sum\": 13581.0, \"min\": 13581}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1613371558.439477, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1613371557.257412}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3829.04507225 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:58 INFO 140255820981440] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:45:58.544] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1206, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] # Finished training epoch 3 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] Loss (name: value) total: 6.85946244001\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] Loss (name: value) kld: 0.00520075780029\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] Loss (name: value) recons: 6.85426170296\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] Loss (name: value) logppx: 6.85946244001\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=6.85946244001\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}, \"Total Records Seen\": {\"count\": 1, \"max\": 13572, \"sum\": 13572.0, \"min\": 13572}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1613371558.549113, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1613371557.337464}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3733.24568257 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:58 INFO 140391001810112] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:45:59.562] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 1122, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] # Finished training epoch 4 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] Loss (name: value) total: 6.8754201134\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] Loss (name: value) kld: 0.00704160362107\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] Loss (name: value) recons: 6.86837851339\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] Loss (name: value) logppx: 6.8754201134\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=6.8754201134\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] Timing: train: 1.12s, val: 0.00s, epoch: 1.13s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}, \"Total Records Seen\": {\"count\": 1, \"max\": 18108, \"sum\": 18108.0, \"min\": 18108}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1613371559.567135, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1613371558.439855}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=4015.15211663 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:45:59 INFO 140255820981440] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:45:59.716] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 1166, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] # Finished training epoch 4 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] Loss (name: value) total: 6.85303495328\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] Loss (name: value) kld: 0.00759612362728\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] Loss (name: value) recons: 6.84543881151\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] Loss (name: value) logppx: 6.85303495328\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=6.85303495328\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] Timing: train: 1.17s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}, \"Total Records Seen\": {\"count\": 1, \"max\": 18096, \"sum\": 18096.0, \"min\": 18096}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1613371559.725095, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1613371558.549416}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3847.31122476 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:45:59 INFO 140391001810112] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:00.860] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1291, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] # Finished training epoch 5 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] Loss (name: value) total: 6.87356864744\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] Loss (name: value) kld: 0.00850124923616\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] Loss (name: value) recons: 6.86506741577\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] Loss (name: value) logppx: 6.87356864744\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=6.87356864744\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] Timing: train: 1.29s, val: 0.00s, epoch: 1.30s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] #progress_metric: host=algo-2, completed 5 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}, \"Total Records Seen\": {\"count\": 1, \"max\": 22635, \"sum\": 22635.0, \"min\": 22635}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1613371560.864342, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1613371559.567767}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3490.97925502 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:00 INFO 140255820981440] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:02.724] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 1858, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] # Finished training epoch 6 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] Loss (name: value) total: 6.86904181374\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] Loss (name: value) kld: 0.00922479655775\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] Loss (name: value) recons: 6.85981700818\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] Loss (name: value) logppx: 6.86904181374\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=6.86904181374\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] patience losses:[6.97715918885337, 6.883000115553538, 6.878507567776574, 6.875420113404592, 6.873568647437626] min patience loss:6.87356864744 current loss:6.86904181374 absolute loss difference:0.00452683369319\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] Timing: train: 1.86s, val: 0.00s, epoch: 1.87s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 216, \"sum\": 216.0, \"min\": 216}, \"Total Records Seen\": {\"count\": 1, \"max\": 27162, \"sum\": 27162.0, \"min\": 27162}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1613371562.732032, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1613371560.86467}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=2423.99139049 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:02 INFO 140255820981440] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:03.905] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1172, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] # Finished training epoch 7 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] Loss (name: value) total: 6.87162414524\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] Loss (name: value) kld: 0.0120756258257\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] Loss (name: value) recons: 6.85954850912\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] Loss (name: value) logppx: 6.87162414524\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] #quality_metric: host=algo-2, epoch=7, train total_loss <loss>=6.87162414524\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] patience losses:[6.883000115553538, 6.878507567776574, 6.875420113404592, 6.873568647437626, 6.869041813744439] min patience loss:6.86904181374 current loss:6.87162414524 absolute loss difference:0.00258233149846\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] Timing: train: 1.17s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] #progress_metric: host=algo-2, completed 7 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 252, \"sum\": 252.0, \"min\": 252}, \"Total Records Seen\": {\"count\": 1, \"max\": 31689, \"sum\": 31689.0, \"min\": 31689}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1613371563.908007, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1613371562.732395}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3850.15757579 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:03 INFO 140255820981440] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:01.129] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1402, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] # Finished training epoch 5 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] Loss (name: value) total: 6.85242312484\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] Loss (name: value) kld: 0.00939876143821\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] Loss (name: value) recons: 6.84302440617\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] Loss (name: value) logppx: 6.85242312484\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=6.85242312484\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] Timing: train: 1.41s, val: 0.00s, epoch: 1.41s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}, \"Total Records Seen\": {\"count\": 1, \"max\": 22620, \"sum\": 22620.0, \"min\": 22620}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1613371561.134753, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1613371559.72554}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3209.88091844 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:01 INFO 140391001810112] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:03.031] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 1895, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] # Finished training epoch 6 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] Loss (name: value) total: 6.84687036276\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] Loss (name: value) kld: 0.010373061497\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] Loss (name: value) recons: 6.83649726709\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] Loss (name: value) logppx: 6.84687036276\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=6.84687036276\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] patience losses:[6.949109686745538, 6.861134548981984, 6.8594624400138855, 6.853034953276317, 6.852423124843174] min patience loss:6.85242312484 current loss:6.84687036276 absolute loss difference:0.00555276208454\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] Timing: train: 1.90s, val: 0.00s, epoch: 1.90s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 216, \"sum\": 216.0, \"min\": 216}, \"Total Records Seen\": {\"count\": 1, \"max\": 27144, \"sum\": 27144.0, \"min\": 27144}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1613371563.036568, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1613371561.135114}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=2379.01178408 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:03 INFO 140391001810112] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:04.228] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1190, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] # Finished training epoch 7 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] Loss (name: value) total: 6.84486519628\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] Loss (name: value) kld: 0.0111835335912\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] Loss (name: value) recons: 6.83368166288\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] Loss (name: value) logppx: 6.84486519628\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=6.84486519628\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] patience losses:[6.861134548981984, 6.8594624400138855, 6.853034953276317, 6.852423124843174, 6.8468703627586365] min patience loss:6.84687036276 current loss:6.84486519628 absolute loss difference:0.00200516647763\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 252, \"sum\": 252.0, \"min\": 252}, \"Total Records Seen\": {\"count\": 1, \"max\": 31668, \"sum\": 31668.0, \"min\": 31668}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1613371564.235029, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1613371563.037185}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3775.4839866 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:04 INFO 140391001810112] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:05.325] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 1416, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] # Finished training epoch 8 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] Loss (name: value) total: 6.8634691702\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] Loss (name: value) kld: 0.0126979968045\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] Loss (name: value) recons: 6.85077116225\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] Loss (name: value) logppx: 6.8634691702\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] #quality_metric: host=algo-2, epoch=8, train total_loss <loss>=6.8634691702\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] patience losses:[6.878507567776574, 6.875420113404592, 6.873568647437626, 6.869041813744439, 6.871624145242903] min patience loss:6.86904181374 current loss:6.8634691702 absolute loss difference:0.00557264354494\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] Timing: train: 1.42s, val: 0.00s, epoch: 1.42s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 288, \"sum\": 288.0, \"min\": 288}, \"Total Records Seen\": {\"count\": 1, \"max\": 36216, \"sum\": 36216.0, \"min\": 36216}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1613371565.329337, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1613371563.908544}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3185.85894989 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:05 INFO 140255820981440] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:05.619] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 1383, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] # Finished training epoch 8 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] Loss (name: value) total: 6.84461353223\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] Loss (name: value) kld: 0.0137624694875\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] Loss (name: value) recons: 6.83085101181\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] Loss (name: value) logppx: 6.84461353223\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=6.84461353223\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] patience losses:[6.8594624400138855, 6.853034953276317, 6.852423124843174, 6.8468703627586365, 6.84486519628101] min patience loss:6.84486519628 current loss:6.84461353223 absolute loss difference:0.000251664055718\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] Timing: train: 1.38s, val: 0.00s, epoch: 1.39s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 288, \"sum\": 288.0, \"min\": 288}, \"Total Records Seen\": {\"count\": 1, \"max\": 36192, \"sum\": 36192.0, \"min\": 36192}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1613371565.624761, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1613371564.235721}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3255.9599491 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:05 INFO 140391001810112] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:07.009] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 1678, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] # Finished training epoch 9 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] Loss (name: value) total: 6.86232000589\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] Loss (name: value) kld: 0.0128731198945\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] Loss (name: value) recons: 6.8494467934\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] Loss (name: value) logppx: 6.86232000589\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] #quality_metric: host=algo-2, epoch=9, train total_loss <loss>=6.86232000589\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] patience losses:[6.875420113404592, 6.873568647437626, 6.869041813744439, 6.871624145242903, 6.8634691701995] min patience loss:6.8634691702 current loss:6.86232000589 absolute loss difference:0.00114916430579\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] Timing: train: 1.68s, val: 0.00s, epoch: 1.68s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] #progress_metric: host=algo-2, completed 9 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 324, \"sum\": 324.0, \"min\": 324}, \"Total Records Seen\": {\"count\": 1, \"max\": 40743, \"sum\": 40743.0, \"min\": 40743}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1613371567.015181, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1613371565.329852}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=2685.86410225 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:07 INFO 140255820981440] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:07.422] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 1797, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] # Finished training epoch 9 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] Loss (name: value) total: 6.84252458149\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] Loss (name: value) kld: 0.0155207527633\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] Loss (name: value) recons: 6.82700376378\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] Loss (name: value) logppx: 6.84252458149\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=6.84252458149\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] patience losses:[6.853034953276317, 6.852423124843174, 6.8468703627586365, 6.84486519628101, 6.844613532225291] min patience loss:6.84461353223 current loss:6.84252458149 absolute loss difference:0.00208895073997\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] Timing: train: 1.80s, val: 0.00s, epoch: 1.80s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 324, \"sum\": 324.0, \"min\": 324}, \"Total Records Seen\": {\"count\": 1, \"max\": 40716, \"sum\": 40716.0, \"min\": 40716}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1613371567.428969, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1613371565.62536}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=2507.99567476 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:07 INFO 140391001810112] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:08.357] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 1341, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] # Finished training epoch 10 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] Loss (name: value) total: 6.86272388697\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] Loss (name: value) kld: 0.0172145544655\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] Loss (name: value) recons: 6.84550931719\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] Loss (name: value) logppx: 6.86272388697\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] #quality_metric: host=algo-2, epoch=10, train total_loss <loss>=6.86272388697\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] patience losses:[6.873568647437626, 6.869041813744439, 6.871624145242903, 6.8634691701995, 6.862320005893707] min patience loss:6.86232000589 current loss:6.86272388697 absolute loss difference:0.000403881072998\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] Timing: train: 1.34s, val: 0.00s, epoch: 1.34s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] #progress_metric: host=algo-2, completed 10 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 360, \"sum\": 360.0, \"min\": 360}, \"Total Records Seen\": {\"count\": 1, \"max\": 45270, \"sum\": 45270.0, \"min\": 45270}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1613371568.359718, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1613371567.015503}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3367.36368231 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:08 INFO 140255820981440] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:08.760] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 1330, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] # Finished training epoch 10 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] Loss (name: value) total: 6.84007820156\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] Loss (name: value) kld: 0.0170080415361\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] Loss (name: value) recons: 6.8230701685\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] Loss (name: value) logppx: 6.84007820156\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=6.84007820156\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] patience losses:[6.852423124843174, 6.8468703627586365, 6.84486519628101, 6.844613532225291, 6.842524581485325] min patience loss:6.84252458149 current loss:6.84007820156 absolute loss difference:0.00244637992647\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] Timing: train: 1.33s, val: 0.00s, epoch: 1.34s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 360, \"sum\": 360.0, \"min\": 360}, \"Total Records Seen\": {\"count\": 1, \"max\": 45240, \"sum\": 45240.0, \"min\": 45240}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1613371568.765643, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1613371567.429327}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3384.82684896 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:08 INFO 140391001810112] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:09.540] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1180, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] # Finished training epoch 11 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] Loss (name: value) total: 6.85869267252\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] Loss (name: value) kld: 0.0184093221065\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] Loss (name: value) recons: 6.8402833475\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] Loss (name: value) logppx: 6.85869267252\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] #quality_metric: host=algo-2, epoch=11, train total_loss <loss>=6.85869267252\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] patience losses:[6.869041813744439, 6.871624145242903, 6.8634691701995, 6.862320005893707, 6.862723886966705] min patience loss:6.86232000589 current loss:6.85869267252 absolute loss difference:0.00362733337614\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] #progress_metric: host=algo-2, completed 11 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 396, \"sum\": 396.0, \"min\": 396}, \"Total Records Seen\": {\"count\": 1, \"max\": 49797, \"sum\": 49797.0, \"min\": 49797}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1613371569.544354, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1613371568.359987}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3821.82660132 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:09 INFO 140255820981440] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:09.948] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1182, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] # Finished training epoch 11 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] Loss (name: value) total: 6.83708527353\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] Loss (name: value) kld: 0.0175453711353\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] Loss (name: value) recons: 6.81953985824\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] Loss (name: value) logppx: 6.83708527353\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=6.83708527353\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] patience losses:[6.8468703627586365, 6.84486519628101, 6.844613532225291, 6.842524581485325, 6.840078201558855] min patience loss:6.84007820156 current loss:6.83708527353 absolute loss difference:0.00299292802811\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] Timing: train: 1.18s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 396, \"sum\": 396.0, \"min\": 396}, \"Total Records Seen\": {\"count\": 1, \"max\": 49764, \"sum\": 49764.0, \"min\": 49764}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1613371569.953474, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1613371568.766041}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3809.39893024 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:09 INFO 140391001810112] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:10.673] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 1128, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] # Finished training epoch 12 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] Loss (name: value) total: 6.86021959782\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] Loss (name: value) kld: 0.018113502159\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] Loss (name: value) recons: 6.84210609727\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] Loss (name: value) logppx: 6.86021959782\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] #quality_metric: host=algo-2, epoch=12, train total_loss <loss>=6.86021959782\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] patience losses:[6.871624145242903, 6.8634691701995, 6.862320005893707, 6.862723886966705, 6.858692672517565] min patience loss:6.85869267252 current loss:6.86021959782 absolute loss difference:0.0015269252989\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] Timing: train: 1.13s, val: 0.00s, epoch: 1.13s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] #progress_metric: host=algo-2, completed 12 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 432, \"sum\": 432.0, \"min\": 432}, \"Total Records Seen\": {\"count\": 1, \"max\": 54324, \"sum\": 54324.0, \"min\": 54324}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1613371570.675484, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1613371569.544618}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=4002.50724779 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:10 INFO 140255820981440] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:11.900] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 1223, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] # Finished training epoch 13 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] Loss (name: value) total: 6.85643994808\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] Loss (name: value) kld: 0.020741164542\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] Loss (name: value) recons: 6.83569877015\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] Loss (name: value) logppx: 6.85643994808\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] #quality_metric: host=algo-2, epoch=13, train total_loss <loss>=6.85643994808\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] patience losses:[6.8634691701995, 6.862320005893707, 6.862723886966705, 6.858692672517565, 6.860219597816467] min patience loss:6.85869267252 current loss:6.85643994808 absolute loss difference:0.00225272443559\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] #progress_metric: host=algo-2, completed 13 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 468, \"sum\": 468.0, \"min\": 468}, \"Total Records Seen\": {\"count\": 1, \"max\": 58851, \"sum\": 58851.0, \"min\": 58851}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1613371571.904806, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1613371570.675822}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3682.99138529 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:11 INFO 140255820981440] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:13.182] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 1277, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] # Finished training epoch 14 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] Loss (name: value) total: 6.84731062253\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] Loss (name: value) kld: 0.0243041684799\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] Loss (name: value) recons: 6.82300645775\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] Loss (name: value) logppx: 6.84731062253\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] #quality_metric: host=algo-2, epoch=14, train total_loss <loss>=6.84731062253\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] patience losses:[6.862320005893707, 6.862723886966705, 6.858692672517565, 6.860219597816467, 6.85643994808197] min patience loss:6.85643994808 current loss:6.84731062253 absolute loss difference:0.00912932554881\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] #progress_metric: host=algo-2, completed 14 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 504, \"sum\": 504.0, \"min\": 504}, \"Total Records Seen\": {\"count\": 1, \"max\": 63378, \"sum\": 63378.0, \"min\": 63378}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1613371573.187225, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1613371571.905139}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3530.46898812 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:13 INFO 140255820981440] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:14.390] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 1202, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] # Finished training epoch 15 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] Loss (name: value) total: 6.8356635703\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] Loss (name: value) kld: 0.0321699808248\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] Loss (name: value) recons: 6.80349353287\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] Loss (name: value) logppx: 6.8356635703\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] #quality_metric: host=algo-2, epoch=15, train total_loss <loss>=6.8356635703\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] patience losses:[6.862723886966705, 6.858692672517565, 6.860219597816467, 6.85643994808197, 6.847310622533162] min patience loss:6.84731062253 current loss:6.8356635703 absolute loss difference:0.0116470522351\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] #progress_metric: host=algo-2, completed 15 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 540, \"sum\": 540.0, \"min\": 540}, \"Total Records Seen\": {\"count\": 1, \"max\": 67905, \"sum\": 67905.0, \"min\": 67905}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1613371574.396358, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1613371573.187893}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3745.31465346 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:14 INFO 140255820981440] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:11.118] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 1162, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] # Finished training epoch 12 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] Loss (name: value) total: 6.83585558997\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] Loss (name: value) kld: 0.0188631542001\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] Loss (name: value) recons: 6.81699242194\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] Loss (name: value) logppx: 6.83585558997\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=6.83585558997\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] patience losses:[6.84486519628101, 6.844613532225291, 6.842524581485325, 6.840078201558855, 6.837085273530748] min patience loss:6.83708527353 current loss:6.83585558997 absolute loss difference:0.00122968355815\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 432, \"sum\": 432.0, \"min\": 432}, \"Total Records Seen\": {\"count\": 1, \"max\": 54288, \"sum\": 54288.0, \"min\": 54288}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1613371571.123218, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1613371569.953785}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3867.47658239 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:11 INFO 140391001810112] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:12.442] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 1318, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] # Finished training epoch 13 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] Loss (name: value) total: 6.83285391993\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] Loss (name: value) kld: 0.0233764809349\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] Loss (name: value) recons: 6.80947747495\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] Loss (name: value) logppx: 6.83285391993\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=6.83285391993\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] patience losses:[6.844613532225291, 6.842524581485325, 6.840078201558855, 6.837085273530748, 6.835855589972602] min patience loss:6.83585558997 current loss:6.83285391993 absolute loss difference:0.00300167004267\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] Timing: train: 1.32s, val: 0.00s, epoch: 1.32s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 468, \"sum\": 468.0, \"min\": 468}, \"Total Records Seen\": {\"count\": 1, \"max\": 58812, \"sum\": 58812.0, \"min\": 58812}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1613371572.447057, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1613371571.123874}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3418.63566772 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:12 INFO 140391001810112] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:13.648] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 1201, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] # Finished training epoch 14 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] Loss (name: value) total: 6.82148507569\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] Loss (name: value) kld: 0.0265199296927\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] Loss (name: value) recons: 6.79496514135\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] Loss (name: value) logppx: 6.82148507569\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=6.82148507569\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] patience losses:[6.842524581485325, 6.840078201558855, 6.837085273530748, 6.835855589972602, 6.832853919929928] min patience loss:6.83285391993 current loss:6.82148507569 absolute loss difference:0.0113688442442\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 504, \"sum\": 504.0, \"min\": 504}, \"Total Records Seen\": {\"count\": 1, \"max\": 63336, \"sum\": 63336.0, \"min\": 63336}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1613371573.654519, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1613371572.447396}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3740.2033043 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:13 INFO 140391001810112] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:14.873] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 1214, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] # Finished training epoch 15 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] Loss (name: value) total: 6.8099034826\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] Loss (name: value) kld: 0.0364460254269\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] Loss (name: value) recons: 6.77345749405\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] Loss (name: value) logppx: 6.8099034826\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=6.8099034826\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] patience losses:[6.840078201558855, 6.837085273530748, 6.835855589972602, 6.832853919929928, 6.821485075685713] min patience loss:6.82148507569 current loss:6.8099034826 absolute loss difference:0.0115815930896\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 540, \"sum\": 540.0, \"min\": 540}, \"Total Records Seen\": {\"count\": 1, \"max\": 67860, \"sum\": 67860.0, \"min\": 67860}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1613371574.877619, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1613371573.658282}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3709.09617363 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:14 INFO 140391001810112] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:15.569] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 1172, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] # Finished training epoch 16 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] Loss (name: value) total: 6.82367093033\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] Loss (name: value) kld: 0.0411881275682\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] Loss (name: value) recons: 6.78248282274\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] Loss (name: value) logppx: 6.82367093033\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] #quality_metric: host=algo-2, epoch=16, train total_loss <loss>=6.82367093033\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] patience losses:[6.858692672517565, 6.860219597816467, 6.85643994808197, 6.847310622533162, 6.835663570298089] min patience loss:6.8356635703 current loss:6.82367093033 absolute loss difference:0.0119926399655\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] Timing: train: 1.17s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] #progress_metric: host=algo-2, completed 16 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 576, \"sum\": 576.0, \"min\": 576}, \"Total Records Seen\": {\"count\": 1, \"max\": 72432, \"sum\": 72432.0, \"min\": 72432}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1613371575.573288, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1613371574.396643}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3846.84088042 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:15 INFO 140255820981440] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:16.125] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 1245, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] # Finished training epoch 16 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] Loss (name: value) total: 6.79033089346\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] Loss (name: value) kld: 0.0434718847327\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] Loss (name: value) recons: 6.7468590339\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] Loss (name: value) logppx: 6.79033089346\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=6.79033089346\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] patience losses:[6.837085273530748, 6.835855589972602, 6.832853919929928, 6.821485075685713, 6.80990348259608] min patience loss:6.8099034826 current loss:6.79033089346 absolute loss difference:0.0195725891325\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 576, \"sum\": 576.0, \"min\": 576}, \"Total Records Seen\": {\"count\": 1, \"max\": 72384, \"sum\": 72384.0, \"min\": 72384}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1613371576.130235, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1613371574.87831}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3613.08937693 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:16 INFO 140391001810112] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:16.800] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 1226, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] # Finished training epoch 17 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] Loss (name: value) total: 6.80592013068\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] Loss (name: value) kld: 0.0454641001092\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] Loss (name: value) recons: 6.76045605209\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] Loss (name: value) logppx: 6.80592013068\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] #quality_metric: host=algo-2, epoch=17, train total_loss <loss>=6.80592013068\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] patience losses:[6.860219597816467, 6.85643994808197, 6.847310622533162, 6.835663570298089, 6.823670930332607] min patience loss:6.82367093033 current loss:6.80592013068 absolute loss difference:0.0177507996559\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] #progress_metric: host=algo-2, completed 17 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 612, \"sum\": 612.0, \"min\": 612}, \"Total Records Seen\": {\"count\": 1, \"max\": 76959, \"sum\": 76959.0, \"min\": 76959}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1613371576.805985, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1613371575.573614}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3672.84943661 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:16 INFO 140255820981440] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:18.120] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 1314, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] # Finished training epoch 18 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] Loss (name: value) total: 6.7880816261\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] Loss (name: value) kld: 0.0522924452865\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] Loss (name: value) recons: 6.73578921954\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] Loss (name: value) logppx: 6.7880816261\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] #quality_metric: host=algo-2, epoch=18, train total_loss <loss>=6.7880816261\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] patience losses:[6.85643994808197, 6.847310622533162, 6.835663570298089, 6.823670930332607, 6.805920130676693] min patience loss:6.80592013068 current loss:6.7880816261 absolute loss difference:0.0178385045793\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] Timing: train: 1.32s, val: 0.00s, epoch: 1.32s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] #progress_metric: host=algo-2, completed 18 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 648, \"sum\": 648.0, \"min\": 648}, \"Total Records Seen\": {\"count\": 1, \"max\": 81486, \"sum\": 81486.0, \"min\": 81486}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1613371578.124726, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1613371576.806325}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3433.27750461 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:18 INFO 140255820981440] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:17.367] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 1233, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] # Finished training epoch 17 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] Loss (name: value) total: 6.78102407853\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] Loss (name: value) kld: 0.0491323145624\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] Loss (name: value) recons: 6.73189173804\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] Loss (name: value) logppx: 6.78102407853\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=6.78102407853\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] patience losses:[6.835855589972602, 6.832853919929928, 6.821485075685713, 6.80990348259608, 6.790330893463558] min patience loss:6.79033089346 current loss:6.78102407853 absolute loss difference:0.00930681493547\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 612, \"sum\": 612.0, \"min\": 612}, \"Total Records Seen\": {\"count\": 1, \"max\": 76908, \"sum\": 76908.0, \"min\": 76908}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1613371577.371079, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1613371576.130517}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3646.34057822 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:17 INFO 140391001810112] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:18.621] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 1249, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] # Finished training epoch 18 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] Loss (name: value) total: 6.76494832171\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] Loss (name: value) kld: 0.0548644163646\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] Loss (name: value) recons: 6.7100838489\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] Loss (name: value) logppx: 6.76494832171\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=6.76494832171\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] patience losses:[6.832853919929928, 6.821485075685713, 6.80990348259608, 6.790330893463558, 6.781024078528087] min patience loss:6.78102407853 current loss:6.76494832171 absolute loss difference:0.0160757568147\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 648, \"sum\": 648.0, \"min\": 648}, \"Total Records Seen\": {\"count\": 1, \"max\": 81432, \"sum\": 81432.0, \"min\": 81432}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1613371578.625868, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1613371577.371381}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3605.78682623 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:18 INFO 140391001810112] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:19.289] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 1164, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] # Finished training epoch 19 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] Loss (name: value) total: 6.7805106971\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] Loss (name: value) kld: 0.0559832325412\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] Loss (name: value) recons: 6.72452748484\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] Loss (name: value) logppx: 6.7805106971\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] #quality_metric: host=algo-2, epoch=19, train total_loss <loss>=6.7805106971\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] patience losses:[6.847310622533162, 6.835663570298089, 6.823670930332607, 6.805920130676693, 6.7880816260973615] min patience loss:6.7880816261 current loss:6.7805106971 absolute loss difference:0.00757092899746\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] #progress_metric: host=algo-2, completed 19 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 684, \"sum\": 684.0, \"min\": 684}, \"Total Records Seen\": {\"count\": 1, \"max\": 86013, \"sum\": 86013.0, \"min\": 86013}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1613371579.295281, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1613371578.125031}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3867.8590855 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:19 INFO 140255820981440] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:19.864] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 1233, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] # Finished training epoch 19 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] Loss (name: value) total: 6.75534237756\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] Loss (name: value) kld: 0.0584339612784\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] Loss (name: value) recons: 6.69690844085\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] Loss (name: value) logppx: 6.75534237756\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=6.75534237756\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] patience losses:[6.821485075685713, 6.80990348259608, 6.790330893463558, 6.781024078528087, 6.764948321713342] min patience loss:6.76494832171 current loss:6.75534237756 absolute loss difference:0.00960594415665\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 684, \"sum\": 684.0, \"min\": 684}, \"Total Records Seen\": {\"count\": 1, \"max\": 85956, \"sum\": 85956.0, \"min\": 85956}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1613371579.869807, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1613371578.626187}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3637.31414189 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:19 INFO 140391001810112] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:20.507] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 1211, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] # Finished training epoch 20 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] Loss (name: value) total: 6.77481762568\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] Loss (name: value) kld: 0.0607801764272\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] Loss (name: value) recons: 6.71403749784\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] Loss (name: value) logppx: 6.77481762568\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] #quality_metric: host=algo-2, epoch=20, train total_loss <loss>=6.77481762568\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] patience losses:[6.835663570298089, 6.823670930332607, 6.805920130676693, 6.7880816260973615, 6.780510697099897] min patience loss:6.7805106971 current loss:6.77481762568 absolute loss difference:0.00569307141834\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] #progress_metric: host=algo-2, completed 20 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 720, \"sum\": 720.0, \"min\": 720}, \"Total Records Seen\": {\"count\": 1, \"max\": 90540, \"sum\": 90540.0, \"min\": 90540}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1613371580.511664, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1613371579.295598}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3722.22812225 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:20 INFO 140255820981440] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:21.121] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 1251, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] # Finished training epoch 20 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] Loss (name: value) total: 6.75247059266\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] Loss (name: value) kld: 0.0640487099687\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] Loss (name: value) recons: 6.68842184544\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] Loss (name: value) logppx: 6.75247059266\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=6.75247059266\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] patience losses:[6.80990348259608, 6.790330893463558, 6.781024078528087, 6.764948321713342, 6.755342377556695] min patience loss:6.75534237756 current loss:6.75247059266 absolute loss difference:0.00287178489897\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] Timing: train: 1.25s, val: 0.01s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 720, \"sum\": 720.0, \"min\": 720}, \"Total Records Seen\": {\"count\": 1, \"max\": 90480, \"sum\": 90480.0, \"min\": 90480}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1613371581.128738, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1613371579.870119}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3593.17952139 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:21 INFO 140391001810112] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:21.741] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 1229, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] # Finished training epoch 21 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] Loss (name: value) total: 6.76655696498\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] Loss (name: value) kld: 0.0621453529845\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] Loss (name: value) recons: 6.70441160599\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] Loss (name: value) logppx: 6.76655696498\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] #quality_metric: host=algo-2, epoch=21, train total_loss <loss>=6.76655696498\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] patience losses:[6.823670930332607, 6.805920130676693, 6.7880816260973615, 6.780510697099897, 6.7748176256815595] min patience loss:6.77481762568 current loss:6.76655696498 absolute loss difference:0.00826066070133\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] #progress_metric: host=algo-2, completed 21 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 756, \"sum\": 756.0, \"min\": 756}, \"Total Records Seen\": {\"count\": 1, \"max\": 95067, \"sum\": 95067.0, \"min\": 95067}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1613371581.745687, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1613371580.511925}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3668.78304334 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:21 INFO 140255820981440] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:23.039] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 1293, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] # Finished training epoch 22 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] Loss (name: value) total: 6.76766616768\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] Loss (name: value) kld: 0.0657215683928\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] Loss (name: value) recons: 6.70194460286\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] Loss (name: value) logppx: 6.76766616768\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] #quality_metric: host=algo-2, epoch=22, train total_loss <loss>=6.76766616768\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] patience losses:[6.805920130676693, 6.7880816260973615, 6.780510697099897, 6.7748176256815595, 6.766556964980231] min patience loss:6.76655696498 current loss:6.76766616768 absolute loss difference:0.00110920270284\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] Timing: train: 1.30s, val: 0.00s, epoch: 1.30s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] #progress_metric: host=algo-2, completed 22 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 792, \"sum\": 792.0, \"min\": 792}, \"Total Records Seen\": {\"count\": 1, \"max\": 99594, \"sum\": 99594.0, \"min\": 99594}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1613371583.043158, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1613371581.745997}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3488.62466391 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:23 INFO 140255820981440] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:22.369] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 1239, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] # Finished training epoch 21 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] Loss (name: value) total: 6.73520305422\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] Loss (name: value) kld: 0.0631825816818\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] Loss (name: value) recons: 6.6720204552\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] Loss (name: value) logppx: 6.73520305422\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=6.73520305422\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] patience losses:[6.790330893463558, 6.781024078528087, 6.764948321713342, 6.755342377556695, 6.752470592657725] min patience loss:6.75247059266 current loss:6.73520305422 absolute loss difference:0.0172675384416\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 756, \"sum\": 756.0, \"min\": 756}, \"Total Records Seen\": {\"count\": 1, \"max\": 95004, \"sum\": 95004.0, \"min\": 95004}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1613371582.375861, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1613371581.129384}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3628.92835312 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:22 INFO 140391001810112] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:24.260] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 1215, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] # Finished training epoch 23 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] Loss (name: value) total: 6.75799848636\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] Loss (name: value) kld: 0.0688205963104\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] Loss (name: value) recons: 6.68917787075\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] Loss (name: value) logppx: 6.75799848636\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] #quality_metric: host=algo-2, epoch=23, train total_loss <loss>=6.75799848636\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] patience losses:[6.7880816260973615, 6.780510697099897, 6.7748176256815595, 6.766556964980231, 6.767666167683071] min patience loss:6.76655696498 current loss:6.75799848636 absolute loss difference:0.00855847862032\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] #progress_metric: host=algo-2, completed 23 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 828, \"sum\": 828.0, \"min\": 828}, \"Total Records Seen\": {\"count\": 1, \"max\": 104121, \"sum\": 104121.0, \"min\": 104121}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1613371584.265685, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1613371583.043905}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3704.75286603 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:24 INFO 140255820981440] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:23.645] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 1268, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] # Finished training epoch 22 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] Loss (name: value) total: 6.73822285732\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] Loss (name: value) kld: 0.0664049073433\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] Loss (name: value) recons: 6.67181791862\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] Loss (name: value) logppx: 6.73822285732\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=6.73822285732\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] patience losses:[6.781024078528087, 6.764948321713342, 6.755342377556695, 6.752470592657725, 6.735203054216173] min patience loss:6.73520305422 current loss:6.73822285732 absolute loss difference:0.00301980310016\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 792, \"sum\": 792.0, \"min\": 792}, \"Total Records Seen\": {\"count\": 1, \"max\": 99528, \"sum\": 99528.0, \"min\": 99528}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1613371583.647636, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1613371582.3763}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3557.63581681 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:23 INFO 140391001810112] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:24.799] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 1150, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] # Finished training epoch 23 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] Loss (name: value) total: 6.73880381717\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] Loss (name: value) kld: 0.0712553809086\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] Loss (name: value) recons: 6.66754845778\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] Loss (name: value) logppx: 6.73880381717\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=6.73880381717\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] patience losses:[6.764948321713342, 6.755342377556695, 6.752470592657725, 6.735203054216173, 6.738222857316335] min patience loss:6.73520305422 current loss:6.73880381717 absolute loss difference:0.00360076295005\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 828, \"sum\": 828.0, \"min\": 828}, \"Total Records Seen\": {\"count\": 1, \"max\": 104052, \"sum\": 104052.0, \"min\": 104052}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1613371584.801428, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1613371583.648047}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3921.6837018 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:24 INFO 140391001810112] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:25.417] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 1150, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] # Finished training epoch 24 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] Loss (name: value) total: 6.75684505701\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] Loss (name: value) kld: 0.0701084536914\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] Loss (name: value) recons: 6.68673664994\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] Loss (name: value) logppx: 6.75684505701\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] #quality_metric: host=algo-2, epoch=24, train total_loss <loss>=6.75684505701\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] patience losses:[6.780510697099897, 6.7748176256815595, 6.766556964980231, 6.767666167683071, 6.757998486359914] min patience loss:6.75799848636 current loss:6.75684505701 absolute loss difference:0.00115342934926\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] Timing: train: 1.15s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] #progress_metric: host=algo-2, completed 24 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 864, \"sum\": 864.0, \"min\": 864}, \"Total Records Seen\": {\"count\": 1, \"max\": 108648, \"sum\": 108648.0, \"min\": 108648}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1613371585.42164, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1613371584.265978}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3916.78098916 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:25 INFO 140255820981440] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:25.982] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 1180, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] # Finished training epoch 24 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] Loss (name: value) total: 6.72995628251\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] Loss (name: value) kld: 0.0710684510672\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] Loss (name: value) recons: 6.65888778369\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] Loss (name: value) logppx: 6.72995628251\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=6.72995628251\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] patience losses:[6.755342377556695, 6.752470592657725, 6.735203054216173, 6.738222857316335, 6.738803817166223] min patience loss:6.73520305422 current loss:6.72995628251 absolute loss difference:0.00524677170648\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 864, \"sum\": 864.0, \"min\": 864}, \"Total Records Seen\": {\"count\": 1, \"max\": 108576, \"sum\": 108576.0, \"min\": 108576}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1613371585.986349, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1613371584.801964}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3818.81427492 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:25 INFO 140391001810112] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:26.591] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 1169, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] # Finished training epoch 25 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] Loss (name: value) total: 6.75236670838\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] Loss (name: value) kld: 0.0738080038896\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] Loss (name: value) recons: 6.67855879333\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] Loss (name: value) logppx: 6.75236670838\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] #quality_metric: host=algo-2, epoch=25, train total_loss <loss>=6.75236670838\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] patience losses:[6.7748176256815595, 6.766556964980231, 6.767666167683071, 6.757998486359914, 6.756845057010651] min patience loss:6.75684505701 current loss:6.75236670838 absolute loss difference:0.00447834862603\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] #progress_metric: host=algo-2, completed 25 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 900, \"sum\": 900.0, \"min\": 900}, \"Total Records Seen\": {\"count\": 1, \"max\": 113175, \"sum\": 113175.0, \"min\": 113175}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1613371586.595592, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1613371585.421908}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3856.38907832 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:26 INFO 140255820981440] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:27.229] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 1242, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] # Finished training epoch 25 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] Loss (name: value) total: 6.72236725357\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] Loss (name: value) kld: 0.0753445465428\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] Loss (name: value) recons: 6.64702263806\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] Loss (name: value) logppx: 6.72236725357\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=6.72236725357\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] patience losses:[6.752470592657725, 6.735203054216173, 6.738222857316335, 6.738803817166223, 6.729956282509698] min patience loss:6.72995628251 current loss:6.72236725357 absolute loss difference:0.00758902894126\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 900, \"sum\": 900.0, \"min\": 900}, \"Total Records Seen\": {\"count\": 1, \"max\": 113100, \"sum\": 113100.0, \"min\": 113100}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1613371587.235748, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1613371585.986641}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3621.29767934 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:27 INFO 140391001810112] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:27.853] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 1256, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] # Finished training epoch 26 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] Loss (name: value) total: 6.74721080727\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] Loss (name: value) kld: 0.074402077227\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] Loss (name: value) recons: 6.67280873325\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] Loss (name: value) logppx: 6.74721080727\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] #quality_metric: host=algo-2, epoch=26, train total_loss <loss>=6.74721080727\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] patience losses:[6.766556964980231, 6.767666167683071, 6.757998486359914, 6.756845057010651, 6.75236670838462] min patience loss:6.75236670838 current loss:6.74721080727 absolute loss difference:0.00515590111415\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] #progress_metric: host=algo-2, completed 26 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 936, \"sum\": 936.0, \"min\": 936}, \"Total Records Seen\": {\"count\": 1, \"max\": 117702, \"sum\": 117702.0, \"min\": 117702}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1613371587.856886, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1613371586.596096}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3590.09301154 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:27 INFO 140255820981440] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:29.084] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 1226, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] # Finished training epoch 27 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] Loss (name: value) total: 6.74083054066\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] Loss (name: value) kld: 0.080732763124\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] Loss (name: value) recons: 6.66009775135\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] Loss (name: value) logppx: 6.74083054066\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] #quality_metric: host=algo-2, epoch=27, train total_loss <loss>=6.74083054066\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] patience losses:[6.767666167683071, 6.757998486359914, 6.756845057010651, 6.75236670838462, 6.7472108072704735] min patience loss:6.74721080727 current loss:6.74083054066 absolute loss difference:0.00638026661343\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] #progress_metric: host=algo-2, completed 27 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 972, \"sum\": 972.0, \"min\": 972}, \"Total Records Seen\": {\"count\": 1, \"max\": 122229, \"sum\": 122229.0, \"min\": 122229}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1613371589.088234, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1613371587.857239}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3677.03166188 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:29 INFO 140255820981440] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:28.522] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 1285, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] # Finished training epoch 26 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] Loss (name: value) total: 6.72130246957\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] Loss (name: value) kld: 0.0767963778021\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] Loss (name: value) recons: 6.64450606373\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] Loss (name: value) logppx: 6.72130246957\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=6.72130246957\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] patience losses:[6.735203054216173, 6.738222857316335, 6.738803817166223, 6.729956282509698, 6.722367253568438] min patience loss:6.72236725357 current loss:6.72130246957 absolute loss difference:0.00106478399701\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] Timing: train: 1.29s, val: 0.00s, epoch: 1.29s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 936, \"sum\": 936.0, \"min\": 936}, \"Total Records Seen\": {\"count\": 1, \"max\": 117624, \"sum\": 117624.0, \"min\": 117624}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1613371588.526256, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1613371587.236079}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3505.97118956 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:28 INFO 140391001810112] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:30.232] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 1144, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] # Finished training epoch 28 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] Loss (name: value) total: 6.73248607583\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] Loss (name: value) kld: 0.0809124016927\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] Loss (name: value) recons: 6.65157371097\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] Loss (name: value) logppx: 6.73248607583\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] #quality_metric: host=algo-2, epoch=28, train total_loss <loss>=6.73248607583\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] patience losses:[6.757998486359914, 6.756845057010651, 6.75236670838462, 6.7472108072704735, 6.7408305406570435] min patience loss:6.74083054066 current loss:6.73248607583 absolute loss difference:0.00834446483188\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] #progress_metric: host=algo-2, completed 28 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1008, \"sum\": 1008.0, \"min\": 1008}, \"Total Records Seen\": {\"count\": 1, \"max\": 126756, \"sum\": 126756.0, \"min\": 126756}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1613371590.236476, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1613371589.088539}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3943.0948042 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:30 INFO 140255820981440] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:29.705] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 1178, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] # Finished training epoch 27 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] Loss (name: value) total: 6.71243702041\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] Loss (name: value) kld: 0.0793170040059\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] Loss (name: value) recons: 6.63312005997\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] Loss (name: value) logppx: 6.71243702041\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=6.71243702041\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] patience losses:[6.738222857316335, 6.738803817166223, 6.729956282509698, 6.722367253568438, 6.721302469571431] min patience loss:6.72130246957 current loss:6.71243702041 absolute loss difference:0.00886544916365\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 972, \"sum\": 972.0, \"min\": 972}, \"Total Records Seen\": {\"count\": 1, \"max\": 122148, \"sum\": 122148.0, \"min\": 122148}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1613371589.711795, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1613371588.526624}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3815.48396162 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:29 INFO 140391001810112] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:30.902] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 1189, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] # Finished training epoch 28 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] Loss (name: value) total: 6.7088889016\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] Loss (name: value) kld: 0.0835382498076\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] Loss (name: value) recons: 6.62535066737\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] Loss (name: value) logppx: 6.7088889016\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=6.7088889016\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] patience losses:[6.738803817166223, 6.729956282509698, 6.722367253568438, 6.721302469571431, 6.712437020407783] min patience loss:6.71243702041 current loss:6.7088889016 absolute loss difference:0.00354811880324\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1008, \"sum\": 1008.0, \"min\": 1008}, \"Total Records Seen\": {\"count\": 1, \"max\": 126672, \"sum\": 126672.0, \"min\": 126672}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1613371590.906486, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1613371589.712716}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3789.14445273 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:30 INFO 140391001810112] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:32.163] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 1254, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] # Finished training epoch 29 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] Loss (name: value) total: 6.70052304533\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] Loss (name: value) kld: 0.0859321010196\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] Loss (name: value) recons: 6.61459090975\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] Loss (name: value) logppx: 6.70052304533\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=6.70052304533\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] patience losses:[6.729956282509698, 6.722367253568438, 6.721302469571431, 6.712437020407783, 6.7088889016045465] min patience loss:6.7088889016 current loss:6.70052304533 absolute loss difference:0.00836585627662\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1044, \"sum\": 1044.0, \"min\": 1044}, \"Total Records Seen\": {\"count\": 1, \"max\": 131196, \"sum\": 131196.0, \"min\": 131196}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1613371592.168533, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1613371590.9068}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3585.11295326 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:32 INFO 140391001810112] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:33.456] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 1287, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] # Finished training epoch 30 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] Loss (name: value) total: 6.69213595655\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] Loss (name: value) kld: 0.0891375807631\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] Loss (name: value) recons: 6.60299838252\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] Loss (name: value) logppx: 6.69213595655\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=6.69213595655\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] patience losses:[6.722367253568438, 6.721302469571431, 6.712437020407783, 6.7088889016045465, 6.7005230453279285] min patience loss:6.70052304533 current loss:6.69213595655 absolute loss difference:0.00838708877563\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] Timing: train: 1.29s, val: 0.00s, epoch: 1.29s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1080, \"sum\": 1080.0, \"min\": 1080}, \"Total Records Seen\": {\"count\": 1, \"max\": 135720, \"sum\": 135720.0, \"min\": 135720}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1613371593.46211, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1613371592.168821}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3497.48177967 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:33 INFO 140391001810112] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:34.675] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 1211, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] # Finished training epoch 31 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] Loss (name: value) total: 6.68778471152\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] Loss (name: value) kld: 0.0920480062875\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] Loss (name: value) recons: 6.59573670891\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] Loss (name: value) logppx: 6.68778471152\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=6.68778471152\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] patience losses:[6.721302469571431, 6.712437020407783, 6.7088889016045465, 6.7005230453279285, 6.692135956552294] min patience loss:6.69213595655 current loss:6.68778471152 absolute loss difference:0.00435124503242\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1116, \"sum\": 1116.0, \"min\": 1116}, \"Total Records Seen\": {\"count\": 1, \"max\": 140244, \"sum\": 140244.0, \"min\": 140244}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1613371594.680198, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1613371593.462527}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3714.1783108 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:34 INFO 140391001810112] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:31.427] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 1190, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] # Finished training epoch 29 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] Loss (name: value) total: 6.7256549067\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] Loss (name: value) kld: 0.0832043532282\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] Loss (name: value) recons: 6.64245055119\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] Loss (name: value) logppx: 6.7256549067\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] #quality_metric: host=algo-2, epoch=29, train total_loss <loss>=6.7256549067\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] patience losses:[6.756845057010651, 6.75236670838462, 6.7472108072704735, 6.7408305406570435, 6.732486075825161] min patience loss:6.73248607583 current loss:6.7256549067 absolute loss difference:0.00683116912842\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] #progress_metric: host=algo-2, completed 29 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1044, \"sum\": 1044.0, \"min\": 1044}, \"Total Records Seen\": {\"count\": 1, \"max\": 131283, \"sum\": 131283.0, \"min\": 131283}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1613371591.433776, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1613371590.236755}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3781.40196873 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:31 INFO 140255820981440] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:32.709] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 1274, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] # Finished training epoch 30 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] Loss (name: value) total: 6.71991222435\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] Loss (name: value) kld: 0.0868163551721\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] Loss (name: value) recons: 6.63309584724\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] Loss (name: value) logppx: 6.71991222435\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] #quality_metric: host=algo-2, epoch=30, train total_loss <loss>=6.71991222435\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] patience losses:[6.75236670838462, 6.7472108072704735, 6.7408305406570435, 6.732486075825161, 6.725654906696743] min patience loss:6.7256549067 current loss:6.71991222435 absolute loss difference:0.00574268235101\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] #progress_metric: host=algo-2, completed 30 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1080, \"sum\": 1080.0, \"min\": 1080}, \"Total Records Seen\": {\"count\": 1, \"max\": 135810, \"sum\": 135810.0, \"min\": 135810}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1613371592.714755, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1613371591.43408}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3534.45675945 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:32 INFO 140255820981440] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:33.983] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 1268, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] # Finished training epoch 31 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] Loss (name: value) total: 6.7144548032\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] Loss (name: value) kld: 0.0900118756625\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] Loss (name: value) recons: 6.62444298135\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] Loss (name: value) logppx: 6.7144548032\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] #quality_metric: host=algo-2, epoch=31, train total_loss <loss>=6.7144548032\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] patience losses:[6.7472108072704735, 6.7408305406570435, 6.732486075825161, 6.725654906696743, 6.719912224345737] min patience loss:6.71991222435 current loss:6.7144548032 absolute loss difference:0.00545742114385\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] #progress_metric: host=algo-2, completed 31 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1116, \"sum\": 1116.0, \"min\": 1116}, \"Total Records Seen\": {\"count\": 1, \"max\": 140337, \"sum\": 140337.0, \"min\": 140337}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1613371593.987374, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1613371592.715037}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3557.56707681 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:33 INFO 140255820981440] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:35.137] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 1149, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] # Finished training epoch 32 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] Loss (name: value) total: 6.70815340016\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] Loss (name: value) kld: 0.0954690906219\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] Loss (name: value) recons: 6.61268434922\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] Loss (name: value) logppx: 6.70815340016\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] #quality_metric: host=algo-2, epoch=32, train total_loss <loss>=6.70815340016\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] patience losses:[6.7408305406570435, 6.732486075825161, 6.725654906696743, 6.719912224345737, 6.714454803201887] min patience loss:6.7144548032 current loss:6.70815340016 absolute loss difference:0.00630140304565\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] Timing: train: 1.15s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] #progress_metric: host=algo-2, completed 32 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1152, \"sum\": 1152.0, \"min\": 1152}, \"Total Records Seen\": {\"count\": 1, \"max\": 144864, \"sum\": 144864.0, \"min\": 144864}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1613371595.144128, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1613371593.987694}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3913.32114494 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:35 INFO 140255820981440] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:35.916] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 1234, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] # Finished training epoch 32 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] Loss (name: value) total: 6.68159941832\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] Loss (name: value) kld: 0.095124031831\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] Loss (name: value) recons: 6.58647537894\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] Loss (name: value) logppx: 6.68159941832\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=6.68159941832\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] patience losses:[6.712437020407783, 6.7088889016045465, 6.7005230453279285, 6.692135956552294, 6.687784711519877] min patience loss:6.68778471152 current loss:6.68159941832 absolute loss difference:0.00618529319763\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1152, \"sum\": 1152.0, \"min\": 1152}, \"Total Records Seen\": {\"count\": 1, \"max\": 144768, \"sum\": 144768.0, \"min\": 144768}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1613371595.920651, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1613371594.680804}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3648.36883016 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:35 INFO 140391001810112] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:36.353] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 1207, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] # Finished training epoch 33 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] Loss (name: value) total: 6.69972872072\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] Loss (name: value) kld: 0.0959020006574\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] Loss (name: value) recons: 6.60382677449\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] Loss (name: value) logppx: 6.69972872072\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] #quality_metric: host=algo-2, epoch=33, train total_loss <loss>=6.69972872072\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] patience losses:[6.732486075825161, 6.725654906696743, 6.719912224345737, 6.714454803201887, 6.708153400156233] min patience loss:6.70815340016 current loss:6.69972872072 absolute loss difference:0.00842467943827\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] #progress_metric: host=algo-2, completed 33 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1188, \"sum\": 1188.0, \"min\": 1188}, \"Total Records Seen\": {\"count\": 1, \"max\": 149391, \"sum\": 149391.0, \"min\": 149391}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1613371596.358689, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1613371595.144679}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3728.45104885 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:36 INFO 140255820981440] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:37.133] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 1211, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] # Finished training epoch 33 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] Loss (name: value) total: 6.67810600334\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] Loss (name: value) kld: 0.0972601762041\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] Loss (name: value) recons: 6.58084581296\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] Loss (name: value) logppx: 6.67810600334\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=6.67810600334\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] patience losses:[6.7088889016045465, 6.7005230453279285, 6.692135956552294, 6.687784711519877, 6.681599418322246] min patience loss:6.68159941832 current loss:6.67810600334 absolute loss difference:0.00349341498481\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1188, \"sum\": 1188.0, \"min\": 1188}, \"Total Records Seen\": {\"count\": 1, \"max\": 149292, \"sum\": 149292.0, \"min\": 149292}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1613371597.137535, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1613371595.920969}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3717.47242384 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:37 INFO 140391001810112] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:37.555] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 1196, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] # Finished training epoch 34 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] Loss (name: value) total: 6.69887924857\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] Loss (name: value) kld: 0.0989756350302\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] Loss (name: value) recons: 6.59990358353\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] Loss (name: value) logppx: 6.69887924857\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] #quality_metric: host=algo-2, epoch=34, train total_loss <loss>=6.69887924857\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] patience losses:[6.725654906696743, 6.719912224345737, 6.714454803201887, 6.708153400156233, 6.69972872071796] min patience loss:6.69972872072 current loss:6.69887924857 absolute loss difference:0.000849472151863\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] Timing: train: 1.20s, val: 0.01s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] #progress_metric: host=algo-2, completed 34 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1224, \"sum\": 1224.0, \"min\": 1224}, \"Total Records Seen\": {\"count\": 1, \"max\": 153918, \"sum\": 153918.0, \"min\": 153918}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1613371597.562309, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1613371596.358989}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3761.58466174 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:37 INFO 140255820981440] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:38.788] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 1225, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] # Finished training epoch 35 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] Loss (name: value) total: 6.69212945302\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] Loss (name: value) kld: 0.102574584799\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] Loss (name: value) recons: 6.58955485953\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] Loss (name: value) logppx: 6.69212945302\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] #quality_metric: host=algo-2, epoch=35, train total_loss <loss>=6.69212945302\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] patience losses:[6.719912224345737, 6.714454803201887, 6.708153400156233, 6.69972872071796, 6.698879248566097] min patience loss:6.69887924857 current loss:6.69212945302 absolute loss difference:0.00674979554282\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] #progress_metric: host=algo-2, completed 35 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1260, \"sum\": 1260.0, \"min\": 1260}, \"Total Records Seen\": {\"count\": 1, \"max\": 158445, \"sum\": 158445.0, \"min\": 158445}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1613371598.791902, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1613371597.562626}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3682.11933039 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:38 INFO 140255820981440] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:38.416] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 1276, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] # Finished training epoch 34 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] Loss (name: value) total: 6.67530202203\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] Loss (name: value) kld: 0.0998712143757\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] Loss (name: value) recons: 6.57543083032\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] Loss (name: value) logppx: 6.67530202203\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=6.67530202203\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] patience losses:[6.7005230453279285, 6.692135956552294, 6.687784711519877, 6.681599418322246, 6.678106003337437] min patience loss:6.67810600334 current loss:6.67530202203 absolute loss difference:0.00280398130417\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1224, \"sum\": 1224.0, \"min\": 1224}, \"Total Records Seen\": {\"count\": 1, \"max\": 153816, \"sum\": 153816.0, \"min\": 153816}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1613371598.422393, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1613371597.138262}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3522.45947944 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:38 INFO 140391001810112] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:40.002] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 1209, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] # Finished training epoch 36 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] Loss (name: value) total: 6.68862244818\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] Loss (name: value) kld: 0.104776328119\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] Loss (name: value) recons: 6.58384616507\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] Loss (name: value) logppx: 6.68862244818\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] #quality_metric: host=algo-2, epoch=36, train total_loss <loss>=6.68862244818\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] patience losses:[6.714454803201887, 6.708153400156233, 6.69972872071796, 6.698879248566097, 6.692129453023274] min patience loss:6.69212945302 current loss:6.68862244818 absolute loss difference:0.00350700484382\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] #progress_metric: host=algo-2, completed 36 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1296, \"sum\": 1296.0, \"min\": 1296}, \"Total Records Seen\": {\"count\": 1, \"max\": 162972, \"sum\": 162972.0, \"min\": 162972}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1613371600.00772, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1613371598.792226}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3723.92760513 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:40 INFO 140255820981440] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:39.664] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 1240, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] # Finished training epoch 35 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] Loss (name: value) total: 6.66750160191\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] Loss (name: value) kld: 0.101310192504\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] Loss (name: value) recons: 6.56619143486\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] Loss (name: value) logppx: 6.66750160191\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=6.66750160191\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] patience losses:[6.692135956552294, 6.687784711519877, 6.681599418322246, 6.678106003337437, 6.675302022033268] min patience loss:6.67530202203 current loss:6.66750160191 absolute loss difference:0.00780042012533\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1260, \"sum\": 1260.0, \"min\": 1260}, \"Total Records Seen\": {\"count\": 1, \"max\": 158340, \"sum\": 158340.0, \"min\": 158340}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1613371599.669169, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1613371598.422695}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3628.12832023 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:39 INFO 140391001810112] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:41.236] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 1228, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] # Finished training epoch 37 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] Loss (name: value) total: 6.68442992369\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] Loss (name: value) kld: 0.109627647946\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] Loss (name: value) recons: 6.5748022\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] Loss (name: value) logppx: 6.68442992369\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] #quality_metric: host=algo-2, epoch=37, train total_loss <loss>=6.68442992369\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] patience losses:[6.708153400156233, 6.69972872071796, 6.698879248566097, 6.692129453023274, 6.688622448179457] min patience loss:6.68862244818 current loss:6.68442992369 absolute loss difference:0.00419252448612\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] #progress_metric: host=algo-2, completed 37 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1332, \"sum\": 1332.0, \"min\": 1332}, \"Total Records Seen\": {\"count\": 1, \"max\": 167499, \"sum\": 167499.0, \"min\": 167499}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1613371601.240478, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1613371600.00803}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3672.64696844 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:41 INFO 140255820981440] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:40.884] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 1213, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] # Finished training epoch 36 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] Loss (name: value) total: 6.66564425495\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] Loss (name: value) kld: 0.104765415399\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] Loss (name: value) recons: 6.56087887287\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] Loss (name: value) logppx: 6.66564425495\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=6.66564425495\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] patience losses:[6.687784711519877, 6.681599418322246, 6.678106003337437, 6.675302022033268, 6.667501601907942] min patience loss:6.66750160191 current loss:6.66564425495 absolute loss difference:0.00185734695858\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1296, \"sum\": 1296.0, \"min\": 1296}, \"Total Records Seen\": {\"count\": 1, \"max\": 162864, \"sum\": 162864.0, \"min\": 162864}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1613371600.889182, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1613371599.669892}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3709.78072466 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:40 INFO 140391001810112] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:42.107] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 1217, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] # Finished training epoch 37 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] Loss (name: value) total: 6.66488128238\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] Loss (name: value) kld: 0.106633055748\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] Loss (name: value) recons: 6.55824820201\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] Loss (name: value) logppx: 6.66488128238\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=6.66488128238\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] patience losses:[6.681599418322246, 6.678106003337437, 6.675302022033268, 6.667501601907942, 6.665644254949358] min patience loss:6.66564425495 current loss:6.66488128238 absolute loss difference:0.000762972566816\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1332, \"sum\": 1332.0, \"min\": 1332}, \"Total Records Seen\": {\"count\": 1, \"max\": 167388, \"sum\": 167388.0, \"min\": 167388}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1613371602.114065, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1613371600.889503}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3693.93286806 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:42 INFO 140391001810112] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:43.416] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 1300, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] # Finished training epoch 38 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] Loss (name: value) total: 6.65504974127\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] Loss (name: value) kld: 0.109461926131\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] Loss (name: value) recons: 6.5455878377\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] Loss (name: value) logppx: 6.65504974127\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=6.65504974127\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] patience losses:[6.678106003337437, 6.675302022033268, 6.667501601907942, 6.665644254949358, 6.664881282382542] min patience loss:6.66488128238 current loss:6.65504974127 absolute loss difference:0.00983154111438\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] Timing: train: 1.30s, val: 0.00s, epoch: 1.31s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1368, \"sum\": 1368.0, \"min\": 1368}, \"Total Records Seen\": {\"count\": 1, \"max\": 171912, \"sum\": 171912.0, \"min\": 171912}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1613371603.420363, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1613371602.11463}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3464.19503304 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:43 INFO 140391001810112] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:44.663] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 1242, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] # Finished training epoch 39 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] Loss (name: value) total: 6.65222056707\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] Loss (name: value) kld: 0.109663507798\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] Loss (name: value) recons: 6.54255706734\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] Loss (name: value) logppx: 6.65222056707\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=6.65222056707\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] patience losses:[6.675302022033268, 6.667501601907942, 6.665644254949358, 6.664881282382542, 6.655049741268158] min patience loss:6.65504974127 current loss:6.65222056707 absolute loss difference:0.00282917420069\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1404, \"sum\": 1404.0, \"min\": 1404}, \"Total Records Seen\": {\"count\": 1, \"max\": 176436, \"sum\": 176436.0, \"min\": 176436}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1613371604.66914, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 38}, \"StartTime\": 1613371603.420653}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3623.16809219 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:44 INFO 140391001810112] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:42.441] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 1200, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] # Finished training epoch 38 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] Loss (name: value) total: 6.67908580436\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] Loss (name: value) kld: 0.108631208332\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] Loss (name: value) recons: 6.57045459085\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] Loss (name: value) logppx: 6.67908580436\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] #quality_metric: host=algo-2, epoch=38, train total_loss <loss>=6.67908580436\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] patience losses:[6.69972872071796, 6.698879248566097, 6.692129453023274, 6.688622448179457, 6.684429923693339] min patience loss:6.68442992369 current loss:6.67908580436 absolute loss difference:0.00534411933687\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] Timing: train: 1.20s, val: 0.01s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] #progress_metric: host=algo-2, completed 38 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1368, \"sum\": 1368.0, \"min\": 1368}, \"Total Records Seen\": {\"count\": 1, \"max\": 172026, \"sum\": 172026.0, \"min\": 172026}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1613371602.447954, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1613371601.240813}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3749.618466 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:42 INFO 140255820981440] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:43.666] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 1217, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] # Finished training epoch 39 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] Loss (name: value) total: 6.68117274178\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] Loss (name: value) kld: 0.110022994172\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] Loss (name: value) recons: 6.57114978631\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] Loss (name: value) logppx: 6.68117274178\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] #quality_metric: host=algo-2, epoch=39, train total_loss <loss>=6.68117274178\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] patience losses:[6.698879248566097, 6.692129453023274, 6.688622448179457, 6.684429923693339, 6.679085804356469] min patience loss:6.67908580436 current loss:6.68117274178 absolute loss difference:0.00208693742752\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] #progress_metric: host=algo-2, completed 39 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1404, \"sum\": 1404.0, \"min\": 1404}, \"Total Records Seen\": {\"count\": 1, \"max\": 176553, \"sum\": 176553.0, \"min\": 176553}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1613371603.667251, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 38}, \"StartTime\": 1613371602.448276}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3713.31597523 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:43 INFO 140255820981440] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:44.877] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 1209, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] # Finished training epoch 40 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] Loss (name: value) total: 6.67808805572\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] Loss (name: value) kld: 0.115892533006\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] Loss (name: value) recons: 6.56219553285\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] Loss (name: value) logppx: 6.67808805572\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] #quality_metric: host=algo-2, epoch=40, train total_loss <loss>=6.67808805572\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] patience losses:[6.692129453023274, 6.688622448179457, 6.684429923693339, 6.679085804356469, 6.68117274178399] min patience loss:6.67908580436 current loss:6.67808805572 absolute loss difference:0.000997748639849\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] #progress_metric: host=algo-2, completed 40 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1440, \"sum\": 1440.0, \"min\": 1440}, \"Total Records Seen\": {\"count\": 1, \"max\": 181080, \"sum\": 181080.0, \"min\": 181080}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1613371604.884119, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 39}, \"StartTime\": 1613371603.667519}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3720.51779656 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:44 INFO 140255820981440] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:46.062] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 1177, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] # Finished training epoch 41 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] Loss (name: value) total: 6.67342868778\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] Loss (name: value) kld: 0.115805378701\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] Loss (name: value) recons: 6.55762335989\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] Loss (name: value) logppx: 6.67342868778\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] #quality_metric: host=algo-2, epoch=41, train total_loss <loss>=6.67342868778\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] patience losses:[6.688622448179457, 6.684429923693339, 6.679085804356469, 6.68117274178399, 6.6780880557166205] min patience loss:6.67808805572 current loss:6.67342868778 absolute loss difference:0.00465936793221\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] #progress_metric: host=algo-2, completed 41 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1476, \"sum\": 1476.0, \"min\": 1476}, \"Total Records Seen\": {\"count\": 1, \"max\": 185607, \"sum\": 185607.0, \"min\": 185607}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1613371606.068698, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 40}, \"StartTime\": 1613371604.884462}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3822.15894932 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:46 INFO 140255820981440] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:45.884] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 1212, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] # Finished training epoch 40 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] Loss (name: value) total: 6.65614558591\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] Loss (name: value) kld: 0.115834254151\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] Loss (name: value) recons: 6.54031132989\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] Loss (name: value) logppx: 6.65614558591\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=6.65614558591\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] patience losses:[6.667501601907942, 6.665644254949358, 6.664881282382542, 6.655049741268158, 6.652220567067464] min patience loss:6.65222056707 current loss:6.65614558591 absolute loss difference:0.00392501884037\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1440, \"sum\": 1440.0, \"min\": 1440}, \"Total Records Seen\": {\"count\": 1, \"max\": 180960, \"sum\": 180960.0, \"min\": 180960}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1613371605.885453, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 39}, \"StartTime\": 1613371604.669401}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3719.78622997 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:45 INFO 140391001810112] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:47.264] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 1193, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] # Finished training epoch 42 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] Loss (name: value) total: 6.66553709242\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] Loss (name: value) kld: 0.117151510384\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] Loss (name: value) recons: 6.548385587\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] Loss (name: value) logppx: 6.66553709242\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] #quality_metric: host=algo-2, epoch=42, train total_loss <loss>=6.66553709242\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] patience losses:[6.684429923693339, 6.679085804356469, 6.68117274178399, 6.6780880557166205, 6.673428687784407] min patience loss:6.67342868778 current loss:6.66553709242 absolute loss difference:0.00789159536362\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] #progress_metric: host=algo-2, completed 42 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1512, \"sum\": 1512.0, \"min\": 1512}, \"Total Records Seen\": {\"count\": 1, \"max\": 190134, \"sum\": 190134.0, \"min\": 190134}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1613371607.268713, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 41}, \"StartTime\": 1613371606.069021}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3773.02534266 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:47 INFO 140255820981440] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:47.076] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 1190, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] # Finished training epoch 41 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] Loss (name: value) total: 6.64309577147\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] Loss (name: value) kld: 0.114794012573\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] Loss (name: value) recons: 6.52830178208\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] Loss (name: value) logppx: 6.64309577147\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=6.64309577147\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] patience losses:[6.665644254949358, 6.664881282382542, 6.655049741268158, 6.652220567067464, 6.65614558590783] min patience loss:6.65222056707 current loss:6.64309577147 absolute loss difference:0.0091247955958\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1476, \"sum\": 1476.0, \"min\": 1476}, \"Total Records Seen\": {\"count\": 1, \"max\": 185484, \"sum\": 185484.0, \"min\": 185484}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1613371607.081365, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 40}, \"StartTime\": 1613371605.885713}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3782.66569065 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:47 INFO 140391001810112] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:48.344] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 1262, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] # Finished training epoch 42 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] Loss (name: value) total: 6.63736057944\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] Loss (name: value) kld: 0.116743724172\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] Loss (name: value) recons: 6.52061683602\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] Loss (name: value) logppx: 6.63736057944\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=6.63736057944\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] patience losses:[6.664881282382542, 6.655049741268158, 6.652220567067464, 6.65614558590783, 6.64309577147166] min patience loss:6.64309577147 current loss:6.63736057944 absolute loss difference:0.00573519203398\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] Timing: train: 1.26s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1512, \"sum\": 1512.0, \"min\": 1512}, \"Total Records Seen\": {\"count\": 1, \"max\": 190008, \"sum\": 190008.0, \"min\": 190008}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1613371608.349048, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 41}, \"StartTime\": 1613371607.081906}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3569.78294175 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:48 INFO 140391001810112] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:48.532] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 1262, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] # Finished training epoch 43 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] Loss (name: value) total: 6.66121551063\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] Loss (name: value) kld: 0.119446284552\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] Loss (name: value) recons: 6.541769286\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] Loss (name: value) logppx: 6.66121551063\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] #quality_metric: host=algo-2, epoch=43, train total_loss <loss>=6.66121551063\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] patience losses:[6.679085804356469, 6.68117274178399, 6.6780880557166205, 6.673428687784407, 6.66553709242079] min patience loss:6.66553709242 current loss:6.66121551063 absolute loss difference:0.00432158178753\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] Timing: train: 1.26s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] #progress_metric: host=algo-2, completed 43 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1548, \"sum\": 1548.0, \"min\": 1548}, \"Total Records Seen\": {\"count\": 1, \"max\": 194661, \"sum\": 194661.0, \"min\": 194661}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1613371608.53743, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 42}, \"StartTime\": 1613371607.269009}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3568.44977664 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:48 INFO 140255820981440] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:49.765] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 1225, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] # Finished training epoch 44 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] Loss (name: value) total: 6.65750374397\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] Loss (name: value) kld: 0.124025113156\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] Loss (name: value) recons: 6.53347863091\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] Loss (name: value) logppx: 6.65750374397\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] #quality_metric: host=algo-2, epoch=44, train total_loss <loss>=6.65750374397\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] patience losses:[6.68117274178399, 6.6780880557166205, 6.673428687784407, 6.66553709242079, 6.661215510633257] min patience loss:6.66121551063 current loss:6.65750374397 absolute loss difference:0.00371176666684\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] #progress_metric: host=algo-2, completed 44 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1584, \"sum\": 1584.0, \"min\": 1584}, \"Total Records Seen\": {\"count\": 1, \"max\": 199188, \"sum\": 199188.0, \"min\": 199188}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1613371609.769369, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 43}, \"StartTime\": 1613371608.537779}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3675.23171154 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:49 INFO 140255820981440] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:49.570] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 1220, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] # Finished training epoch 43 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] Loss (name: value) total: 6.6340619127\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] Loss (name: value) kld: 0.121354490519\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] Loss (name: value) recons: 6.51270742549\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] Loss (name: value) logppx: 6.6340619127\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=6.6340619127\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] patience losses:[6.655049741268158, 6.652220567067464, 6.65614558590783, 6.64309577147166, 6.637360579437679] min patience loss:6.63736057944 current loss:6.6340619127 absolute loss difference:0.00329866674211\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1548, \"sum\": 1548.0, \"min\": 1548}, \"Total Records Seen\": {\"count\": 1, \"max\": 194532, \"sum\": 194532.0, \"min\": 194532}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1613371609.577674, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 42}, \"StartTime\": 1613371608.349353}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3682.47712327 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:49 INFO 140391001810112] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:50.984] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 1214, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] # Finished training epoch 45 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] Loss (name: value) total: 6.65388822556\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] Loss (name: value) kld: 0.125104715944\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] Loss (name: value) recons: 6.52878356642\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] Loss (name: value) logppx: 6.65388822556\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] #quality_metric: host=algo-2, epoch=45, train total_loss <loss>=6.65388822556\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] patience losses:[6.6780880557166205, 6.673428687784407, 6.66553709242079, 6.661215510633257, 6.65750374396642] min patience loss:6.65750374397 current loss:6.65388822556 absolute loss difference:0.003615518411\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] #progress_metric: host=algo-2, completed 45 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1620, \"sum\": 1620.0, \"min\": 1620}, \"Total Records Seen\": {\"count\": 1, \"max\": 203715, \"sum\": 203715.0, \"min\": 203715}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1613371610.990433, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 44}, \"StartTime\": 1613371609.769865}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3708.4611796 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:50 INFO 140255820981440] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:50.812] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 1232, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] # Finished training epoch 44 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] Loss (name: value) total: 6.63400289085\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] Loss (name: value) kld: 0.121845973345\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] Loss (name: value) recons: 6.5121569501\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] Loss (name: value) logppx: 6.63400289085\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=6.63400289085\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] patience losses:[6.652220567067464, 6.65614558590783, 6.64309577147166, 6.637360579437679, 6.634061912695567] min patience loss:6.6340619127 current loss:6.63400289085 absolute loss difference:5.90218438044e-05\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1584, \"sum\": 1584.0, \"min\": 1584}, \"Total Records Seen\": {\"count\": 1, \"max\": 199056, \"sum\": 199056.0, \"min\": 199056}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1613371610.818338, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 43}, \"StartTime\": 1613371609.578083}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3647.16408636 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:50 INFO 140391001810112] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:52.200] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 1208, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] # Finished training epoch 46 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] Loss (name: value) total: 6.65040548642\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] Loss (name: value) kld: 0.129628588963\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] Loss (name: value) recons: 6.52077681488\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] Loss (name: value) logppx: 6.65040548642\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] #quality_metric: host=algo-2, epoch=46, train total_loss <loss>=6.65040548642\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] patience losses:[6.673428687784407, 6.66553709242079, 6.661215510633257, 6.65750374396642, 6.65388822555542] min patience loss:6.65388822556 current loss:6.65040548642 absolute loss difference:0.00348273913066\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] #progress_metric: host=algo-2, completed 46 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1656, \"sum\": 1656.0, \"min\": 1656}, \"Total Records Seen\": {\"count\": 1, \"max\": 208242, \"sum\": 208242.0, \"min\": 208242}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1613371612.205023, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 45}, \"StartTime\": 1613371610.99069}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3727.44757272 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:52 INFO 140255820981440] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:52.014] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 1192, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] # Finished training epoch 45 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] Loss (name: value) total: 6.62550126844\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] Loss (name: value) kld: 0.12297136678\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] Loss (name: value) recons: 6.50252989928\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] Loss (name: value) logppx: 6.62550126844\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=6.62550126844\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] patience losses:[6.65614558590783, 6.64309577147166, 6.637360579437679, 6.634061912695567, 6.634002890851763] min patience loss:6.63400289085 current loss:6.62550126844 absolute loss difference:0.00850162241194\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1620, \"sum\": 1620.0, \"min\": 1620}, \"Total Records Seen\": {\"count\": 1, \"max\": 203580, \"sum\": 203580.0, \"min\": 203580}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1613371612.018275, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 44}, \"StartTime\": 1613371610.818645}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3770.7790041 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:52 INFO 140391001810112] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:53.318] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 1299, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] # Finished training epoch 46 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] Loss (name: value) total: 6.62986816963\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] Loss (name: value) kld: 0.12819824792\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] Loss (name: value) recons: 6.50166993671\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] Loss (name: value) logppx: 6.62986816963\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=6.62986816963\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] patience losses:[6.64309577147166, 6.637360579437679, 6.634061912695567, 6.634002890851763, 6.625501268439823] min patience loss:6.62550126844 current loss:6.62986816963 absolute loss difference:0.00436690118578\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] Timing: train: 1.30s, val: 0.00s, epoch: 1.30s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1656, \"sum\": 1656.0, \"min\": 1656}, \"Total Records Seen\": {\"count\": 1, \"max\": 208104, \"sum\": 208104.0, \"min\": 208104}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1613371613.320907, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 45}, \"StartTime\": 1613371612.01854}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3473.09609597 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:53 INFO 140391001810112] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:54.514] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 1192, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] # Finished training epoch 47 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] Loss (name: value) total: 6.62745416164\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] Loss (name: value) kld: 0.130449454197\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] Loss (name: value) recons: 6.49700475401\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] Loss (name: value) logppx: 6.62745416164\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=6.62745416164\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] patience losses:[6.637360579437679, 6.634061912695567, 6.634002890851763, 6.625501268439823, 6.6298681696256] min patience loss:6.62550126844 current loss:6.62745416164 absolute loss difference:0.00195289320416\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1692, \"sum\": 1692.0, \"min\": 1692}, \"Total Records Seen\": {\"count\": 1, \"max\": 212628, \"sum\": 212628.0, \"min\": 212628}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1613371614.51594, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 46}, \"StartTime\": 1613371613.321287}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3786.36350586 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:54 INFO 140391001810112] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:53.434] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 1228, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] # Finished training epoch 47 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] Loss (name: value) total: 6.64764882459\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] Loss (name: value) kld: 0.129268943332\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] Loss (name: value) recons: 6.51837992006\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] Loss (name: value) logppx: 6.64764882459\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] #quality_metric: host=algo-2, epoch=47, train total_loss <loss>=6.64764882459\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] patience losses:[6.66553709242079, 6.661215510633257, 6.65750374396642, 6.65388822555542, 6.650405486424764] min patience loss:6.65040548642 current loss:6.64764882459 absolute loss difference:0.00275666183895\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] #progress_metric: host=algo-2, completed 47 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1692, \"sum\": 1692.0, \"min\": 1692}, \"Total Records Seen\": {\"count\": 1, \"max\": 212769, \"sum\": 212769.0, \"min\": 212769}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1613371613.439886, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 46}, \"StartTime\": 1613371612.205354}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3666.43885315 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:53 INFO 140255820981440] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:54.578] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 1137, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] # Finished training epoch 48 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] Loss (name: value) total: 6.64527791076\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] Loss (name: value) kld: 0.131939003348\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] Loss (name: value) recons: 6.51333895657\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] Loss (name: value) logppx: 6.64527791076\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] #quality_metric: host=algo-2, epoch=48, train total_loss <loss>=6.64527791076\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] patience losses:[6.661215510633257, 6.65750374396642, 6.65388822555542, 6.650405486424764, 6.647648824585809] min patience loss:6.64764882459 current loss:6.64527791076 absolute loss difference:0.00237091382345\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] Timing: train: 1.14s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] #progress_metric: host=algo-2, completed 48 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1728, \"sum\": 1728.0, \"min\": 1728}, \"Total Records Seen\": {\"count\": 1, \"max\": 217296, \"sum\": 217296.0, \"min\": 217296}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1613371614.582358, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 47}, \"StartTime\": 1613371613.440178}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3962.04022258 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:54 INFO 140255820981440] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:55.744] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 1161, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] # Finished training epoch 49 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] Loss (name: value) total: 6.63751865758\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] Loss (name: value) kld: 0.134428049852\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] Loss (name: value) recons: 6.50309062004\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] Loss (name: value) logppx: 6.63751865758\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] #quality_metric: host=algo-2, epoch=49, train total_loss <loss>=6.63751865758\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] patience losses:[6.65750374396642, 6.65388822555542, 6.650405486424764, 6.647648824585809, 6.645277910762363] min patience loss:6.64527791076 current loss:6.63751865758 absolute loss difference:0.007759253184\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] Timing: train: 1.16s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] #progress_metric: host=algo-2, completed 49 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1764, \"sum\": 1764.0, \"min\": 1764}, \"Total Records Seen\": {\"count\": 1, \"max\": 221823, \"sum\": 221823.0, \"min\": 221823}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 98, \"sum\": 98.0, \"min\": 98}}, \"EndTime\": 1613371615.750441, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 48}, \"StartTime\": 1613371614.583061}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3877.23649765 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:55 INFO 140255820981440] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:55.743] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 1226, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] # Finished training epoch 48 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] Loss (name: value) total: 6.61815985044\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] Loss (name: value) kld: 0.130263151497\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] Loss (name: value) recons: 6.48789678017\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] Loss (name: value) logppx: 6.61815985044\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=6.61815985044\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] patience losses:[6.634061912695567, 6.634002890851763, 6.625501268439823, 6.6298681696256, 6.627454161643982] min patience loss:6.62550126844 current loss:6.61815985044 absolute loss difference:0.00734141800139\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1728, \"sum\": 1728.0, \"min\": 1728}, \"Total Records Seen\": {\"count\": 1, \"max\": 217152, \"sum\": 217152.0, \"min\": 217152}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1613371615.748412, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 47}, \"StartTime\": 1613371614.516258}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3671.16323851 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:55 INFO 140391001810112] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:56.923] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 1172, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] # Finished training epoch 50 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] Loss (name: value) total: 6.63062062528\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] Loss (name: value) kld: 0.137811521172\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] Loss (name: value) recons: 6.49280914333\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] Loss (name: value) logppx: 6.63062062528\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] #quality_metric: host=algo-2, epoch=50, train total_loss <loss>=6.63062062528\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] patience losses:[6.65388822555542, 6.650405486424764, 6.647648824585809, 6.645277910762363, 6.6375186575783625] min patience loss:6.63751865758 current loss:6.63062062528 absolute loss difference:0.00689803229438\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] Timing: train: 1.17s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] #progress_metric: host=algo-2, completed 50 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1800, \"sum\": 1800.0, \"min\": 1800}, \"Total Records Seen\": {\"count\": 1, \"max\": 226350, \"sum\": 226350.0, \"min\": 226350}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}}, \"EndTime\": 1613371616.928054, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 49}, \"StartTime\": 1613371615.750786}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3844.61865115 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:56 INFO 140255820981440] # Starting training for epoch 51\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:56.943] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 1194, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] # Finished training epoch 49 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] Loss (name: value) total: 6.61150861449\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] Loss (name: value) kld: 0.133571208869\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] Loss (name: value) recons: 6.47793740696\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] Loss (name: value) logppx: 6.61150861449\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] #quality_metric: host=algo-1, epoch=49, train total_loss <loss>=6.61150861449\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] patience losses:[6.634002890851763, 6.625501268439823, 6.6298681696256, 6.627454161643982, 6.618159850438436] min patience loss:6.61815985044 current loss:6.61150861449 absolute loss difference:0.00665123595132\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1764, \"sum\": 1764.0, \"min\": 1764}, \"Total Records Seen\": {\"count\": 1, \"max\": 221676, \"sum\": 221676.0, \"min\": 221676}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 98, \"sum\": 98.0, \"min\": 98}}, \"EndTime\": 1613371616.948566, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 48}, \"StartTime\": 1613371615.748762}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3769.9466697 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:56 INFO 140391001810112] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:58.095] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 1166, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] # Finished training epoch 51 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] Loss (name: value) total: 6.63603901201\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] Loss (name: value) kld: 0.138070436608\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] Loss (name: value) recons: 6.49796857436\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] Loss (name: value) logppx: 6.63603901201\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] #quality_metric: host=algo-2, epoch=51, train total_loss <loss>=6.63603901201\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] patience losses:[6.650405486424764, 6.647648824585809, 6.645277910762363, 6.6375186575783625, 6.630620625283983] min patience loss:6.63062062528 current loss:6.63603901201 absolute loss difference:0.00541838672426\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] #progress_metric: host=algo-2, completed 51 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1836, \"sum\": 1836.0, \"min\": 1836}, \"Total Records Seen\": {\"count\": 1, \"max\": 230877, \"sum\": 230877.0, \"min\": 230877}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 102, \"sum\": 102.0, \"min\": 102}}, \"EndTime\": 1613371618.097309, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 50}, \"StartTime\": 1613371616.928436}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3872.46747922 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:58 INFO 140255820981440] # Starting training for epoch 52\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:58.160] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 1211, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] # Finished training epoch 50 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] Loss (name: value) total: 6.61157355044\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] Loss (name: value) kld: 0.135401673201\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] Loss (name: value) recons: 6.47617187765\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] Loss (name: value) logppx: 6.61157355044\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] #quality_metric: host=algo-1, epoch=50, train total_loss <loss>=6.61157355044\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] patience losses:[6.625501268439823, 6.6298681696256, 6.627454161643982, 6.618159850438436, 6.611508614487118] min patience loss:6.61150861449 current loss:6.61157355044 absolute loss difference:6.49359491138e-05\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1800, \"sum\": 1800.0, \"min\": 1800}, \"Total Records Seen\": {\"count\": 1, \"max\": 226200, \"sum\": 226200.0, \"min\": 226200}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}}, \"EndTime\": 1613371618.163293, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 49}, \"StartTime\": 1613371616.948881}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3724.75002424 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:58 INFO 140391001810112] # Starting training for epoch 51\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:46:59.277] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 155, \"duration\": 1177, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] # Finished training epoch 52 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] Loss (name: value) total: 6.62576881382\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] Loss (name: value) kld: 0.140018766125\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] Loss (name: value) recons: 6.4857500063\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] Loss (name: value) logppx: 6.62576881382\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] #quality_metric: host=algo-2, epoch=52, train total_loss <loss>=6.62576881382\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] patience losses:[6.647648824585809, 6.645277910762363, 6.6375186575783625, 6.630620625283983, 6.6360390120082435] min patience loss:6.63062062528 current loss:6.62576881382 absolute loss difference:0.00485181146198\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] #progress_metric: host=algo-2, completed 52 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1872, \"sum\": 1872.0, \"min\": 1872}, \"Total Records Seen\": {\"count\": 1, \"max\": 235404, \"sum\": 235404.0, \"min\": 235404}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 104, \"sum\": 104.0, \"min\": 104}}, \"EndTime\": 1613371619.281343, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 51}, \"StartTime\": 1613371618.097642}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3823.87392157 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:46:59 INFO 140255820981440] # Starting training for epoch 53\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:46:59.370] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 1205, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] # Finished training epoch 51 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] Loss (name: value) total: 6.60911530919\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] Loss (name: value) kld: 0.13818564028\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] Loss (name: value) recons: 6.47092960278\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] Loss (name: value) logppx: 6.60911530919\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] #quality_metric: host=algo-1, epoch=51, train total_loss <loss>=6.60911530919\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] patience losses:[6.6298681696256, 6.627454161643982, 6.618159850438436, 6.611508614487118, 6.611573550436232] min patience loss:6.61150861449 current loss:6.60911530919 absolute loss difference:0.00239330530167\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1836, \"sum\": 1836.0, \"min\": 1836}, \"Total Records Seen\": {\"count\": 1, \"max\": 230724, \"sum\": 230724.0, \"min\": 230724}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 102, \"sum\": 102.0, \"min\": 102}}, \"EndTime\": 1613371619.377214, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 50}, \"StartTime\": 1613371618.16374}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3726.65859523 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:46:59 INFO 140391001810112] # Starting training for epoch 52\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:00.462] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 1179, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] # Finished training epoch 53 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] Loss (name: value) total: 6.63138024012\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] Loss (name: value) kld: 0.142745357007\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] Loss (name: value) recons: 6.48863487111\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] Loss (name: value) logppx: 6.63138024012\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] #quality_metric: host=algo-2, epoch=53, train total_loss <loss>=6.63138024012\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] patience losses:[6.645277910762363, 6.6375186575783625, 6.630620625283983, 6.6360390120082435, 6.625768813822004] min patience loss:6.62576881382 current loss:6.63138024012 absolute loss difference:0.00561142630047\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] #progress_metric: host=algo-2, completed 53 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1908, \"sum\": 1908.0, \"min\": 1908}, \"Total Records Seen\": {\"count\": 1, \"max\": 239931, \"sum\": 239931.0, \"min\": 239931}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 106, \"sum\": 106.0, \"min\": 106}}, \"EndTime\": 1613371620.465403, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 52}, \"StartTime\": 1613371619.281698}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3823.24717791 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:00 INFO 140255820981440] # Starting training for epoch 54\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:00.638] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 155, \"duration\": 1259, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] # Finished training epoch 52 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] Loss (name: value) total: 6.60484227207\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] Loss (name: value) kld: 0.139874304645\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] Loss (name: value) recons: 6.46496794621\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] Loss (name: value) logppx: 6.60484227207\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] #quality_metric: host=algo-1, epoch=52, train total_loss <loss>=6.60484227207\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] patience losses:[6.627454161643982, 6.618159850438436, 6.611508614487118, 6.611573550436232, 6.6091153091854515] min patience loss:6.60911530919 current loss:6.60484227207 absolute loss difference:0.00427303711573\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1872, \"sum\": 1872.0, \"min\": 1872}, \"Total Records Seen\": {\"count\": 1, \"max\": 235248, \"sum\": 235248.0, \"min\": 235248}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 104, \"sum\": 104.0, \"min\": 104}}, \"EndTime\": 1613371620.642532, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 51}, \"StartTime\": 1613371619.378629}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3578.96423079 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:00 INFO 140391001810112] # Starting training for epoch 53\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:01.839] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 161, \"duration\": 1372, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] # Finished training epoch 54 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] Loss (name: value) total: 6.62374919653\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] Loss (name: value) kld: 0.145636420593\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] Loss (name: value) recons: 6.47811273734\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] Loss (name: value) logppx: 6.62374919653\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] #quality_metric: host=algo-2, epoch=54, train total_loss <loss>=6.62374919653\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] patience losses:[6.6375186575783625, 6.630620625283983, 6.6360390120082435, 6.625768813822004, 6.6313802401224775] min patience loss:6.62576881382 current loss:6.62374919653 absolute loss difference:0.00201961729262\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] Timing: train: 1.37s, val: 0.00s, epoch: 1.38s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] #progress_metric: host=algo-2, completed 54 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1944, \"sum\": 1944.0, \"min\": 1944}, \"Total Records Seen\": {\"count\": 1, \"max\": 244458, \"sum\": 244458.0, \"min\": 244458}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}}, \"EndTime\": 1613371621.842801, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 53}, \"StartTime\": 1613371620.466011}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3287.63384559 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:01 INFO 140255820981440] # Starting training for epoch 55\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:02.068] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 1423, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] # Finished training epoch 53 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] Loss (name: value) total: 6.59986482726\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] Loss (name: value) kld: 0.143803414061\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] Loss (name: value) recons: 6.45606134335\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] Loss (name: value) logppx: 6.59986482726\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] #quality_metric: host=algo-1, epoch=53, train total_loss <loss>=6.59986482726\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] patience losses:[6.618159850438436, 6.611508614487118, 6.611573550436232, 6.6091153091854515, 6.604842272069719] min patience loss:6.60484227207 current loss:6.59986482726 absolute loss difference:0.00497744480769\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] Timing: train: 1.43s, val: 0.00s, epoch: 1.43s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1908, \"sum\": 1908.0, \"min\": 1908}, \"Total Records Seen\": {\"count\": 1, \"max\": 239772, \"sum\": 239772.0, \"min\": 239772}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 106, \"sum\": 106.0, \"min\": 106}}, \"EndTime\": 1613371622.073027, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 52}, \"StartTime\": 1613371620.642802}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3162.86894593 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:02 INFO 140391001810112] # Starting training for epoch 54\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:03.167] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 1323, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] # Finished training epoch 55 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] Loss (name: value) total: 6.62703206804\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] Loss (name: value) kld: 0.148025092048\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] Loss (name: value) recons: 6.47900697258\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] Loss (name: value) logppx: 6.62703206804\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] #quality_metric: host=algo-2, epoch=55, train total_loss <loss>=6.62703206804\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] patience losses:[6.630620625283983, 6.6360390120082435, 6.625768813822004, 6.6313802401224775, 6.623749196529388] min patience loss:6.62374919653 current loss:6.62703206804 absolute loss difference:0.00328287151125\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] Timing: train: 1.33s, val: 0.00s, epoch: 1.33s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] #progress_metric: host=algo-2, completed 55 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1980, \"sum\": 1980.0, \"min\": 1980}, \"Total Records Seen\": {\"count\": 1, \"max\": 248985, \"sum\": 248985.0, \"min\": 248985}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 110, \"sum\": 110.0, \"min\": 110}}, \"EndTime\": 1613371623.168803, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 54}, \"StartTime\": 1613371621.843161}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3414.54231256 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:03 INFO 140255820981440] # Starting training for epoch 56\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:03.494] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 161, \"duration\": 1418, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] # Finished training epoch 54 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] Loss (name: value) total: 6.60288197464\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] Loss (name: value) kld: 0.145641380818\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] Loss (name: value) recons: 6.45724064112\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] Loss (name: value) logppx: 6.60288197464\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] #quality_metric: host=algo-1, epoch=54, train total_loss <loss>=6.60288197464\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] patience losses:[6.611508614487118, 6.611573550436232, 6.6091153091854515, 6.604842272069719, 6.599864827262031] min patience loss:6.59986482726 current loss:6.60288197464 absolute loss difference:0.0030171473821\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] Timing: train: 1.42s, val: 0.00s, epoch: 1.42s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1944, \"sum\": 1944.0, \"min\": 1944}, \"Total Records Seen\": {\"count\": 1, \"max\": 244296, \"sum\": 244296.0, \"min\": 244296}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}}, \"EndTime\": 1613371623.497797, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 53}, \"StartTime\": 1613371622.073272}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3174.83893613 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:03 INFO 140391001810112] # Starting training for epoch 55\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:05.233] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 1717, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] # Finished training epoch 55 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] Loss (name: value) total: 6.59113548199\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] Loss (name: value) kld: 0.143810033695\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] Loss (name: value) recons: 6.44732546144\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] Loss (name: value) logppx: 6.59113548199\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] #quality_metric: host=algo-1, epoch=55, train total_loss <loss>=6.59113548199\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] patience losses:[6.611573550436232, 6.6091153091854515, 6.604842272069719, 6.599864827262031, 6.602881974644131] min patience loss:6.59986482726 current loss:6.59113548199 absolute loss difference:0.00872934526867\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] Timing: train: 1.74s, val: 0.00s, epoch: 1.74s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1980, \"sum\": 1980.0, \"min\": 1980}, \"Total Records Seen\": {\"count\": 1, \"max\": 248820, \"sum\": 248820.0, \"min\": 248820}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 110, \"sum\": 110.0, \"min\": 110}}, \"EndTime\": 1613371625.237839, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 54}, \"StartTime\": 1613371623.498506}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=2600.73836122 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:05 INFO 140391001810112] # Starting training for epoch 56\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:04.704] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 167, \"duration\": 1534, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] # Finished training epoch 56 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] Loss (name: value) total: 6.61755678389\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] Loss (name: value) kld: 0.148954737621\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] Loss (name: value) recons: 6.46860200829\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] Loss (name: value) logppx: 6.61755678389\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] #quality_metric: host=algo-2, epoch=56, train total_loss <loss>=6.61755678389\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] patience losses:[6.6360390120082435, 6.625768813822004, 6.6313802401224775, 6.623749196529388, 6.627032068040636] min patience loss:6.62374919653 current loss:6.61755678389 absolute loss difference:0.00619241264131\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] Timing: train: 1.54s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] #progress_metric: host=algo-2, completed 56 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2016, \"sum\": 2016.0, \"min\": 2016}, \"Total Records Seen\": {\"count\": 1, \"max\": 253512, \"sum\": 253512.0, \"min\": 253512}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 112, \"sum\": 112.0, \"min\": 112}}, \"EndTime\": 1613371624.709214, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 55}, \"StartTime\": 1613371623.169095}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=2939.04942821 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:04 INFO 140255820981440] # Starting training for epoch 57\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:06.067] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 1355, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] # Finished training epoch 57 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] Loss (name: value) total: 6.61490730445\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] Loss (name: value) kld: 0.149526943349\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] Loss (name: value) recons: 6.46538029777\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] Loss (name: value) logppx: 6.61490730445\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] #quality_metric: host=algo-2, epoch=57, train total_loss <loss>=6.61490730445\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] patience losses:[6.625768813822004, 6.6313802401224775, 6.623749196529388, 6.627032068040636, 6.617556783888075] min patience loss:6.61755678389 current loss:6.61490730445 absolute loss difference:0.00264947944217\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] Timing: train: 1.36s, val: 0.00s, epoch: 1.36s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] #progress_metric: host=algo-2, completed 57 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2052, \"sum\": 2052.0, \"min\": 2052}, \"Total Records Seen\": {\"count\": 1, \"max\": 258039, \"sum\": 258039.0, \"min\": 258039}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 114, \"sum\": 114.0, \"min\": 114}}, \"EndTime\": 1613371626.072855, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 56}, \"StartTime\": 1613371624.709538}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3320.13927243 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:06 INFO 140255820981440] # Starting training for epoch 58\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:06.521] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 167, \"duration\": 1279, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] # Finished training epoch 56 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] Loss (name: value) total: 6.5873388052\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] Loss (name: value) kld: 0.147081639721\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] Loss (name: value) recons: 6.44025717841\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] Loss (name: value) logppx: 6.5873388052\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] #quality_metric: host=algo-1, epoch=56, train total_loss <loss>=6.5873388052\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] patience losses:[6.6091153091854515, 6.604842272069719, 6.599864827262031, 6.602881974644131, 6.591135481993358] min patience loss:6.59113548199 current loss:6.5873388052 absolute loss difference:0.00379667679469\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] Timing: train: 1.28s, val: 0.00s, epoch: 1.29s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2016, \"sum\": 2016.0, \"min\": 2016}, \"Total Records Seen\": {\"count\": 1, \"max\": 253344, \"sum\": 253344.0, \"min\": 253344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 112, \"sum\": 112.0, \"min\": 112}}, \"EndTime\": 1613371626.525657, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 55}, \"StartTime\": 1613371625.238142}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3512.70881997 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:06 INFO 140391001810112] # Starting training for epoch 57\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:07.449] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 173, \"duration\": 1375, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] # Finished training epoch 58 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] Loss (name: value) total: 6.60936474138\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] Loss (name: value) kld: 0.151639424471\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] Loss (name: value) recons: 6.45772531297\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] Loss (name: value) logppx: 6.60936474138\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] #quality_metric: host=algo-2, epoch=58, train total_loss <loss>=6.60936474138\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] patience losses:[6.6313802401224775, 6.623749196529388, 6.627032068040636, 6.617556783888075, 6.614907304445903] min patience loss:6.61490730445 current loss:6.60936474138 absolute loss difference:0.00554256306754\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] Timing: train: 1.38s, val: 0.00s, epoch: 1.38s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] #progress_metric: host=algo-2, completed 58 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2088, \"sum\": 2088.0, \"min\": 2088}, \"Total Records Seen\": {\"count\": 1, \"max\": 262566, \"sum\": 262566.0, \"min\": 262566}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 116, \"sum\": 116.0, \"min\": 116}}, \"EndTime\": 1613371627.452987, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 57}, \"StartTime\": 1613371626.073189}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3280.51782509 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:07 INFO 140255820981440] # Starting training for epoch 59\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:07.692] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 1165, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] # Finished training epoch 57 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] Loss (name: value) total: 6.58797366752\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] Loss (name: value) kld: 0.148107052367\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] Loss (name: value) recons: 6.43986664216\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] Loss (name: value) logppx: 6.58797366752\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] #quality_metric: host=algo-1, epoch=57, train total_loss <loss>=6.58797366752\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] patience losses:[6.604842272069719, 6.599864827262031, 6.602881974644131, 6.591135481993358, 6.587338805198669] min patience loss:6.5873388052 current loss:6.58797366752 absolute loss difference:0.000634862316979\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2052, \"sum\": 2052.0, \"min\": 2052}, \"Total Records Seen\": {\"count\": 1, \"max\": 257868, \"sum\": 257868.0, \"min\": 257868}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 114, \"sum\": 114.0, \"min\": 114}}, \"EndTime\": 1613371627.694063, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 56}, \"StartTime\": 1613371626.526285}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3873.31381935 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:07 INFO 140391001810112] # Starting training for epoch 58\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:08.619] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 1162, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] # Finished training epoch 59 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] Loss (name: value) total: 6.61078577571\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] Loss (name: value) kld: 0.155384612373\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] Loss (name: value) recons: 6.45540114244\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] Loss (name: value) logppx: 6.61078577571\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] #quality_metric: host=algo-2, epoch=59, train total_loss <loss>=6.61078577571\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] patience losses:[6.623749196529388, 6.627032068040636, 6.617556783888075, 6.614907304445903, 6.609364741378361] min patience loss:6.60936474138 current loss:6.61078577571 absolute loss difference:0.00142103433609\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] #progress_metric: host=algo-2, completed 59 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2124, \"sum\": 2124.0, \"min\": 2124}, \"Total Records Seen\": {\"count\": 1, \"max\": 267093, \"sum\": 267093.0, \"min\": 267093}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 118, \"sum\": 118.0, \"min\": 118}}, \"EndTime\": 1613371628.620987, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 58}, \"StartTime\": 1613371627.453324}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3876.41723575 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:08 INFO 140255820981440] # Starting training for epoch 60\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:08.916] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 173, \"duration\": 1221, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] # Finished training epoch 58 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] Loss (name: value) total: 6.59114534325\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] Loss (name: value) kld: 0.154239282943\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] Loss (name: value) recons: 6.43690603971\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] Loss (name: value) logppx: 6.59114534325\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] #quality_metric: host=algo-1, epoch=58, train total_loss <loss>=6.59114534325\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] patience losses:[6.599864827262031, 6.602881974644131, 6.591135481993358, 6.587338805198669, 6.587973667515649] min patience loss:6.5873388052 current loss:6.59114534325 absolute loss difference:0.00380653805203\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2088, \"sum\": 2088.0, \"min\": 2088}, \"Total Records Seen\": {\"count\": 1, \"max\": 262392, \"sum\": 262392.0, \"min\": 262392}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 116, \"sum\": 116.0, \"min\": 116}}, \"EndTime\": 1613371628.917943, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 57}, \"StartTime\": 1613371627.69449}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3697.25382585 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:08 INFO 140391001810112] # Starting training for epoch 59\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:09.887] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 179, \"duration\": 1265, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] # Finished training epoch 60 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] Loss (name: value) total: 6.60761876901\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] Loss (name: value) kld: 0.155880553441\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] Loss (name: value) recons: 6.45173818535\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] Loss (name: value) logppx: 6.60761876901\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] #quality_metric: host=algo-2, epoch=60, train total_loss <loss>=6.60761876901\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] patience losses:[6.627032068040636, 6.617556783888075, 6.614907304445903, 6.609364741378361, 6.610785775714451] min patience loss:6.60936474138 current loss:6.60761876901 absolute loss difference:0.00174597236845\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] #progress_metric: host=algo-2, completed 60 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2160, \"sum\": 2160.0, \"min\": 2160}, \"Total Records Seen\": {\"count\": 1, \"max\": 271620, \"sum\": 271620.0, \"min\": 271620}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 120, \"sum\": 120.0, \"min\": 120}}, \"EndTime\": 1613371629.892854, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 59}, \"StartTime\": 1613371628.621294}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3559.73469951 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:09 INFO 140255820981440] # Starting training for epoch 61\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:10.154] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 1235, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] # Finished training epoch 59 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] Loss (name: value) total: 6.5790307853\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] Loss (name: value) kld: 0.15227524574\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] Loss (name: value) recons: 6.42675556739\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] Loss (name: value) logppx: 6.5790307853\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] #quality_metric: host=algo-1, epoch=59, train total_loss <loss>=6.5790307853\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] patience losses:[6.602881974644131, 6.591135481993358, 6.587338805198669, 6.587973667515649, 6.591145343250698] min patience loss:6.5873388052 current loss:6.5790307853 absolute loss difference:0.00830801990297\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2124, \"sum\": 2124.0, \"min\": 2124}, \"Total Records Seen\": {\"count\": 1, \"max\": 266916, \"sum\": 266916.0, \"min\": 266916}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 118, \"sum\": 118.0, \"min\": 118}}, \"EndTime\": 1613371630.158433, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 58}, \"StartTime\": 1613371628.918204}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3647.25732391 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:10 INFO 140391001810112] # Starting training for epoch 60\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:11.092] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 1199, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] # Finished training epoch 61 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] Loss (name: value) total: 6.61113621129\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] Loss (name: value) kld: 0.159842056326\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] Loss (name: value) recons: 6.45129421022\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] Loss (name: value) logppx: 6.61113621129\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] #quality_metric: host=algo-2, epoch=61, train total_loss <loss>=6.61113621129\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] patience losses:[6.617556783888075, 6.614907304445903, 6.609364741378361, 6.610785775714451, 6.607618769009908] min patience loss:6.60761876901 current loss:6.61113621129 absolute loss difference:0.00351744227939\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] #progress_metric: host=algo-2, completed 61 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2196, \"sum\": 2196.0, \"min\": 2196}, \"Total Records Seen\": {\"count\": 1, \"max\": 276147, \"sum\": 276147.0, \"min\": 276147}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 122, \"sum\": 122.0, \"min\": 122}}, \"EndTime\": 1613371631.094535, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 60}, \"StartTime\": 1613371629.893153}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3767.63644229 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:11 INFO 140255820981440] # Starting training for epoch 62\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:11.359] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 179, \"duration\": 1198, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] # Finished training epoch 60 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] Loss (name: value) total: 6.57593089342\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] Loss (name: value) kld: 0.156514027363\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] Loss (name: value) recons: 6.41941685809\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] Loss (name: value) logppx: 6.57593089342\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] #quality_metric: host=algo-1, epoch=60, train total_loss <loss>=6.57593089342\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] patience losses:[6.591135481993358, 6.587338805198669, 6.587973667515649, 6.591145343250698, 6.579030785295698] min patience loss:6.5790307853 current loss:6.57593089342 absolute loss difference:0.00309989187453\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2160, \"sum\": 2160.0, \"min\": 2160}, \"Total Records Seen\": {\"count\": 1, \"max\": 271440, \"sum\": 271440.0, \"min\": 271440}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 120, \"sum\": 120.0, \"min\": 120}}, \"EndTime\": 1613371631.362821, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 59}, \"StartTime\": 1613371630.158744}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3756.4270026 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:11 INFO 140391001810112] # Starting training for epoch 61\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:12.218] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 185, \"duration\": 1122, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] # Finished training epoch 62 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] Loss (name: value) total: 6.59721154637\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] Loss (name: value) kld: 0.158914042637\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] Loss (name: value) recons: 6.43829755651\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] Loss (name: value) logppx: 6.59721154637\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] #quality_metric: host=algo-2, epoch=62, train total_loss <loss>=6.59721154637\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] patience losses:[6.614907304445903, 6.609364741378361, 6.610785775714451, 6.607618769009908, 6.6111362112893] min patience loss:6.60761876901 current loss:6.59721154637 absolute loss difference:0.0104072226418\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] Timing: train: 1.12s, val: 0.00s, epoch: 1.13s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] #progress_metric: host=algo-2, completed 62 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2232, \"sum\": 2232.0, \"min\": 2232}, \"Total Records Seen\": {\"count\": 1, \"max\": 280674, \"sum\": 280674.0, \"min\": 280674}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}}, \"EndTime\": 1613371632.222929, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 61}, \"StartTime\": 1613371631.09485}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=4012.17415911 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:12 INFO 140255820981440] # Starting training for epoch 63\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:12.635] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 1271, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] # Finished training epoch 61 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] Loss (name: value) total: 6.58136773109\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] Loss (name: value) kld: 0.15945437654\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] Loss (name: value) recons: 6.42191333903\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] Loss (name: value) logppx: 6.58136773109\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] #quality_metric: host=algo-1, epoch=61, train total_loss <loss>=6.58136773109\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] patience losses:[6.587338805198669, 6.587973667515649, 6.591145343250698, 6.579030785295698, 6.575930893421173] min patience loss:6.57593089342 current loss:6.58136773109 absolute loss difference:0.00543683767319\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2196, \"sum\": 2196.0, \"min\": 2196}, \"Total Records Seen\": {\"count\": 1, \"max\": 275964, \"sum\": 275964.0, \"min\": 275964}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 122, \"sum\": 122.0, \"min\": 122}}, \"EndTime\": 1613371632.636991, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 60}, \"StartTime\": 1613371631.36349}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3551.53570134 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:12 INFO 140391001810112] # Starting training for epoch 62\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:13.457] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 1233, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] # Finished training epoch 63 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] Loss (name: value) total: 6.60111859772\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] Loss (name: value) kld: 0.162158664626\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] Loss (name: value) recons: 6.43895989656\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] Loss (name: value) logppx: 6.60111859772\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] #quality_metric: host=algo-2, epoch=63, train total_loss <loss>=6.60111859772\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] patience losses:[6.609364741378361, 6.610785775714451, 6.607618769009908, 6.6111362112893, 6.597211546368069] min patience loss:6.59721154637 current loss:6.60111859772 absolute loss difference:0.00390705135134\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] #progress_metric: host=algo-2, completed 63 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2268, \"sum\": 2268.0, \"min\": 2268}, \"Total Records Seen\": {\"count\": 1, \"max\": 285201, \"sum\": 285201.0, \"min\": 285201}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 126, \"sum\": 126.0, \"min\": 126}}, \"EndTime\": 1613371633.458459, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 62}, \"StartTime\": 1613371632.223317}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3664.64359768 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:13 INFO 140255820981440] # Starting training for epoch 64\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:13.871] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 185, \"duration\": 1234, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] # Finished training epoch 62 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] Loss (name: value) total: 6.57718485594\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] Loss (name: value) kld: 0.160212023494\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] Loss (name: value) recons: 6.41697281599\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] Loss (name: value) logppx: 6.57718485594\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] #quality_metric: host=algo-1, epoch=62, train total_loss <loss>=6.57718485594\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] patience losses:[6.587973667515649, 6.591145343250698, 6.579030785295698, 6.575930893421173, 6.58136773109436] min patience loss:6.57593089342 current loss:6.57718485594 absolute loss difference:0.00125396251678\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2232, \"sum\": 2232.0, \"min\": 2232}, \"Total Records Seen\": {\"count\": 1, \"max\": 280488, \"sum\": 280488.0, \"min\": 280488}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}}, \"EndTime\": 1613371633.874518, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 61}, \"StartTime\": 1613371632.637307}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3656.14443124 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:13 INFO 140391001810112] # Starting training for epoch 63\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:14.684] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 191, \"duration\": 1224, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] # Finished training epoch 64 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] Loss (name: value) total: 6.59486434857\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] Loss (name: value) kld: 0.165137162225\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] Loss (name: value) recons: 6.42972722318\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] Loss (name: value) logppx: 6.59486434857\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] #quality_metric: host=algo-2, epoch=64, train total_loss <loss>=6.59486434857\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] patience losses:[6.610785775714451, 6.607618769009908, 6.6111362112893, 6.597211546368069, 6.601118597719404] min patience loss:6.59721154637 current loss:6.59486434857 absolute loss difference:0.00234719779756\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] #progress_metric: host=algo-2, completed 64 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2304, \"sum\": 2304.0, \"min\": 2304}, \"Total Records Seen\": {\"count\": 1, \"max\": 289728, \"sum\": 289728.0, \"min\": 289728}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 128, \"sum\": 128.0, \"min\": 128}}, \"EndTime\": 1613371634.688234, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 63}, \"StartTime\": 1613371633.458798}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3681.72093664 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:14 INFO 140255820981440] # Starting training for epoch 65\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:15.100] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 1213, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] # Finished training epoch 63 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] Loss (name: value) total: 6.57087919447\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] Loss (name: value) kld: 0.159803857303\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] Loss (name: value) recons: 6.41107532713\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] Loss (name: value) logppx: 6.57087919447\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] #quality_metric: host=algo-1, epoch=63, train total_loss <loss>=6.57087919447\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] patience losses:[6.591145343250698, 6.579030785295698, 6.575930893421173, 6.58136773109436, 6.577184855937958] min patience loss:6.57593089342 current loss:6.57087919447 absolute loss difference:0.0050516989496\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2268, \"sum\": 2268.0, \"min\": 2268}, \"Total Records Seen\": {\"count\": 1, \"max\": 285012, \"sum\": 285012.0, \"min\": 285012}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 126, \"sum\": 126.0, \"min\": 126}}, \"EndTime\": 1613371635.107688, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 62}, \"StartTime\": 1613371633.874901}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3669.29049594 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:15 INFO 140391001810112] # Starting training for epoch 64\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:15.867] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 1178, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] # Finished training epoch 65 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] Loss (name: value) total: 6.6002163291\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] Loss (name: value) kld: 0.168585914808\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] Loss (name: value) recons: 6.43163041936\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] Loss (name: value) logppx: 6.6002163291\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] #quality_metric: host=algo-2, epoch=65, train total_loss <loss>=6.6002163291\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] patience losses:[6.607618769009908, 6.6111362112893, 6.597211546368069, 6.601118597719404, 6.594864348570506] min patience loss:6.59486434857 current loss:6.6002163291 absolute loss difference:0.00535198052724\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] #progress_metric: host=algo-2, completed 65 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2340, \"sum\": 2340.0, \"min\": 2340}, \"Total Records Seen\": {\"count\": 1, \"max\": 294255, \"sum\": 294255.0, \"min\": 294255}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 130, \"sum\": 130.0, \"min\": 130}}, \"EndTime\": 1613371635.870065, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 64}, \"StartTime\": 1613371634.688526}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3830.88216348 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:15 INFO 140255820981440] # Starting training for epoch 66\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:16.333] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 191, \"duration\": 1223, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] # Finished training epoch 64 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] Loss (name: value) total: 6.56834328837\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] Loss (name: value) kld: 0.164907613459\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] Loss (name: value) recons: 6.403435621\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] Loss (name: value) logppx: 6.56834328837\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] #quality_metric: host=algo-1, epoch=64, train total_loss <loss>=6.56834328837\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] patience losses:[6.579030785295698, 6.575930893421173, 6.58136773109436, 6.577184855937958, 6.570879194471571] min patience loss:6.57087919447 current loss:6.56834328837 absolute loss difference:0.00253590610292\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2304, \"sum\": 2304.0, \"min\": 2304}, \"Total Records Seen\": {\"count\": 1, \"max\": 289536, \"sum\": 289536.0, \"min\": 289536}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 128, \"sum\": 128.0, \"min\": 128}}, \"EndTime\": 1613371636.338338, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 63}, \"StartTime\": 1613371635.108012}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3675.89405849 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:16 INFO 140391001810112] # Starting training for epoch 65\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:17.055] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 197, \"duration\": 1184, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] # Finished training epoch 66 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] Loss (name: value) total: 6.59054364761\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] Loss (name: value) kld: 0.170168504533\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] Loss (name: value) recons: 6.42037514183\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] Loss (name: value) logppx: 6.59054364761\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] #quality_metric: host=algo-2, epoch=66, train total_loss <loss>=6.59054364761\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] patience losses:[6.6111362112893, 6.597211546368069, 6.601118597719404, 6.594864348570506, 6.600216329097748] min patience loss:6.59486434857 current loss:6.59054364761 absolute loss difference:0.00432070096334\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] #progress_metric: host=algo-2, completed 66 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2376, \"sum\": 2376.0, \"min\": 2376}, \"Total Records Seen\": {\"count\": 1, \"max\": 298782, \"sum\": 298782.0, \"min\": 298782}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 132, \"sum\": 132.0, \"min\": 132}}, \"EndTime\": 1613371637.061863, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 65}, \"StartTime\": 1613371635.870404}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3798.89576253 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:17 INFO 140255820981440] # Starting training for epoch 67\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:18.266] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 1203, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] # Finished training epoch 67 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] Loss (name: value) total: 6.5843785074\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] Loss (name: value) kld: 0.16993226173\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] Loss (name: value) recons: 6.41444621483\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] Loss (name: value) logppx: 6.5843785074\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] #quality_metric: host=algo-2, epoch=67, train total_loss <loss>=6.5843785074\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] patience losses:[6.597211546368069, 6.601118597719404, 6.594864348570506, 6.600216329097748, 6.590543647607167] min patience loss:6.59054364761 current loss:6.5843785074 absolute loss difference:0.00616514020496\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] #progress_metric: host=algo-2, completed 67 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2412, \"sum\": 2412.0, \"min\": 2412}, \"Total Records Seen\": {\"count\": 1, \"max\": 303309, \"sum\": 303309.0, \"min\": 303309}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 134, \"sum\": 134.0, \"min\": 134}}, \"EndTime\": 1613371638.272505, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 66}, \"StartTime\": 1613371637.062249}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3739.8049361 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:18 INFO 140255820981440] # Starting training for epoch 68\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:17.551] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 1211, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] # Finished training epoch 65 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] Loss (name: value) total: 6.56804974212\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] Loss (name: value) kld: 0.166646156357\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] Loss (name: value) recons: 6.40140354633\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] Loss (name: value) logppx: 6.56804974212\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] #quality_metric: host=algo-1, epoch=65, train total_loss <loss>=6.56804974212\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] patience losses:[6.575930893421173, 6.58136773109436, 6.577184855937958, 6.570879194471571, 6.568343288368649] min patience loss:6.56834328837 current loss:6.56804974212 absolute loss difference:0.00029354625278\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2340, \"sum\": 2340.0, \"min\": 2340}, \"Total Records Seen\": {\"count\": 1, \"max\": 294060, \"sum\": 294060.0, \"min\": 294060}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 130, \"sum\": 130.0, \"min\": 130}}, \"EndTime\": 1613371637.557328, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 64}, \"StartTime\": 1613371636.339103}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3712.48150776 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:17 INFO 140391001810112] # Starting training for epoch 66\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:18.759] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 197, \"duration\": 1201, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] # Finished training epoch 66 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] Loss (name: value) total: 6.55323900779\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] Loss (name: value) kld: 0.167090166981\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] Loss (name: value) recons: 6.38614887661\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] Loss (name: value) logppx: 6.55323900779\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] #quality_metric: host=algo-1, epoch=66, train total_loss <loss>=6.55323900779\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] patience losses:[6.58136773109436, 6.577184855937958, 6.570879194471571, 6.568343288368649, 6.568049742115869] min patience loss:6.56804974212 current loss:6.55323900779 absolute loss difference:0.014810734325\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2376, \"sum\": 2376.0, \"min\": 2376}, \"Total Records Seen\": {\"count\": 1, \"max\": 298584, \"sum\": 298584.0, \"min\": 298584}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 132, \"sum\": 132.0, \"min\": 132}}, \"EndTime\": 1613371638.764152, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 65}, \"StartTime\": 1613371637.557939}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3749.30745041 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:18 INFO 140391001810112] # Starting training for epoch 67\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:19.498] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 203, \"duration\": 1225, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] # Finished training epoch 68 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] Loss (name: value) total: 6.58863041136\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] Loss (name: value) kld: 0.175785792578\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] Loss (name: value) recons: 6.41284457843\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] Loss (name: value) logppx: 6.58863041136\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] #quality_metric: host=algo-2, epoch=68, train total_loss <loss>=6.58863041136\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] patience losses:[6.601118597719404, 6.594864348570506, 6.600216329097748, 6.590543647607167, 6.584378507402208] min patience loss:6.5843785074 current loss:6.58863041136 absolute loss difference:0.00425190395779\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] #progress_metric: host=algo-2, completed 68 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2448, \"sum\": 2448.0, \"min\": 2448}, \"Total Records Seen\": {\"count\": 1, \"max\": 307836, \"sum\": 307836.0, \"min\": 307836}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 136, \"sum\": 136.0, \"min\": 136}}, \"EndTime\": 1613371639.500054, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 67}, \"StartTime\": 1613371638.272887}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3688.50322712 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:19 INFO 140255820981440] # Starting training for epoch 69\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:20.046] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 1280, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] # Finished training epoch 67 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] Loss (name: value) total: 6.56411568324\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] Loss (name: value) kld: 0.172943590726\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] Loss (name: value) recons: 6.3911720713\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] Loss (name: value) logppx: 6.56411568324\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] #quality_metric: host=algo-1, epoch=67, train total_loss <loss>=6.56411568324\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] patience losses:[6.577184855937958, 6.570879194471571, 6.568343288368649, 6.568049742115869, 6.553239007790883] min patience loss:6.55323900779 current loss:6.56411568324 absolute loss difference:0.0108766754468\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2412, \"sum\": 2412.0, \"min\": 2412}, \"Total Records Seen\": {\"count\": 1, \"max\": 303108, \"sum\": 303108.0, \"min\": 303108}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 134, \"sum\": 134.0, \"min\": 134}}, \"EndTime\": 1613371640.047655, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 66}, \"StartTime\": 1613371638.765293}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3527.43546402 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:20 INFO 140391001810112] # Starting training for epoch 68\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:20.704] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 206, \"duration\": 1203, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] # Finished training epoch 69 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] Loss (name: value) total: 6.57281795475\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] Loss (name: value) kld: 0.173330057826\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] Loss (name: value) recons: 6.39948789279\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] Loss (name: value) logppx: 6.57281795475\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] #quality_metric: host=algo-2, epoch=69, train total_loss <loss>=6.57281795475\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] patience losses:[6.594864348570506, 6.600216329097748, 6.590543647607167, 6.584378507402208, 6.588630411359999] min patience loss:6.5843785074 current loss:6.57281795475 absolute loss difference:0.01156055265\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] #progress_metric: host=algo-2, completed 69 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2484, \"sum\": 2484.0, \"min\": 2484}, \"Total Records Seen\": {\"count\": 1, \"max\": 312363, \"sum\": 312363.0, \"min\": 312363}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 138, \"sum\": 138.0, \"min\": 138}}, \"EndTime\": 1613371640.709328, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 68}, \"StartTime\": 1613371639.500326}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3743.84435008 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:20 INFO 140255820981440] # Starting training for epoch 70\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:21.282] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 203, \"duration\": 1234, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] # Finished training epoch 68 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] Loss (name: value) total: 6.55541425281\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] Loss (name: value) kld: 0.173284151074\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] Loss (name: value) recons: 6.38213013278\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] Loss (name: value) logppx: 6.55541425281\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] #quality_metric: host=algo-1, epoch=68, train total_loss <loss>=6.55541425281\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] patience losses:[6.570879194471571, 6.568343288368649, 6.568049742115869, 6.553239007790883, 6.564115683237712] min patience loss:6.55323900779 current loss:6.55541425281 absolute loss difference:0.00217524502013\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2448, \"sum\": 2448.0, \"min\": 2448}, \"Total Records Seen\": {\"count\": 1, \"max\": 307632, \"sum\": 307632.0, \"min\": 307632}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 136, \"sum\": 136.0, \"min\": 136}}, \"EndTime\": 1613371641.285458, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 67}, \"StartTime\": 1613371640.047969}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3654.90427557 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:21 INFO 140391001810112] # Starting training for epoch 69\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:21.901] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 209, \"duration\": 1191, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] # Finished training epoch 70 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] Loss (name: value) total: 6.57966809803\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] Loss (name: value) kld: 0.176167477336\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] Loss (name: value) recons: 6.40350062317\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] Loss (name: value) logppx: 6.57966809803\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] #quality_metric: host=algo-2, epoch=70, train total_loss <loss>=6.57966809803\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] patience losses:[6.600216329097748, 6.590543647607167, 6.584378507402208, 6.588630411359999, 6.57281795475218] min patience loss:6.57281795475 current loss:6.57966809803 absolute loss difference:0.00685014327367\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] #progress_metric: host=algo-2, completed 70 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2520, \"sum\": 2520.0, \"min\": 2520}, \"Total Records Seen\": {\"count\": 1, \"max\": 316890, \"sum\": 316890.0, \"min\": 316890}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 140, \"sum\": 140.0, \"min\": 140}}, \"EndTime\": 1613371641.903138, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 69}, \"StartTime\": 1613371640.709682}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3792.66443845 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:21 INFO 140255820981440] # Starting training for epoch 71\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:23.109] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 212, \"duration\": 1206, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] # Finished training epoch 71 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] Loss (name: value) total: 6.5747599668\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] Loss (name: value) kld: 0.177350911829\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] Loss (name: value) recons: 6.39740902848\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] Loss (name: value) logppx: 6.5747599668\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] #quality_metric: host=algo-2, epoch=71, train total_loss <loss>=6.5747599668\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] patience losses:[6.590543647607167, 6.584378507402208, 6.588630411359999, 6.57281795475218, 6.579668098025852] min patience loss:6.57281795475 current loss:6.5747599668 absolute loss difference:0.00194201204512\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] #progress_metric: host=algo-2, completed 71 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2556, \"sum\": 2556.0, \"min\": 2556}, \"Total Records Seen\": {\"count\": 1, \"max\": 321417, \"sum\": 321417.0, \"min\": 321417}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 142, \"sum\": 142.0, \"min\": 142}}, \"EndTime\": 1613371643.111346, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 70}, \"StartTime\": 1613371641.903453}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3747.32739119 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:23 INFO 140255820981440] # Starting training for epoch 72\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:22.526] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 206, \"duration\": 1240, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] # Finished training epoch 69 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] Loss (name: value) total: 6.54855546024\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] Loss (name: value) kld: 0.173358899231\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] Loss (name: value) recons: 6.37519657612\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] Loss (name: value) logppx: 6.54855546024\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] #quality_metric: host=algo-1, epoch=69, train total_loss <loss>=6.54855546024\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] patience losses:[6.568343288368649, 6.568049742115869, 6.553239007790883, 6.564115683237712, 6.555414252811008] min patience loss:6.55323900779 current loss:6.54855546024 absolute loss difference:0.00468354754978\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2484, \"sum\": 2484.0, \"min\": 2484}, \"Total Records Seen\": {\"count\": 1, \"max\": 312156, \"sum\": 312156.0, \"min\": 312156}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 138, \"sum\": 138.0, \"min\": 138}}, \"EndTime\": 1613371642.530836, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 68}, \"StartTime\": 1613371641.286008}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3633.67960554 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:22 INFO 140391001810112] # Starting training for epoch 70\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:24.334] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 215, \"duration\": 1222, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] # Finished training epoch 72 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] Loss (name: value) total: 6.57340469625\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] Loss (name: value) kld: 0.180589879759\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] Loss (name: value) recons: 6.39281484816\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] Loss (name: value) logppx: 6.57340469625\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] #quality_metric: host=algo-2, epoch=72, train total_loss <loss>=6.57340469625\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] patience losses:[6.584378507402208, 6.588630411359999, 6.57281795475218, 6.579668098025852, 6.5747599667972985] min patience loss:6.57281795475 current loss:6.57340469625 absolute loss difference:0.000586741500431\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] #progress_metric: host=algo-2, completed 72 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2592, \"sum\": 2592.0, \"min\": 2592}, \"Total Records Seen\": {\"count\": 1, \"max\": 325944, \"sum\": 325944.0, \"min\": 325944}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}}, \"EndTime\": 1613371644.336581, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 71}, \"StartTime\": 1613371643.111721}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3695.37743575 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:24 INFO 140255820981440] # Starting training for epoch 73\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:23.764] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 209, \"duration\": 1232, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] # Finished training epoch 70 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] Loss (name: value) total: 6.55121678114\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] Loss (name: value) kld: 0.17607688552\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] Loss (name: value) recons: 6.37513989872\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] Loss (name: value) logppx: 6.55121678114\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] #quality_metric: host=algo-1, epoch=70, train total_loss <loss>=6.55121678114\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] patience losses:[6.568049742115869, 6.553239007790883, 6.564115683237712, 6.555414252811008, 6.548555460241106] min patience loss:6.54855546024 current loss:6.55121678114 absolute loss difference:0.00266132089827\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2520, \"sum\": 2520.0, \"min\": 2520}, \"Total Records Seen\": {\"count\": 1, \"max\": 316680, \"sum\": 316680.0, \"min\": 316680}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 140, \"sum\": 140.0, \"min\": 140}}, \"EndTime\": 1613371643.766161, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 69}, \"StartTime\": 1613371642.531377}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3663.19073638 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:23 INFO 140391001810112] # Starting training for epoch 71\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:25.021] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 212, \"duration\": 1255, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] # Finished training epoch 71 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] Loss (name: value) total: 6.55140655571\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] Loss (name: value) kld: 0.17680233955\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] Loss (name: value) recons: 6.37460419205\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] Loss (name: value) logppx: 6.55140655571\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] #quality_metric: host=algo-1, epoch=71, train total_loss <loss>=6.55140655571\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] patience losses:[6.553239007790883, 6.564115683237712, 6.555414252811008, 6.548555460241106, 6.551216781139374] min patience loss:6.54855546024 current loss:6.55140655571 absolute loss difference:0.00285109546449\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2556, \"sum\": 2556.0, \"min\": 2556}, \"Total Records Seen\": {\"count\": 1, \"max\": 321204, \"sum\": 321204.0, \"min\": 321204}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 142, \"sum\": 142.0, \"min\": 142}}, \"EndTime\": 1613371645.024066, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 70}, \"StartTime\": 1613371643.766458}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3596.84799035 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:25 INFO 140391001810112] # Starting training for epoch 72\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:26.242] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 215, \"duration\": 1216, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] # Finished training epoch 72 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] Loss (name: value) total: 6.54308744272\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] Loss (name: value) kld: 0.178685422469\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] Loss (name: value) recons: 6.36440200276\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] Loss (name: value) logppx: 6.54308744272\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] #quality_metric: host=algo-1, epoch=72, train total_loss <loss>=6.54308744272\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] patience losses:[6.564115683237712, 6.555414252811008, 6.548555460241106, 6.551216781139374, 6.551406555705601] min patience loss:6.54855546024 current loss:6.54308744272 absolute loss difference:0.00546801752514\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2592, \"sum\": 2592.0, \"min\": 2592}, \"Total Records Seen\": {\"count\": 1, \"max\": 325728, \"sum\": 325728.0, \"min\": 325728}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}}, \"EndTime\": 1613371646.248579, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 71}, \"StartTime\": 1613371645.02438}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3694.79528259 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:26 INFO 140391001810112] # Starting training for epoch 73\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:27.477] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 218, \"duration\": 1227, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] # Finished training epoch 73 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] Loss (name: value) total: 6.54326472018\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] Loss (name: value) kld: 0.18321247016\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] Loss (name: value) recons: 6.36005220811\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] Loss (name: value) logppx: 6.54326472018\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] #quality_metric: host=algo-1, epoch=73, train total_loss <loss>=6.54326472018\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] patience losses:[6.555414252811008, 6.548555460241106, 6.551216781139374, 6.551406555705601, 6.543087442715962] min patience loss:6.54308744272 current loss:6.54326472018 absolute loss difference:0.000177277459039\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2628, \"sum\": 2628.0, \"min\": 2628}, \"Total Records Seen\": {\"count\": 1, \"max\": 330252, \"sum\": 330252.0, \"min\": 330252}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 146, \"sum\": 146.0, \"min\": 146}}, \"EndTime\": 1613371647.479084, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 72}, \"StartTime\": 1613371646.248988}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3677.36727992 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:27 INFO 140391001810112] # Starting training for epoch 74\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:25.416] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 218, \"duration\": 1079, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] # Finished training epoch 73 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] Loss (name: value) total: 6.56971353955\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] Loss (name: value) kld: 0.181991324243\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] Loss (name: value) recons: 6.38772224718\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] Loss (name: value) logppx: 6.56971353955\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] #quality_metric: host=algo-2, epoch=73, train total_loss <loss>=6.56971353955\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] patience losses:[6.588630411359999, 6.57281795475218, 6.579668098025852, 6.5747599667972985, 6.573404696252611] min patience loss:6.57281795475 current loss:6.56971353955 absolute loss difference:0.00310441520479\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] Timing: train: 1.08s, val: 0.00s, epoch: 1.08s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] #progress_metric: host=algo-2, completed 73 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2628, \"sum\": 2628.0, \"min\": 2628}, \"Total Records Seen\": {\"count\": 1, \"max\": 330471, \"sum\": 330471.0, \"min\": 330471}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 146, \"sum\": 146.0, \"min\": 146}}, \"EndTime\": 1613371645.422186, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 72}, \"StartTime\": 1613371644.336922}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=4170.55844139 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:25 INFO 140255820981440] # Starting training for epoch 74\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:26.552] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 221, \"duration\": 1129, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] # Finished training epoch 74 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] Loss (name: value) total: 6.56561931637\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] Loss (name: value) kld: 0.185377277434\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] Loss (name: value) recons: 6.38024199009\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] Loss (name: value) logppx: 6.56561931637\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] #quality_metric: host=algo-2, epoch=74, train total_loss <loss>=6.56561931637\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] patience losses:[6.57281795475218, 6.579668098025852, 6.5747599667972985, 6.573404696252611, 6.56971353954739] min patience loss:6.56971353955 current loss:6.56561931637 absolute loss difference:0.00409422318141\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] Timing: train: 1.13s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] #progress_metric: host=algo-2, completed 74 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2664, \"sum\": 2664.0, \"min\": 2664}, \"Total Records Seen\": {\"count\": 1, \"max\": 334998, \"sum\": 334998.0, \"min\": 334998}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 148, \"sum\": 148.0, \"min\": 148}}, \"EndTime\": 1613371646.560441, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 73}, \"StartTime\": 1613371645.422536}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3977.73491759 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:26 INFO 140255820981440] # Starting training for epoch 75\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:27.757] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 224, \"duration\": 1196, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] # Finished training epoch 75 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] Loss (name: value) total: 6.56997956832\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] Loss (name: value) kld: 0.188830404025\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] Loss (name: value) recons: 6.38114917941\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] Loss (name: value) logppx: 6.56997956832\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] #quality_metric: host=algo-2, epoch=75, train total_loss <loss>=6.56997956832\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] patience losses:[6.579668098025852, 6.5747599667972985, 6.573404696252611, 6.56971353954739, 6.565619316365984] min patience loss:6.56561931637 current loss:6.56997956832 absolute loss difference:0.00436025195652\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] #progress_metric: host=algo-2, completed 75 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2700, \"sum\": 2700.0, \"min\": 2700}, \"Total Records Seen\": {\"count\": 1, \"max\": 339525, \"sum\": 339525.0, \"min\": 339525}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}}, \"EndTime\": 1613371647.758454, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 74}, \"StartTime\": 1613371646.560781}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3779.34419602 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:27 INFO 140255820981440] # Starting training for epoch 76\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:28.918] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 227, \"duration\": 1159, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] # Finished training epoch 76 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] Loss (name: value) total: 6.57120112578\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] Loss (name: value) kld: 0.189954960512\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] Loss (name: value) recons: 6.38124616279\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] Loss (name: value) logppx: 6.57120112578\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] #quality_metric: host=algo-2, epoch=76, train total_loss <loss>=6.57120112578\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] patience losses:[6.5747599667972985, 6.573404696252611, 6.56971353954739, 6.565619316365984, 6.569979568322499] min patience loss:6.56561931637 current loss:6.57120112578 absolute loss difference:0.00558180941476\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] Timing: train: 1.16s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] #progress_metric: host=algo-2, completed 76 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2736, \"sum\": 2736.0, \"min\": 2736}, \"Total Records Seen\": {\"count\": 1, \"max\": 344052, \"sum\": 344052.0, \"min\": 344052}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 152, \"sum\": 152.0, \"min\": 152}}, \"EndTime\": 1613371648.919615, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 75}, \"StartTime\": 1613371647.758727}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3899.08784133 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:28 INFO 140255820981440] # Starting training for epoch 77\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:28.688] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 221, \"duration\": 1208, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] # Finished training epoch 74 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] Loss (name: value) total: 6.53944379091\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] Loss (name: value) kld: 0.186424978905\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] Loss (name: value) recons: 6.35301873419\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] Loss (name: value) logppx: 6.53944379091\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] #quality_metric: host=algo-1, epoch=74, train total_loss <loss>=6.53944379091\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] patience losses:[6.548555460241106, 6.551216781139374, 6.551406555705601, 6.543087442715962, 6.543264720175001] min patience loss:6.54308744272 current loss:6.53944379091 absolute loss difference:0.00364365180333\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2664, \"sum\": 2664.0, \"min\": 2664}, \"Total Records Seen\": {\"count\": 1, \"max\": 334776, \"sum\": 334776.0, \"min\": 334776}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 148, \"sum\": 148.0, \"min\": 148}}, \"EndTime\": 1613371648.691903, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 73}, \"StartTime\": 1613371647.479349}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3730.50361342 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:28 INFO 140391001810112] # Starting training for epoch 75\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:30.157] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 230, \"duration\": 1237, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] # Finished training epoch 77 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] Loss (name: value) total: 6.56102119552\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] Loss (name: value) kld: 0.190944163957\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] Loss (name: value) recons: 6.37007704046\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] Loss (name: value) logppx: 6.56102119552\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] #quality_metric: host=algo-2, epoch=77, train total_loss <loss>=6.56102119552\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] patience losses:[6.573404696252611, 6.56971353954739, 6.565619316365984, 6.569979568322499, 6.571201125780742] min patience loss:6.56561931637 current loss:6.56102119552 absolute loss difference:0.00459812084834\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] #progress_metric: host=algo-2, completed 77 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2772, \"sum\": 2772.0, \"min\": 2772}, \"Total Records Seen\": {\"count\": 1, \"max\": 348579, \"sum\": 348579.0, \"min\": 348579}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 154, \"sum\": 154.0, \"min\": 154}}, \"EndTime\": 1613371650.162055, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 76}, \"StartTime\": 1613371648.919873}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3643.78289613 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:30 INFO 140255820981440] # Starting training for epoch 78\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:29.907] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 224, \"duration\": 1212, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] # Finished training epoch 75 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] Loss (name: value) total: 6.53923857874\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] Loss (name: value) kld: 0.186604264192\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] Loss (name: value) recons: 6.35263433721\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] Loss (name: value) logppx: 6.53923857874\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] #quality_metric: host=algo-1, epoch=75, train total_loss <loss>=6.53923857874\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] patience losses:[6.551216781139374, 6.551406555705601, 6.543087442715962, 6.543264720175001, 6.539443790912628] min patience loss:6.53944379091 current loss:6.53923857874 absolute loss difference:0.000205212169224\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2700, \"sum\": 2700.0, \"min\": 2700}, \"Total Records Seen\": {\"count\": 1, \"max\": 339300, \"sum\": 339300.0, \"min\": 339300}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}}, \"EndTime\": 1613371649.91205, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 74}, \"StartTime\": 1613371648.692215}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3708.05967592 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:29 INFO 140391001810112] # Starting training for epoch 76\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:31.300] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 233, \"duration\": 1137, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] # Finished training epoch 78 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] Loss (name: value) total: 6.55758113994\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] Loss (name: value) kld: 0.19240422195\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] Loss (name: value) recons: 6.36517690288\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] Loss (name: value) logppx: 6.55758113994\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] #quality_metric: host=algo-2, epoch=78, train total_loss <loss>=6.55758113994\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] patience losses:[6.56971353954739, 6.565619316365984, 6.569979568322499, 6.571201125780742, 6.561021195517646] min patience loss:6.56102119552 current loss:6.55758113994 absolute loss difference:0.00344005558226\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] Timing: train: 1.14s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] #progress_metric: host=algo-2, completed 78 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2808, \"sum\": 2808.0, \"min\": 2808}, \"Total Records Seen\": {\"count\": 1, \"max\": 353106, \"sum\": 353106.0, \"min\": 353106}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 156, \"sum\": 156.0, \"min\": 156}}, \"EndTime\": 1613371651.305137, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 77}, \"StartTime\": 1613371650.16246}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3961.22935959 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:31 INFO 140255820981440] # Starting training for epoch 79\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:31.064] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 227, \"duration\": 1152, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] # Finished training epoch 76 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] Loss (name: value) total: 6.53741400109\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] Loss (name: value) kld: 0.188042691391\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] Loss (name: value) recons: 6.3493713008\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] Loss (name: value) logppx: 6.53741400109\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] #quality_metric: host=algo-1, epoch=76, train total_loss <loss>=6.53741400109\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] patience losses:[6.551406555705601, 6.543087442715962, 6.543264720175001, 6.539443790912628, 6.539238578743404] min patience loss:6.53923857874 current loss:6.53741400109 absolute loss difference:0.00182457764943\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] Timing: train: 1.15s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2736, \"sum\": 2736.0, \"min\": 2736}, \"Total Records Seen\": {\"count\": 1, \"max\": 343824, \"sum\": 343824.0, \"min\": 343824}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 152, \"sum\": 152.0, \"min\": 152}}, \"EndTime\": 1613371651.070343, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 75}, \"StartTime\": 1613371649.912418}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3906.32479694 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:31 INFO 140391001810112] # Starting training for epoch 77\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:32.209] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 230, \"duration\": 1137, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] # Finished training epoch 77 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] Loss (name: value) total: 6.5339810186\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] Loss (name: value) kld: 0.189495951248\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] Loss (name: value) recons: 6.34448509084\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] Loss (name: value) logppx: 6.5339810186\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] #quality_metric: host=algo-1, epoch=77, train total_loss <loss>=6.5339810186\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] patience losses:[6.543087442715962, 6.543264720175001, 6.539443790912628, 6.539238578743404, 6.53741400109397] min patience loss:6.53741400109 current loss:6.5339810186 absolute loss difference:0.00343298249774\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] Timing: train: 1.14s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2772, \"sum\": 2772.0, \"min\": 2772}, \"Total Records Seen\": {\"count\": 1, \"max\": 348348, \"sum\": 348348.0, \"min\": 348348}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 154, \"sum\": 154.0, \"min\": 154}}, \"EndTime\": 1613371652.213804, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 76}, \"StartTime\": 1613371651.070753}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3956.85921112 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:32 INFO 140391001810112] # Starting training for epoch 78\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:32.450] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 236, \"duration\": 1144, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] # Finished training epoch 79 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] Loss (name: value) total: 6.55520196093\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] Loss (name: value) kld: 0.193504459949\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] Loss (name: value) recons: 6.36169750161\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] Loss (name: value) logppx: 6.55520196093\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] #quality_metric: host=algo-2, epoch=79, train total_loss <loss>=6.55520196093\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] patience losses:[6.565619316365984, 6.569979568322499, 6.571201125780742, 6.561021195517646, 6.557581139935388] min patience loss:6.55758113994 current loss:6.55520196093 absolute loss difference:0.00237917900085\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] #progress_metric: host=algo-2, completed 79 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2844, \"sum\": 2844.0, \"min\": 2844}, \"Total Records Seen\": {\"count\": 1, \"max\": 357633, \"sum\": 357633.0, \"min\": 357633}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 158, \"sum\": 158.0, \"min\": 158}}, \"EndTime\": 1613371652.455233, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 78}, \"StartTime\": 1613371651.305412}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3936.5834389 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:32 INFO 140255820981440] # Starting training for epoch 80\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:33.420] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 233, \"duration\": 1206, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] # Finished training epoch 78 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] Loss (name: value) total: 6.53118289842\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] Loss (name: value) kld: 0.193692643402\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] Loss (name: value) recons: 6.3374902606\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] Loss (name: value) logppx: 6.53118289842\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] #quality_metric: host=algo-1, epoch=78, train total_loss <loss>=6.53118289842\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] patience losses:[6.543264720175001, 6.539443790912628, 6.539238578743404, 6.53741400109397, 6.533981018596226] min patience loss:6.5339810186 current loss:6.53118289842 absolute loss difference:0.00279812018077\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2808, \"sum\": 2808.0, \"min\": 2808}, \"Total Records Seen\": {\"count\": 1, \"max\": 352872, \"sum\": 352872.0, \"min\": 352872}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 156, \"sum\": 156.0, \"min\": 156}}, \"EndTime\": 1613371653.42757, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 77}, \"StartTime\": 1613371652.214359}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3728.48633451 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:33 INFO 140391001810112] # Starting training for epoch 79\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:33.662] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 239, \"duration\": 1206, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] # Finished training epoch 80 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] Loss (name: value) total: 6.55517677466\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] Loss (name: value) kld: 0.19781480295\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] Loss (name: value) recons: 6.35736195909\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] Loss (name: value) logppx: 6.55517677466\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] #quality_metric: host=algo-2, epoch=80, train total_loss <loss>=6.55517677466\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] patience losses:[6.569979568322499, 6.571201125780742, 6.561021195517646, 6.557581139935388, 6.555201960934533] min patience loss:6.55520196093 current loss:6.55517677466 absolute loss difference:2.51862737866e-05\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] #progress_metric: host=algo-2, completed 80 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2880, \"sum\": 2880.0, \"min\": 2880}, \"Total Records Seen\": {\"count\": 1, \"max\": 362160, \"sum\": 362160.0, \"min\": 362160}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 160, \"sum\": 160.0, \"min\": 160}}, \"EndTime\": 1613371653.667443, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 79}, \"StartTime\": 1613371652.455538}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3734.87700266 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:33 INFO 140255820981440] # Starting training for epoch 81\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:34.895] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 242, \"duration\": 1227, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] # Finished training epoch 81 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] Loss (name: value) total: 6.54965903362\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] Loss (name: value) kld: 0.197722101377\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] Loss (name: value) recons: 6.35193689002\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] Loss (name: value) logppx: 6.54965903362\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] #quality_metric: host=algo-2, epoch=81, train total_loss <loss>=6.54965903362\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] patience losses:[6.571201125780742, 6.561021195517646, 6.557581139935388, 6.555201960934533, 6.5551767746607466] min patience loss:6.55517677466 current loss:6.54965903362 absolute loss difference:0.00551774104436\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] #progress_metric: host=algo-2, completed 81 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2916, \"sum\": 2916.0, \"min\": 2916}, \"Total Records Seen\": {\"count\": 1, \"max\": 366687, \"sum\": 366687.0, \"min\": 366687}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 162, \"sum\": 162.0, \"min\": 162}}, \"EndTime\": 1613371654.902483, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 80}, \"StartTime\": 1613371653.667786}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3665.94616811 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:34 INFO 140255820981440] # Starting training for epoch 82\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:34.670] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 236, \"duration\": 1241, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] # Finished training epoch 79 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] Loss (name: value) total: 6.52620798349\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] Loss (name: value) kld: 0.193872860322\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] Loss (name: value) recons: 6.33233514097\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] Loss (name: value) logppx: 6.52620798349\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] #quality_metric: host=algo-1, epoch=79, train total_loss <loss>=6.52620798349\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] patience losses:[6.539443790912628, 6.539238578743404, 6.53741400109397, 6.533981018596226, 6.53118289841546] min patience loss:6.53118289842 current loss:6.52620798349 absolute loss difference:0.00497491492165\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2844, \"sum\": 2844.0, \"min\": 2844}, \"Total Records Seen\": {\"count\": 1, \"max\": 357396, \"sum\": 357396.0, \"min\": 357396}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 158, \"sum\": 158.0, \"min\": 158}}, \"EndTime\": 1613371654.67596, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 78}, \"StartTime\": 1613371653.427861}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3624.21858271 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:34 INFO 140391001810112] # Starting training for epoch 80\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:36.086] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 245, \"duration\": 1183, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] # Finished training epoch 82 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] Loss (name: value) total: 6.5376005835\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] Loss (name: value) kld: 0.197281464107\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] Loss (name: value) recons: 6.34031911029\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] Loss (name: value) logppx: 6.5376005835\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] #quality_metric: host=algo-2, epoch=82, train total_loss <loss>=6.5376005835\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] patience losses:[6.561021195517646, 6.557581139935388, 6.555201960934533, 6.5551767746607466, 6.549659033616384] min patience loss:6.54965903362 current loss:6.5376005835 absolute loss difference:0.0120584501161\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] #progress_metric: host=algo-2, completed 82 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2952, \"sum\": 2952.0, \"min\": 2952}, \"Total Records Seen\": {\"count\": 1, \"max\": 371214, \"sum\": 371214.0, \"min\": 371214}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 164, \"sum\": 164.0, \"min\": 164}}, \"EndTime\": 1613371656.092197, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 81}, \"StartTime\": 1613371654.902827}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3805.71731212 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:36 INFO 140255820981440] # Starting training for epoch 83\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:35.899] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 239, \"duration\": 1221, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] # Finished training epoch 80 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] Loss (name: value) total: 6.51887388362\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] Loss (name: value) kld: 0.193804594792\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] Loss (name: value) recons: 6.32506930828\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] Loss (name: value) logppx: 6.51887388362\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] #quality_metric: host=algo-1, epoch=80, train total_loss <loss>=6.51887388362\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] patience losses:[6.539238578743404, 6.53741400109397, 6.533981018596226, 6.53118289841546, 6.526207983493805] min patience loss:6.52620798349 current loss:6.51887388362 absolute loss difference:0.00733409987556\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2880, \"sum\": 2880.0, \"min\": 2880}, \"Total Records Seen\": {\"count\": 1, \"max\": 361920, \"sum\": 361920.0, \"min\": 361920}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 160, \"sum\": 160.0, \"min\": 160}}, \"EndTime\": 1613371655.903574, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 79}, \"StartTime\": 1613371654.67626}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3685.63289176 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:35 INFO 140391001810112] # Starting training for epoch 81\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:37.264] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 248, \"duration\": 1171, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] # Finished training epoch 83 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] Loss (name: value) total: 6.54444648822\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] Loss (name: value) kld: 0.198992493873\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] Loss (name: value) recons: 6.34545402394\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] Loss (name: value) logppx: 6.54444648822\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] #quality_metric: host=algo-2, epoch=83, train total_loss <loss>=6.54444648822\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] patience losses:[6.557581139935388, 6.555201960934533, 6.5551767746607466, 6.549659033616384, 6.537600583500332] min patience loss:6.5376005835 current loss:6.54444648822 absolute loss difference:0.00684590472115\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] #progress_metric: host=algo-2, completed 83 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2988, \"sum\": 2988.0, \"min\": 2988}, \"Total Records Seen\": {\"count\": 1, \"max\": 375741, \"sum\": 375741.0, \"min\": 375741}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 166, \"sum\": 166.0, \"min\": 166}}, \"EndTime\": 1613371657.266487, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 82}, \"StartTime\": 1613371656.09247}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3855.51910214 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:37 INFO 140255820981440] # Starting training for epoch 84\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:37.085] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 242, \"duration\": 1181, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] # Finished training epoch 81 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] Loss (name: value) total: 6.51520244943\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] Loss (name: value) kld: 0.195292352388\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] Loss (name: value) recons: 6.31991006931\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] Loss (name: value) logppx: 6.51520244943\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] #quality_metric: host=algo-1, epoch=81, train total_loss <loss>=6.51520244943\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] patience losses:[6.53741400109397, 6.533981018596226, 6.53118289841546, 6.526207983493805, 6.518873883618249] min patience loss:6.51887388362 current loss:6.51520244943 absolute loss difference:0.00367143419054\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] Timing: train: 1.18s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2916, \"sum\": 2916.0, \"min\": 2916}, \"Total Records Seen\": {\"count\": 1, \"max\": 366444, \"sum\": 366444.0, \"min\": 366444}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 162, \"sum\": 162.0, \"min\": 162}}, \"EndTime\": 1613371657.08967, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 80}, \"StartTime\": 1613371655.903872}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3814.11037525 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:37 INFO 140391001810112] # Starting training for epoch 82\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:38.323] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 245, \"duration\": 1232, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] # Finished training epoch 82 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] Loss (name: value) total: 6.52751353714\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] Loss (name: value) kld: 0.198715866026\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] Loss (name: value) recons: 6.32879764504\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] Loss (name: value) logppx: 6.52751353714\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] #quality_metric: host=algo-1, epoch=82, train total_loss <loss>=6.52751353714\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] patience losses:[6.533981018596226, 6.53118289841546, 6.526207983493805, 6.518873883618249, 6.5152024494277105] min patience loss:6.51520244943 current loss:6.52751353714 absolute loss difference:0.0123110877143\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2952, \"sum\": 2952.0, \"min\": 2952}, \"Total Records Seen\": {\"count\": 1, \"max\": 370968, \"sum\": 370968.0, \"min\": 370968}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 164, \"sum\": 164.0, \"min\": 164}}, \"EndTime\": 1613371658.324276, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 81}, \"StartTime\": 1613371657.090243}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3665.63641931 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:38 INFO 140391001810112] # Starting training for epoch 83\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:38.422] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 251, \"duration\": 1155, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] # Finished training epoch 84 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] Loss (name: value) total: 6.54978559415\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] Loss (name: value) kld: 0.202637721888\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] Loss (name: value) recons: 6.347147829\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] Loss (name: value) logppx: 6.54978559415\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] #quality_metric: host=algo-2, epoch=84, train total_loss <loss>=6.54978559415\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] patience losses:[6.555201960934533, 6.5551767746607466, 6.549659033616384, 6.537600583500332, 6.544446488221486] min patience loss:6.5376005835 current loss:6.54978559415 absolute loss difference:0.0121850106451\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] Timing: train: 1.16s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] #progress_metric: host=algo-2, completed 84 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3024, \"sum\": 3024.0, \"min\": 3024}, \"Total Records Seen\": {\"count\": 1, \"max\": 380268, \"sum\": 380268.0, \"min\": 380268}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 168, \"sum\": 168.0, \"min\": 168}}, \"EndTime\": 1613371658.424946, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 83}, \"StartTime\": 1613371657.26678}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3908.13880226 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:38 INFO 140255820981440] # Starting training for epoch 85\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:39.560] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 248, \"duration\": 1235, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] # Finished training epoch 83 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] Loss (name: value) total: 6.51807597611\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] Loss (name: value) kld: 0.198378164942\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] Loss (name: value) recons: 6.31969781054\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] Loss (name: value) logppx: 6.51807597611\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] #quality_metric: host=algo-1, epoch=83, train total_loss <loss>=6.51807597611\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] patience losses:[6.53118289841546, 6.526207983493805, 6.518873883618249, 6.5152024494277105, 6.527513537142012] min patience loss:6.51520244943 current loss:6.51807597611 absolute loss difference:0.00287352667914\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2988, \"sum\": 2988.0, \"min\": 2988}, \"Total Records Seen\": {\"count\": 1, \"max\": 375492, \"sum\": 375492.0, \"min\": 375492}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 166, \"sum\": 166.0, \"min\": 166}}, \"EndTime\": 1613371659.561943, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 82}, \"StartTime\": 1613371658.324513}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3655.41192639 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:39 INFO 140391001810112] # Starting training for epoch 84\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:39.600] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 254, \"duration\": 1174, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] # Finished training epoch 85 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] Loss (name: value) total: 6.54071952237\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] Loss (name: value) kld: 0.204750004121\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] Loss (name: value) recons: 6.33596949445\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] Loss (name: value) logppx: 6.54071952237\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] #quality_metric: host=algo-2, epoch=85, train total_loss <loss>=6.54071952237\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] patience losses:[6.5551767746607466, 6.549659033616384, 6.537600583500332, 6.544446488221486, 6.549785594145457] min patience loss:6.5376005835 current loss:6.54071952237 absolute loss difference:0.0031189388699\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] #progress_metric: host=algo-2, completed 85 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3060, \"sum\": 3060.0, \"min\": 3060}, \"Total Records Seen\": {\"count\": 1, \"max\": 384795, \"sum\": 384795.0, \"min\": 384795}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 170, \"sum\": 170.0, \"min\": 170}}, \"EndTime\": 1613371659.602938, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 84}, \"StartTime\": 1613371658.425556}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3844.44895443 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:39 INFO 140255820981440] # Starting training for epoch 86\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:40.795] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 257, \"duration\": 1191, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] # Finished training epoch 86 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] Loss (name: value) total: 6.53961333964\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] Loss (name: value) kld: 0.209435331739\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] Loss (name: value) recons: 6.33017803563\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] Loss (name: value) logppx: 6.53961333964\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] #quality_metric: host=algo-2, epoch=86, train total_loss <loss>=6.53961333964\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] patience losses:[6.549659033616384, 6.537600583500332, 6.544446488221486, 6.549785594145457, 6.540719522370233] min patience loss:6.5376005835 current loss:6.53961333964 absolute loss difference:0.00201275613573\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] #progress_metric: host=algo-2, completed 86 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3096, \"sum\": 3096.0, \"min\": 3096}, \"Total Records Seen\": {\"count\": 1, \"max\": 389322, \"sum\": 389322.0, \"min\": 389322}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 172, \"sum\": 172.0, \"min\": 172}}, \"EndTime\": 1613371660.796804, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 85}, \"StartTime\": 1613371659.603441}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3792.3561356 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:40 INFO 140255820981440] # Starting training for epoch 87\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:40.814] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 251, \"duration\": 1252, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] # Finished training epoch 84 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] Loss (name: value) total: 6.51567900181\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] Loss (name: value) kld: 0.20348664518\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] Loss (name: value) recons: 6.31219229433\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] Loss (name: value) logppx: 6.51567900181\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] #quality_metric: host=algo-1, epoch=84, train total_loss <loss>=6.51567900181\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] patience losses:[6.526207983493805, 6.518873883618249, 6.5152024494277105, 6.527513537142012, 6.518075976106855] min patience loss:6.51520244943 current loss:6.51567900181 absolute loss difference:0.000476552380456\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3024, \"sum\": 3024.0, \"min\": 3024}, \"Total Records Seen\": {\"count\": 1, \"max\": 380016, \"sum\": 380016.0, \"min\": 380016}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 168, \"sum\": 168.0, \"min\": 168}}, \"EndTime\": 1613371660.816351, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 83}, \"StartTime\": 1613371659.562307}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3607.07477763 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:40 INFO 140391001810112] # Starting training for epoch 85\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:41.917] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 260, \"duration\": 1120, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] # Finished training epoch 87 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] Loss (name: value) total: 6.53554640214\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] Loss (name: value) kld: 0.204309269372\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] Loss (name: value) recons: 6.3312371042\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] Loss (name: value) logppx: 6.53554640214\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] #quality_metric: host=algo-2, epoch=87, train total_loss <loss>=6.53554640214\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] patience losses:[6.537600583500332, 6.544446488221486, 6.549785594145457, 6.540719522370233, 6.539613339636061] min patience loss:6.5376005835 current loss:6.53554640214 absolute loss difference:0.00205418136385\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] Timing: train: 1.12s, val: 0.00s, epoch: 1.13s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] #progress_metric: host=algo-2, completed 87 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3132, \"sum\": 3132.0, \"min\": 3132}, \"Total Records Seen\": {\"count\": 1, \"max\": 393849, \"sum\": 393849.0, \"min\": 393849}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 174, \"sum\": 174.0, \"min\": 174}}, \"EndTime\": 1613371661.922685, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 86}, \"StartTime\": 1613371660.797135}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=4021.54033292 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:41 INFO 140255820981440] # Starting training for epoch 88\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:41.986] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 254, \"duration\": 1169, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] # Finished training epoch 85 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] Loss (name: value) total: 6.52348066701\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] Loss (name: value) kld: 0.200855370818\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] Loss (name: value) recons: 6.32262529267\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] Loss (name: value) logppx: 6.52348066701\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] #quality_metric: host=algo-1, epoch=85, train total_loss <loss>=6.52348066701\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] patience losses:[6.518873883618249, 6.5152024494277105, 6.527513537142012, 6.518075976106855, 6.5156790018081665] min patience loss:6.51520244943 current loss:6.52348066701 absolute loss difference:0.00827821758058\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3060, \"sum\": 3060.0, \"min\": 3060}, \"Total Records Seen\": {\"count\": 1, \"max\": 384540, \"sum\": 384540.0, \"min\": 384540}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 170, \"sum\": 170.0, \"min\": 170}}, \"EndTime\": 1613371661.987703, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 84}, \"StartTime\": 1613371660.816656}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3862.6882833 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:41 INFO 140391001810112] # Starting training for epoch 86\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:43.094] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 263, \"duration\": 1170, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] # Finished training epoch 88 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] Loss (name: value) total: 6.53034275108\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] Loss (name: value) kld: 0.207472031522\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] Loss (name: value) recons: 6.32287069162\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] Loss (name: value) logppx: 6.53034275108\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] #quality_metric: host=algo-2, epoch=88, train total_loss <loss>=6.53034275108\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] patience losses:[6.544446488221486, 6.549785594145457, 6.540719522370233, 6.539613339636061, 6.535546402136485] min patience loss:6.53554640214 current loss:6.53034275108 absolute loss difference:0.00520365105735\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] Timing: train: 1.17s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] #progress_metric: host=algo-2, completed 88 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3168, \"sum\": 3168.0, \"min\": 3168}, \"Total Records Seen\": {\"count\": 1, \"max\": 398376, \"sum\": 398376.0, \"min\": 398376}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 176, \"sum\": 176.0, \"min\": 176}}, \"EndTime\": 1613371663.100449, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 87}, \"StartTime\": 1613371661.92299}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3844.07069363 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:43 INFO 140255820981440] # Starting training for epoch 89\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:43.235] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 257, \"duration\": 1247, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] # Finished training epoch 86 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] Loss (name: value) total: 6.51294059224\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] Loss (name: value) kld: 0.202239119965\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] Loss (name: value) recons: 6.31070146296\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] Loss (name: value) logppx: 6.51294059224\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] #quality_metric: host=algo-1, epoch=86, train total_loss <loss>=6.51294059224\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] patience losses:[6.5152024494277105, 6.527513537142012, 6.518075976106855, 6.5156790018081665, 6.523480667008294] min patience loss:6.51520244943 current loss:6.51294059224 absolute loss difference:0.00226185719172\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3096, \"sum\": 3096.0, \"min\": 3096}, \"Total Records Seen\": {\"count\": 1, \"max\": 389064, \"sum\": 389064.0, \"min\": 389064}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 172, \"sum\": 172.0, \"min\": 172}}, \"EndTime\": 1613371663.242008, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 85}, \"StartTime\": 1613371661.988006}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3607.17831978 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:43 INFO 140391001810112] # Starting training for epoch 87\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:44.276] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 266, \"duration\": 1175, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] # Finished training epoch 89 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] Loss (name: value) total: 6.52922878\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] Loss (name: value) kld: 0.208004402825\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] Loss (name: value) recons: 6.3212244312\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] Loss (name: value) logppx: 6.52922878\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] #quality_metric: host=algo-2, epoch=89, train total_loss <loss>=6.52922878\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] patience losses:[6.549785594145457, 6.540719522370233, 6.539613339636061, 6.535546402136485, 6.530342751079136] min patience loss:6.53034275108 current loss:6.52922878 absolute loss difference:0.00111397107442\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] #progress_metric: host=algo-2, completed 89 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3204, \"sum\": 3204.0, \"min\": 3204}, \"Total Records Seen\": {\"count\": 1, \"max\": 402903, \"sum\": 402903.0, \"min\": 402903}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 178, \"sum\": 178.0, \"min\": 178}}, \"EndTime\": 1613371664.282279, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 88}, \"StartTime\": 1613371663.10081}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3830.26007446 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:44 INFO 140255820981440] # Starting training for epoch 90\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:44.420] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 260, \"duration\": 1176, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] # Finished training epoch 87 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] Loss (name: value) total: 6.51390116745\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] Loss (name: value) kld: 0.205417665756\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] Loss (name: value) recons: 6.30848350128\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] Loss (name: value) logppx: 6.51390116745\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] #quality_metric: host=algo-1, epoch=87, train total_loss <loss>=6.51390116745\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] patience losses:[6.527513537142012, 6.518075976106855, 6.5156790018081665, 6.523480667008294, 6.512940592235989] min patience loss:6.51294059224 current loss:6.51390116745 absolute loss difference:0.000960575209724\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3132, \"sum\": 3132.0, \"min\": 3132}, \"Total Records Seen\": {\"count\": 1, \"max\": 393588, \"sum\": 393588.0, \"min\": 393588}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 174, \"sum\": 174.0, \"min\": 174}}, \"EndTime\": 1613371664.421582, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 86}, \"StartTime\": 1613371663.242318}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3835.82325445 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:44 INFO 140391001810112] # Starting training for epoch 88\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:45.525] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 269, \"duration\": 1242, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] # Finished training epoch 90 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] Loss (name: value) total: 6.52776267793\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] Loss (name: value) kld: 0.210975531903\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] Loss (name: value) recons: 6.31678713693\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] Loss (name: value) logppx: 6.52776267793\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] #quality_metric: host=algo-2, epoch=90, train total_loss <loss>=6.52776267793\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] patience losses:[6.540719522370233, 6.539613339636061, 6.535546402136485, 6.530342751079136, 6.529228780004713] min patience loss:6.52922878 current loss:6.52776267793 absolute loss difference:0.00146610207028\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] #progress_metric: host=algo-2, completed 90 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3240, \"sum\": 3240.0, \"min\": 3240}, \"Total Records Seen\": {\"count\": 1, \"max\": 407430, \"sum\": 407430.0, \"min\": 407430}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}}, \"EndTime\": 1613371665.530822, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 89}, \"StartTime\": 1613371664.28294}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3627.24958957 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:45 INFO 140255820981440] # Starting training for epoch 91\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:45.683] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 263, \"duration\": 1261, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] # Finished training epoch 88 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] Loss (name: value) total: 6.49971981181\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] Loss (name: value) kld: 0.206129222073\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] Loss (name: value) recons: 6.29359059864\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] Loss (name: value) logppx: 6.49971981181\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] #quality_metric: host=algo-1, epoch=88, train total_loss <loss>=6.49971981181\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] patience losses:[6.518075976106855, 6.5156790018081665, 6.523480667008294, 6.512940592235989, 6.513901167445713] min patience loss:6.51294059224 current loss:6.49971981181 absolute loss difference:0.0132207804256\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] Timing: train: 1.26s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3168, \"sum\": 3168.0, \"min\": 3168}, \"Total Records Seen\": {\"count\": 1, \"max\": 398112, \"sum\": 398112.0, \"min\": 398112}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 176, \"sum\": 176.0, \"min\": 176}}, \"EndTime\": 1613371665.687948, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 87}, \"StartTime\": 1613371664.421827}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3572.68049557 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:45 INFO 140391001810112] # Starting training for epoch 89\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:46.747] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 272, \"duration\": 1215, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] # Finished training epoch 91 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] Loss (name: value) total: 6.53345645799\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] Loss (name: value) kld: 0.214775382852\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] Loss (name: value) recons: 6.31868106789\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] Loss (name: value) logppx: 6.53345645799\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] #quality_metric: host=algo-2, epoch=91, train total_loss <loss>=6.53345645799\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] patience losses:[6.539613339636061, 6.535546402136485, 6.530342751079136, 6.529228780004713, 6.527762677934435] min patience loss:6.52776267793 current loss:6.53345645799 absolute loss difference:0.00569378005134\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] #progress_metric: host=algo-2, completed 91 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3276, \"sum\": 3276.0, \"min\": 3276}, \"Total Records Seen\": {\"count\": 1, \"max\": 411957, \"sum\": 411957.0, \"min\": 411957}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 182, \"sum\": 182.0, \"min\": 182}}, \"EndTime\": 1613371666.748816, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 90}, \"StartTime\": 1613371665.531144}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3716.84936941 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:46 INFO 140255820981440] # Starting training for epoch 92\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:46.914] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 266, \"duration\": 1225, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] # Finished training epoch 89 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] Loss (name: value) total: 6.51273041301\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] Loss (name: value) kld: 0.20941119103\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] Loss (name: value) recons: 6.30331922902\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] Loss (name: value) logppx: 6.51273041301\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] #quality_metric: host=algo-1, epoch=89, train total_loss <loss>=6.51273041301\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] patience losses:[6.5156790018081665, 6.523480667008294, 6.512940592235989, 6.513901167445713, 6.499719811810388] min patience loss:6.49971981181 current loss:6.51273041301 absolute loss difference:0.0130106012026\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3204, \"sum\": 3204.0, \"min\": 3204}, \"Total Records Seen\": {\"count\": 1, \"max\": 402636, \"sum\": 402636.0, \"min\": 402636}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 178, \"sum\": 178.0, \"min\": 178}}, \"EndTime\": 1613371666.91586, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 88}, \"StartTime\": 1613371665.688266}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3684.77761088 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:46 INFO 140391001810112] # Starting training for epoch 90\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:47.969] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 275, \"duration\": 1219, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] # Finished training epoch 92 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] Loss (name: value) total: 6.52893335952\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] Loss (name: value) kld: 0.212909131207\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] Loss (name: value) recons: 6.31602425045\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] Loss (name: value) logppx: 6.52893335952\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] #quality_metric: host=algo-2, epoch=92, train total_loss <loss>=6.52893335952\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] patience losses:[6.535546402136485, 6.530342751079136, 6.529228780004713, 6.527762677934435, 6.533456457985772] min patience loss:6.52776267793 current loss:6.52893335952 absolute loss difference:0.00117068158256\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] #progress_metric: host=algo-2, completed 92 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3312, \"sum\": 3312.0, \"min\": 3312}, \"Total Records Seen\": {\"count\": 1, \"max\": 416484, \"sum\": 416484.0, \"min\": 416484}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 184, \"sum\": 184.0, \"min\": 184}}, \"EndTime\": 1613371667.971567, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 91}, \"StartTime\": 1613371666.749433}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3703.67974496 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:47 INFO 140255820981440] # Starting training for epoch 93\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:48.167] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 269, \"duration\": 1250, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] # Finished training epoch 90 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] Loss (name: value) total: 6.50812155671\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] Loss (name: value) kld: 0.206974284827\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] Loss (name: value) recons: 6.30114730861\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] Loss (name: value) logppx: 6.50812155671\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] #quality_metric: host=algo-1, epoch=90, train total_loss <loss>=6.50812155671\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] patience losses:[6.523480667008294, 6.512940592235989, 6.513901167445713, 6.499719811810388, 6.512730413013035] min patience loss:6.49971981181 current loss:6.50812155671 absolute loss difference:0.00840174489551\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3240, \"sum\": 3240.0, \"min\": 3240}, \"Total Records Seen\": {\"count\": 1, \"max\": 407160, \"sum\": 407160.0, \"min\": 407160}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}}, \"EndTime\": 1613371668.169208, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 89}, \"StartTime\": 1613371666.91617}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3609.97277276 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:48 INFO 140391001810112] # Starting training for epoch 91\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:49.133] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 278, \"duration\": 1160, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] # Finished training epoch 93 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] Loss (name: value) total: 6.51344048315\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] Loss (name: value) kld: 0.211994954281\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] Loss (name: value) recons: 6.30144549078\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] Loss (name: value) logppx: 6.51344048315\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] #quality_metric: host=algo-2, epoch=93, train total_loss <loss>=6.51344048315\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] patience losses:[6.530342751079136, 6.529228780004713, 6.527762677934435, 6.533456457985772, 6.528933359516992] min patience loss:6.52776267793 current loss:6.51344048315 absolute loss difference:0.0143221947882\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] Timing: train: 1.16s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] #progress_metric: host=algo-2, completed 93 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3348, \"sum\": 3348.0, \"min\": 3348}, \"Total Records Seen\": {\"count\": 1, \"max\": 421011, \"sum\": 421011.0, \"min\": 421011}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 186, \"sum\": 186.0, \"min\": 186}}, \"EndTime\": 1613371669.137815, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 92}, \"StartTime\": 1613371667.971893}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3882.07521622 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:49 INFO 140255820981440] # Starting training for epoch 94\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:49.384] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 272, \"duration\": 1214, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] # Finished training epoch 91 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] Loss (name: value) total: 6.50137932433\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] Loss (name: value) kld: 0.209376680147\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] Loss (name: value) recons: 6.29200269116\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] Loss (name: value) logppx: 6.50137932433\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] #quality_metric: host=algo-1, epoch=91, train total_loss <loss>=6.50137932433\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] patience losses:[6.512940592235989, 6.513901167445713, 6.499719811810388, 6.512730413013035, 6.508121556705898] min patience loss:6.49971981181 current loss:6.50137932433 absolute loss difference:0.00165951251984\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3276, \"sum\": 3276.0, \"min\": 3276}, \"Total Records Seen\": {\"count\": 1, \"max\": 411684, \"sum\": 411684.0, \"min\": 411684}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 182, \"sum\": 182.0, \"min\": 182}}, \"EndTime\": 1613371669.385964, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 90}, \"StartTime\": 1613371668.169526}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3718.62423006 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:49 INFO 140391001810112] # Starting training for epoch 92\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:50.395] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 281, \"duration\": 1256, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] # Finished training epoch 94 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] Loss (name: value) total: 6.52493310637\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] Loss (name: value) kld: 0.217345133631\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] Loss (name: value) recons: 6.30758794811\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] Loss (name: value) logppx: 6.52493310637\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] #quality_metric: host=algo-2, epoch=94, train total_loss <loss>=6.52493310637\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] patience losses:[6.529228780004713, 6.527762677934435, 6.533456457985772, 6.528933359516992, 6.513440483146244] min patience loss:6.51344048315 current loss:6.52493310637 absolute loss difference:0.0114926232232\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] #progress_metric: host=algo-2, completed 94 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3384, \"sum\": 3384.0, \"min\": 3384}, \"Total Records Seen\": {\"count\": 1, \"max\": 425538, \"sum\": 425538.0, \"min\": 425538}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 188, \"sum\": 188.0, \"min\": 188}}, \"EndTime\": 1613371670.397391, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 93}, \"StartTime\": 1613371669.138178}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3593.63576054 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:50 INFO 140255820981440] # Starting training for epoch 95\u001b[0m\n",
      "\n",
      "2021-02-15 06:48:00 Uploading - Uploading generated training model\u001b[34m[2021-02-15 06:47:50.679] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 275, \"duration\": 1292, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] # Finished training epoch 92 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] Loss (name: value) total: 6.50023965041\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] Loss (name: value) kld: 0.211393925051\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] Loss (name: value) recons: 6.28884571791\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] Loss (name: value) logppx: 6.50023965041\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] #quality_metric: host=algo-1, epoch=92, train total_loss <loss>=6.50023965041\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] patience losses:[6.513901167445713, 6.499719811810388, 6.512730413013035, 6.508121556705898, 6.501379324330224] min patience loss:6.49971981181 current loss:6.50023965041 absolute loss difference:0.00051983859804\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] Timing: train: 1.29s, val: 0.00s, epoch: 1.30s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3312, \"sum\": 3312.0, \"min\": 3312}, \"Total Records Seen\": {\"count\": 1, \"max\": 416208, \"sum\": 416208.0, \"min\": 416208}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 184, \"sum\": 184.0, \"min\": 184}}, \"EndTime\": 1613371670.681827, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 91}, \"StartTime\": 1613371669.386315}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3491.40871918 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:50 INFO 140391001810112] # Starting training for epoch 93\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:51.542] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 284, \"duration\": 1143, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] # Finished training epoch 95 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] Loss (name: value) total: 6.52511227131\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] Loss (name: value) kld: 0.21723576097\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] Loss (name: value) recons: 6.30787646108\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] Loss (name: value) logppx: 6.52511227131\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] #quality_metric: host=algo-2, epoch=95, train total_loss <loss>=6.52511227131\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] patience losses:[6.527762677934435, 6.533456457985772, 6.528933359516992, 6.513440483146244, 6.524933106369442] min patience loss:6.51344048315 current loss:6.52511227131 absolute loss difference:0.0116717881627\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] #progress_metric: host=algo-2, completed 95 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3420, \"sum\": 3420.0, \"min\": 3420}, \"Total Records Seen\": {\"count\": 1, \"max\": 430065, \"sum\": 430065.0, \"min\": 430065}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 190, \"sum\": 190.0, \"min\": 190}}, \"EndTime\": 1613371671.544019, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 94}, \"StartTime\": 1613371670.398193}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3950.21689561 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:51 INFO 140255820981440] # Starting training for epoch 96\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:51.900] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 278, \"duration\": 1218, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] # Finished training epoch 93 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] Loss (name: value) total: 6.50340753794\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] Loss (name: value) kld: 0.213545857618\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] Loss (name: value) recons: 6.28986166583\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] Loss (name: value) logppx: 6.50340753794\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] #quality_metric: host=algo-1, epoch=93, train total_loss <loss>=6.50340753794\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] patience losses:[6.499719811810388, 6.512730413013035, 6.508121556705898, 6.501379324330224, 6.500239650408427] min patience loss:6.49971981181 current loss:6.50340753794 absolute loss difference:0.00368772612678\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3348, \"sum\": 3348.0, \"min\": 3348}, \"Total Records Seen\": {\"count\": 1, \"max\": 420732, \"sum\": 420732.0, \"min\": 420732}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 186, \"sum\": 186.0, \"min\": 186}}, \"EndTime\": 1613371671.902163, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 92}, \"StartTime\": 1613371670.682148}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3707.74376807 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:51 INFO 140391001810112] # Starting training for epoch 94\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:52.748] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 287, \"duration\": 1203, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] # Finished training epoch 96 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] Loss (name: value) total: 6.51250437233\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] Loss (name: value) kld: 0.218712295509\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] Loss (name: value) recons: 6.29379204909\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] Loss (name: value) logppx: 6.51250437233\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] #quality_metric: host=algo-2, epoch=96, train total_loss <loss>=6.51250437233\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] patience losses:[6.533456457985772, 6.528933359516992, 6.513440483146244, 6.524933106369442, 6.525112271308899] min patience loss:6.51344048315 current loss:6.51250437233 absolute loss difference:0.000936110814413\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] #progress_metric: host=algo-2, completed 96 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3456, \"sum\": 3456.0, \"min\": 3456}, \"Total Records Seen\": {\"count\": 1, \"max\": 434592, \"sum\": 434592.0, \"min\": 434592}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 192, \"sum\": 192.0, \"min\": 192}}, \"EndTime\": 1613371672.754405, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 95}, \"StartTime\": 1613371671.544402}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3739.67824642 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:52 INFO 140255820981440] # Starting training for epoch 97\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:53.116] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 281, \"duration\": 1213, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] # Finished training epoch 94 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] Loss (name: value) total: 6.49400985241\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] Loss (name: value) kld: 0.213220978363\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] Loss (name: value) recons: 6.28078888522\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] Loss (name: value) logppx: 6.49400985241\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] #quality_metric: host=algo-1, epoch=94, train total_loss <loss>=6.49400985241\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] patience losses:[6.512730413013035, 6.508121556705898, 6.501379324330224, 6.500239650408427, 6.503407537937164] min patience loss:6.50023965041 current loss:6.49400985241 absolute loss difference:0.00622979799906\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3384, \"sum\": 3384.0, \"min\": 3384}, \"Total Records Seen\": {\"count\": 1, \"max\": 425256, \"sum\": 425256.0, \"min\": 425256}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 188, \"sum\": 188.0, \"min\": 188}}, \"EndTime\": 1613371673.1217, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 93}, \"StartTime\": 1613371671.902399}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3709.7901535 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:53 INFO 140391001810112] # Starting training for epoch 95\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:53.907] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 290, \"duration\": 1149, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] # Finished training epoch 97 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] Loss (name: value) total: 6.51296020216\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] Loss (name: value) kld: 0.218826205573\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] Loss (name: value) recons: 6.29413394796\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] Loss (name: value) logppx: 6.51296020216\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] #quality_metric: host=algo-2, epoch=97, train total_loss <loss>=6.51296020216\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] patience losses:[6.528933359516992, 6.513440483146244, 6.524933106369442, 6.525112271308899, 6.512504372331831] min patience loss:6.51250437233 current loss:6.51296020216 absolute loss difference:0.000455829832289\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] #progress_metric: host=algo-2, completed 97 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3492, \"sum\": 3492.0, \"min\": 3492}, \"Total Records Seen\": {\"count\": 1, \"max\": 439119, \"sum\": 439119.0, \"min\": 439119}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 194, \"sum\": 194.0, \"min\": 194}}, \"EndTime\": 1613371673.90903, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 96}, \"StartTime\": 1613371672.755126}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3922.64177731 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:53 INFO 140255820981440] # Starting training for epoch 98\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:54.321] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 284, \"duration\": 1196, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] # Finished training epoch 95 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] Loss (name: value) total: 6.49583793349\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] Loss (name: value) kld: 0.213384378494\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] Loss (name: value) recons: 6.28245353036\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] Loss (name: value) logppx: 6.49583793349\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] #quality_metric: host=algo-1, epoch=95, train total_loss <loss>=6.49583793349\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] patience losses:[6.508121556705898, 6.501379324330224, 6.500239650408427, 6.503407537937164, 6.494009852409363] min patience loss:6.49400985241 current loss:6.49583793349 absolute loss difference:0.001828081078\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3420, \"sum\": 3420.0, \"min\": 3420}, \"Total Records Seen\": {\"count\": 1, \"max\": 429780, \"sum\": 429780.0, \"min\": 429780}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 190, \"sum\": 190.0, \"min\": 190}}, \"EndTime\": 1613371674.323651, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 94}, \"StartTime\": 1613371673.122183}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3764.51295139 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:54 INFO 140391001810112] # Starting training for epoch 96\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:55.151] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 293, \"duration\": 1242, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] # Finished training epoch 98 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] Loss (name: value) total: 6.5117243131\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] Loss (name: value) kld: 0.222597666085\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] Loss (name: value) recons: 6.28912665447\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] Loss (name: value) logppx: 6.5117243131\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] #quality_metric: host=algo-2, epoch=98, train total_loss <loss>=6.5117243131\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] patience losses:[6.513440483146244, 6.524933106369442, 6.525112271308899, 6.512504372331831, 6.51296020216412] min patience loss:6.51250437233 current loss:6.5117243131 absolute loss difference:0.000780059231652\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] #progress_metric: host=algo-2, completed 98 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3528, \"sum\": 3528.0, \"min\": 3528}, \"Total Records Seen\": {\"count\": 1, \"max\": 443646, \"sum\": 443646.0, \"min\": 443646}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 196, \"sum\": 196.0, \"min\": 196}}, \"EndTime\": 1613371675.156577, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 97}, \"StartTime\": 1613371673.909335}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3629.08745709 records/second\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] \u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:55 INFO 140255820981440] # Starting training for epoch 99\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:55.582] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 287, \"duration\": 1258, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] # Finished training epoch 96 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] Loss (name: value) total: 6.50394159555\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] Loss (name: value) kld: 0.217810831964\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] Loss (name: value) recons: 6.28613072634\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] Loss (name: value) logppx: 6.50394159555\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] #quality_metric: host=algo-1, epoch=96, train total_loss <loss>=6.50394159555\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] patience losses:[6.501379324330224, 6.500239650408427, 6.503407537937164, 6.494009852409363, 6.495837933487362] min patience loss:6.49400985241 current loss:6.50394159555 absolute loss difference:0.00993174314499\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3456, \"sum\": 3456.0, \"min\": 3456}, \"Total Records Seen\": {\"count\": 1, \"max\": 434304, \"sum\": 434304.0, \"min\": 434304}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 192, \"sum\": 192.0, \"min\": 192}}, \"EndTime\": 1613371675.583556, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 95}, \"StartTime\": 1613371674.323958}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=3591.20875364 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:55 INFO 140391001810112] # Starting training for epoch 97\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:56.318] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 296, \"duration\": 1161, \"num_examples\": 36, \"num_bytes\": 1049864}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] # Finished training epoch 99 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] Loss (name: value) total: 6.51620770163\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] Loss (name: value) kld: 0.222234734438\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] Loss (name: value) recons: 6.2939729823\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] Loss (name: value) logppx: 6.51620770163\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] #quality_metric: host=algo-2, epoch=99, train total_loss <loss>=6.51620770163\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] patience losses:[6.524933106369442, 6.525112271308899, 6.512504372331831, 6.51296020216412, 6.511724313100179] min patience loss:6.5117243131 current loss:6.51620770163 absolute loss difference:0.00448338852988\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] Timing: train: 1.16s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] #progress_metric: host=algo-2, completed 100 % epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3564, \"sum\": 3564.0, \"min\": 3564}, \"Total Records Seen\": {\"count\": 1, \"max\": 448173, \"sum\": 448173.0, \"min\": 448173}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 198, \"sum\": 198.0, \"min\": 198}}, \"EndTime\": 1613371676.320289, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 98}, \"StartTime\": 1613371675.156897}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:56 INFO 140255820981440] #throughput_metric: host=algo-2, train throughput=3890.62949607 records/second\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:56.697] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 290, \"duration\": 1113, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] # Finished training epoch 97 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] Loss (name: value) total: 6.48544151915\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] Loss (name: value) kld: 0.214946283028\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] Loss (name: value) recons: 6.27049528228\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] Loss (name: value) logppx: 6.48544151915\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] #quality_metric: host=algo-1, epoch=97, train total_loss <loss>=6.48544151915\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] patience losses:[6.500239650408427, 6.503407537937164, 6.494009852409363, 6.495837933487362, 6.503941595554352] min patience loss:6.49400985241 current loss:6.48544151915 absolute loss difference:0.00856833325492\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] Timing: train: 1.12s, val: 0.00s, epoch: 1.12s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3492, \"sum\": 3492.0, \"min\": 3492}, \"Total Records Seen\": {\"count\": 1, \"max\": 438828, \"sum\": 438828.0, \"min\": 438828}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 194, \"sum\": 194.0, \"min\": 194}}, \"EndTime\": 1613371676.702509, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 96}, \"StartTime\": 1613371675.5838}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=4043.38664898 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:56 INFO 140391001810112] # Starting training for epoch 98\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:57.646] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 293, \"duration\": 941, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] # Finished training epoch 98 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] Loss (name: value) total: 6.4734544754\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] Loss (name: value) kld: 0.216632629434\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] Loss (name: value) recons: 6.25682183769\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] Loss (name: value) logppx: 6.4734544754\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] #quality_metric: host=algo-1, epoch=98, train total_loss <loss>=6.4734544754\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] patience losses:[6.503407537937164, 6.494009852409363, 6.495837933487362, 6.503941595554352, 6.485441519154443] min patience loss:6.48544151915 current loss:6.4734544754 absolute loss difference:0.0119870437516\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] Timing: train: 0.94s, val: 0.00s, epoch: 0.95s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3528, \"sum\": 3528.0, \"min\": 3528}, \"Total Records Seen\": {\"count\": 1, \"max\": 443352, \"sum\": 443352.0, \"min\": 443352}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 196, \"sum\": 196.0, \"min\": 196}}, \"EndTime\": 1613371677.651248, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 97}, \"StartTime\": 1613371676.702824}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=4768.04265726 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:57 INFO 140391001810112] # Starting training for epoch 99\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:58.576] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 296, \"duration\": 920, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] # Finished training epoch 99 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] Loss (name: value) total: 6.46279770136\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] Loss (name: value) kld: 0.217367462607\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] Loss (name: value) recons: 6.24543023109\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] Loss (name: value) logppx: 6.46279770136\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] #quality_metric: host=algo-1, epoch=99, train total_loss <loss>=6.46279770136\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] patience losses:[6.494009852409363, 6.495837933487362, 6.503941595554352, 6.485441519154443, 6.473454475402832] min patience loss:6.4734544754 current loss:6.46279770136 absolute loss difference:0.010656774044\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] Timing: train: 0.93s, val: 0.00s, epoch: 0.93s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3564, \"sum\": 3564.0, \"min\": 3564}, \"Total Records Seen\": {\"count\": 1, \"max\": 447876, \"sum\": 447876.0, \"min\": 447876}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 198, \"sum\": 198.0, \"min\": 198}}, \"EndTime\": 1613371678.580589, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 98}, \"StartTime\": 1613371677.651879}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=4870.51167265 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] \u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:58 INFO 140391001810112] # Starting training for epoch 100\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:59.504] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 299, \"duration\": 923, \"num_examples\": 36, \"num_bytes\": 1044312}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] # Finished training epoch 100 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Loss (name: value) total: 6.45837059948\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Loss (name: value) kld: 0.220802074091\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Loss (name: value) recons: 6.23756855064\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Loss (name: value) logppx: 6.45837059948\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] #quality_metric: host=algo-1, epoch=100, train total_loss <loss>=6.45837059948\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] patience losses:[6.495837933487362, 6.503941595554352, 6.485441519154443, 6.473454475402832, 6.462797701358795] min patience loss:6.46279770136 current loss:6.45837059948 absolute loss difference:0.004427101877\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Timing: train: 0.92s, val: 0.00s, epoch: 0.93s\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3600, \"sum\": 3600.0, \"min\": 3600}, \"Total Records Seen\": {\"count\": 1, \"max\": 452400, \"sum\": 452400.0, \"min\": 452400}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 200, \"sum\": 200.0, \"min\": 200}}, \"EndTime\": 1613371679.509078, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 99}, \"StartTime\": 1613371678.580843}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] #throughput_metric: host=algo-1, train throughput=4872.71045157 records/second\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Best model based on early stopping at epoch 100. Best loss: 6.45837059948\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] Best model based on early stopping at epoch 98. Best loss: 6.5117243131\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] Topics from epoch:final (num_topics:30) [, tu 0.57]:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1031 174 1170 83 1969 1702 225 31 71 1679 1801 598 861 726 511 224 1210 1765 589 1606\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1031 1170 785 741 55 397 1325 1171 792 762 409 732 1765 1717 912 798 1028 1616 174 1523\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1030 1820 55 1139 861 1969 1523 68 1216 1170 433 100 979 704 1724 1812 359 769 680 698\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 681 1907 1366 1906 1327 891 832 177 1231 1989 1955 1992 1109 1075 819 1296 493 319 657 1577\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1020 1918 1120 1216 705 474 134 1367 296 1769 137 183 1707 662 385 1831 686 1224 1971 1893\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1193 1813 1582 201 1312 907 31 27 1292 1934 76 939 1293 71 1406 670 1680 1574 1676 283\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 0 136 730 1831 137 1651 134 513 1527 1893 1418 1298 1419 1060 73 1526 793 731 421 1859\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 68 1216 1820 1765 1031 1797 741 1129 1812 1656 1714 1128 1969 1303 308 869 756 1513 1139 1752\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 751 1765 174 1169 83 100 1800 784 1091 68 82 1680 1420 1216 1312 737 71 1934 1656 556\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 730 0 731 1418 1831 137 136 134 1893 1298 1651 1024 1060 1751 976 1419 513 130 421 135\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1765 197 1454 174 513 1969 707 741 892 1163 1679 49 1702 1526 414 1724 225 1293 507 1820\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 232 476 1574 1192 1312 1825 1727 472 1575 1406 1174 1292 283 1193 1826 713 1783 961 1310 808\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 91 97 133 1856 1145 721 906 96 88 1613 896 893 1232 1858 131 1244 895 892 1673 1857\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 48 295 1531 1547 1146 1031 170 1787 641 135 409 578 704 849 558 410 557 1283 1105 138\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 0 730 1060 1831 137 1418 136 1419 134 1893 731 133 1298 130 1702 793 421 976 1094 1232\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 174 1528 1526 513 532 714 1141 507 319 146 192 1776 1819 34 245 511 1216 1214 1809 1080\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 513 1651 955 137 134 1831 174 1526 1092 146 969 1418 1847 183 88 154 198 1893 1293 1993\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1578 1141 112 1122 584 1522 1196 380 629 601 172 1255 736 171 508 256 916 1904 163 726\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 730 136 0 1298 731 1893 1024 1418 1060 1022 976 1831 1964 421 1137 137 1023 134 1419 1147\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 88 49 513 906 1934 939 1242 788 1765 826 1582 21 414 185 145 741 926 1445 286 1993\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1170 1335 1031 1338 1283 1656 68 1887 861 698 704 82 1680 885 556 1739 1752 498 1165 1369\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1556 595 350 1205 104 1636 1602 200 1360 1243 1092 1241 1601 1691 1815 853 759 666 852 1491\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1875 1242 68 1375 1216 13 1461 1724 1379 1470 816 1931 1497 1012 1268 1788 624 1843 586 1210\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1283 704 1547 1680 1196 635 364 1031 1656 68 751 1170 1198 409 55 31 83 1618 1007 1338\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1170 414 27 83 635 751 1409 1801 1680 308 1216 861 82 1656 785 1440 704 989 1800 575\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 513 1751 137 134 1526 1141 1528 1831 1765 183 793 511 174 1495 652 1563 198 1527 0 530\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 927 174 1651 513 1445 1661 806 1765 507 1526 1242 939 1645 969 741 1979 1260 64 683 1142\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1765 145 741 814 767 1679 406 405 1269 55 561 1131 876 106 530 527 364 1204 1780 451\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1211 1526 1815 1420 1811 183 939 439 174 1527 1934 722 1563 1751 850 511 1907 385 1702 1243\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] 1969 1170 1031 861 27 31 1724 1312 100 83 174 783 989 82 912 1535 1702 1801 148 71\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] Serializing model to /opt/ml/model/model_algo-2\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] Saved checkpoint to \"/tmp/tmpfq3jTG/state-0001.params\"\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:59.642] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 125262, \"num_examples\": 1, \"num_bytes\": 25384}\u001b[0m\n",
      "\u001b[35m[2021-02-15 06:47:59.777] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 134, \"num_examples\": 18, \"num_bytes\": 528360}\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] Finished scoring on 2176 examples from 17 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] Loss (name: value) total: 6.71428329804\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] Loss (name: value) kld: 0.207899527515\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] Loss (name: value) recons: 6.50638375563\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] Loss (name: value) logppx: 6.71428329804\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Total Records Seen\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613371679.778673, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1613371679.642698}\n",
      "\u001b[0m\n",
      "\u001b[35m[02/15/2021 06:47:59 INFO 140255820981440] #test_score (algo-2) : ('log_perplexity', 6.714283298043644)\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 125506.29091262817, \"sum\": 125506.29091262817, \"min\": 125506.29091262817}, \"finalize.time\": {\"count\": 1, \"max\": 126.13797187805176, \"sum\": 126.13797187805176, \"min\": 126.13797187805176}, \"initialize.time\": {\"count\": 1, \"max\": 430.23204803466797, \"sum\": 430.23204803466797, \"min\": 430.23204803466797}, \"model.serialize.time\": {\"count\": 1, \"max\": 5.645036697387695, \"sum\": 5.645036697387695, \"min\": 5.645036697387695}, \"setuptime\": {\"count\": 1, \"max\": 47.46198654174805, \"sum\": 47.46198654174805, \"min\": 47.46198654174805}, \"early_stop.time\": {\"count\": 99, \"max\": 5.142927169799805, \"sum\": 249.71318244934082, \"min\": 0.16808509826660156}, \"update.time\": {\"count\": 99, \"max\": 1867.1619892120361, \"sum\": 121447.83473014832, \"min\": 1085.0698947906494}, \"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"model.score.time\": {\"count\": 1, \"max\": 135.8969211578369, \"sum\": 135.8969211578369, \"min\": 135.8969211578369}}, \"EndTime\": 1613371679.77977, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1613371554.363614}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Topics from epoch:final (num_topics:30) [, tu 0.57]:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1170 1031 762 861 1801 1969 71 174 1169 278 27 458 83 1210 511 1765 225 183 1788 1080\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1170 1171 55 1294 1765 458 861 1547 762 1169 409 1031 1789 1128 414 1969 741 1335 174 225\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 433 1489 698 1139 1030 100 611 1285 1216 68 1447 1490 1820 370 704 1656 1724 989 1083 359\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 681 1907 1366 1327 891 177 832 1906 1989 1231 1992 1955 819 1109 1075 1296 493 657 22 704\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1918 490 1216 296 1367 183 1707 342 1020 134 137 1647 1937 511 59 1000 68 1936 1234 1829\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 201 27 1193 556 1813 1934 1338 1676 1213 148 1294 76 1801 264 907 31 575 1680 1292 939\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 0 136 1419 730 1893 1831 1298 137 134 1418 1651 421 1527 969 513 731 682 130 1060 1022\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1348 1216 68 704 1656 1448 1169 1139 1129 1820 1812 414 89 1917 1714 18 1513 744 973 676\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 82 741 100 1765 864 83 145 1338 1169 68 44 635 798 565 183 71 751 527 792 174\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 0 730 1418 1831 731 1893 136 137 1419 134 1298 130 421 1060 1024 1651 513 1751 969 976\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1969 1765 1454 197 174 31 741 1702 1128 83 1582 145 88 1420 513 892 414 1293 1724 201\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 476 232 1192 1574 1312 472 1825 1575 1727 1193 1174 1292 1406 1826 283 1295 1783 1310 713 960\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 91 97 133 96 1145 1856 721 1613 88 906 896 892 1673 893 1858 1232 1244 1857 131 895\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 48 295 1547 1031 170 1531 641 1146 578 409 135 704 873 849 558 410 138 557 1283 885\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 0 1831 1418 137 1419 136 134 730 1893 1060 1298 731 421 133 1024 174 1526 1964 976 793\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1526 174 1729 192 1128 714 1528 513 908 511 1776 1216 1809 88 507 319 34 751 1141 1090\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 513 183 955 154 1117 969 1831 174 1526 134 137 146 1418 1420 1893 1527 939 279 1563 1847\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1141 1578 112 1122 1522 584 1196 601 380 629 508 172 1255 736 171 916 113 256 1904 163\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 730 1298 136 1893 731 0 1418 976 1964 1024 1831 1022 421 1419 1060 1137 137 134 1023 130\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 88 49 1249 1969 1765 1526 1582 741 145 414 928 1124 906 939 788 991 1210 431 806 1051\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1170 1887 1031 1338 1283 1169 1335 861 102 1680 1135 989 1447 68 1030 1196 1801 698 1656 676\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1556 595 350 1636 1205 104 1602 200 1360 1601 1243 1691 1241 1815 1092 759 853 852 1491 1414\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1875 68 1216 930 1019 1965 1789 13 1724 586 1347 872 1538 776 925 578 1879 863 1030 1043\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1283 1031 364 989 704 68 55 1169 1338 1103 1547 1680 635 1196 1801 308 332 31 1486 1173\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 635 308 1170 1165 414 1605 751 27 732 1656 1216 70 1440 785 83 1139 861 704 1523 333\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1528 513 1526 134 137 1831 528 174 1751 652 1722 1765 71 0 142 1495 793 1563 289 1893\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 513 174 1651 1445 507 1242 1645 142 1185 969 806 668 927 1582 682 1526 1420 1131 64 941\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1765 145 741 814 1679 406 767 405 55 1269 1204 1780 106 530 761 1131 876 561 451 1260\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1815 1527 174 1135 1420 183 1651 939 1211 1526 1702 1730 1075 1909 154 1907 850 511 1092 1212\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] 1969 31 83 1031 1170 751 82 1724 1030 861 1702 27 148 1335 1801 414 756 1312 174 1169\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Saved checkpoint to \"/tmp/tmpzhr2U9/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:59.641] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 128417, \"num_examples\": 1, \"num_bytes\": 25384}\u001b[0m\n",
      "\u001b[34m[2021-02-15 06:47:59.763] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 121, \"num_examples\": 18, \"num_bytes\": 528360}\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Finished scoring on 2176 examples from 17 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Loss (name: value) total: 6.70601597954\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Loss (name: value) kld: 0.21171937357\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Loss (name: value) recons: 6.49429666295\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] Loss (name: value) logppx: 6.70601597954\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Total Records Seen\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613371679.764337, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1613371679.641653}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/15/2021 06:47:59 INFO 140391001810112] #test_score (algo-1) : ('log_perplexity', 6.706015979542451)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 132765.25211334229, \"sum\": 132765.25211334229, \"min\": 132765.25211334229}, \"finalize.time\": {\"count\": 1, \"max\": 125.94079971313477, \"sum\": 125.94079971313477, \"min\": 125.94079971313477}, \"initialize.time\": {\"count\": 1, \"max\": 3564.851999282837, \"sum\": 3564.851999282837, \"min\": 3564.851999282837}, \"model.serialize.time\": {\"count\": 1, \"max\": 5.521059036254883, \"sum\": 5.521059036254883, \"min\": 5.521059036254883}, \"setuptime\": {\"count\": 1, \"max\": 4167.7680015563965, \"sum\": 4167.7680015563965, \"min\": 4167.7680015563965}, \"early_stop.time\": {\"count\": 100, \"max\": 5.269050598144531, \"sum\": 257.1694850921631, \"min\": 0.1621246337890625}, \"update.time\": {\"count\": 100, \"max\": 1901.1540412902832, \"sum\": 124627.5143623352, \"min\": 927.9100894927979}, \"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"model.score.time\": {\"count\": 1, \"max\": 122.61700630187988, \"sum\": 122.61700630187988, \"min\": 122.61700630187988}}, \"EndTime\": 1613371679.765389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1613371551.203899}\n",
      "\u001b[0m\n",
      "\n",
      "2021-02-15 06:48:32 Completed - Training job completed\n",
      "ProfilerReport-1613371344: NoIssuesFound\n",
      "Training seconds: 340\n",
      "Billable seconds: 340\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.session import s3_input\n",
    "\n",
    "s3_train = s3_input(s3_train_data, distribution='ShardedByS3Key')\n",
    "ntm.fit({'train': s3_train, 'test': s3_val_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "timely-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "## deploy the topic model\n",
    "## option A: deploy the model with SageMaker hosting services\n",
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "comparative-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "# ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "internal-infrared",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-27fe96c270b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mntm_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic_weights'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             http, parsed_response = self._make_request(\n\u001b[0;32m--> 663\u001b[0;31m                 operation_model, request_dict, request_context)\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    100\u001b[0m         logger.debug(\"Making request for %s with params: %s\",\n\u001b[1;32m    101\u001b[0m                      operation_model, request_dict)\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         success_response, exception = self._get_response(\n\u001b[0;32m--> 135\u001b[0;31m             request, operation_model, context)\n\u001b[0m\u001b[1;32m    136\u001b[0m         while self._needs_retry(attempts, operation_model, request_dict,\n\u001b[1;32m    137\u001b[0m                                 success_response, exception):\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# If no exception occurs then exception is None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         success_response, exception = self._do_get_response(\n\u001b[0;32m--> 167\u001b[0;31m             request, operation_model)\n\u001b[0m\u001b[1;32m    168\u001b[0m         kwargs_to_emit = {\n\u001b[1;32m    169\u001b[0m             \u001b[0;34m'response_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_do_get_response\u001b[0;34m(self, request, operation_model)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_non_none_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhttp_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mpreload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m             )\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             )\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# extract the topic vectors for the training data which will be used in KNN\n",
    "predictions = []\n",
    "for item in np.array(vectors.todense()):\n",
    "    np.shape(item)\n",
    "    results = ntm_predictor.predict(item)\n",
    "    predictions.append(np.array([prediction['topic_weights'] for prediction in results['predictions']]))\n",
    "    \n",
    "predictions = np.array([np.ndarray.flatten(x) for x in predictions])\n",
    "topicvec = train_labels[newidx]\n",
    "topicnames = [categories[x] for x in topicvec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "disciplinary-composition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-xl/20newsgroups/batch/train/trainvectors.csv\n",
      "..................................\u001b[32m2021-02-15T07:50:27.554:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loading entry points\u001b[0m\n",
      "\u001b[35mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[35mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loading entry points\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/serve.py:195: DeprecationWarning: entrypoint algorithm.request_iterators is deprecated in favor of algorithm.io.data_handlers.serve\n",
      "  \"in favor of algorithm.io.data_handlers.serve\", DeprecationWarning)\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator text/csv\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/json\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/jsonlines\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/json\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] Number of server workers: 1\u001b[0m\n",
      "\u001b[34m[2021-02-15 07:50:27 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2021-02-15 07:50:27 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2021-02-15 07:50:27 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2021-02-15 07:50:27 +0000] [31] [INFO] Booting worker with pid: 31\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loading model...\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] Deserialized model from /opt/ml/model\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 WARNING 140559675376832] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] nvidia-smi took: 0.0252740383148 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 WARNING 140559675376832] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] ...model loaded.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"execution_parameters.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375427.548628, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375427.501916}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 4.781007766723633, \"sum\": 4.781007766723633, \"min\": 4.781007766723633}, \"json.encoder.time\": {\"count\": 1, \"max\": 45.690059661865234, \"sum\": 45.690059661865234, \"min\": 45.690059661865234}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375428.347159, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375427.548727}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.7698535919189453, \"sum\": 0.7698535919189453, \"min\": 0.7698535919189453}, \"json.encoder.time\": {\"count\": 1, \"max\": 28.426170349121094, \"sum\": 28.426170349121094, \"min\": 28.426170349121094}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375429.093456, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375428.347299}\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/serve.py:195: DeprecationWarning: entrypoint algorithm.request_iterators is deprecated in favor of algorithm.io.data_handlers.serve\n",
      "  \"in favor of algorithm.io.data_handlers.serve\", DeprecationWarning)\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator text/csv\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/json\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/jsonlines\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/json\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] Number of server workers: 1\u001b[0m\n",
      "\u001b[35m[2021-02-15 07:50:27 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[35m[2021-02-15 07:50:27 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[35m[2021-02-15 07:50:27 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2021-02-15 07:50:27 +0000] [31] [INFO] Booting worker with pid: 31\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loading model...\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] Deserialized model from /opt/ml/model\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 WARNING 140559675376832] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] nvidia-smi took: 0.0252740383148 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 WARNING 140559675376832] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] ...model loaded.\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"execution_parameters.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375427.548628, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375427.501916}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 4.781007766723633, \"sum\": 4.781007766723633, \"min\": 4.781007766723633}, \"json.encoder.time\": {\"count\": 1, \"max\": 45.690059661865234, \"sum\": 45.690059661865234, \"min\": 45.690059661865234}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375428.347159, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375427.548727}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.7698535919189453, \"sum\": 0.7698535919189453, \"min\": 0.7698535919189453}, \"json.encoder.time\": {\"count\": 1, \"max\": 28.426170349121094, \"sum\": 28.426170349121094, \"min\": 28.426170349121094}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375429.093456, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375428.347299}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9100437164306641, \"sum\": 0.9100437164306641, \"min\": 0.9100437164306641}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.186994552612305, \"sum\": 22.186994552612305, \"min\": 22.186994552612305}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375429.731803, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375429.093595}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9140968322753906, \"sum\": 0.9140968322753906, \"min\": 0.9140968322753906}, \"json.encoder.time\": {\"count\": 1, \"max\": 25.7568359375, \"sum\": 25.7568359375, \"min\": 25.7568359375}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375430.42381, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375429.731947}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9100437164306641, \"sum\": 0.9100437164306641, \"min\": 0.9100437164306641}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.186994552612305, \"sum\": 22.186994552612305, \"min\": 22.186994552612305}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375429.731803, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375429.093595}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9140968322753906, \"sum\": 0.9140968322753906, \"min\": 0.9140968322753906}, \"json.encoder.time\": {\"count\": 1, \"max\": 25.7568359375, \"sum\": 25.7568359375, \"min\": 25.7568359375}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375430.42381, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375429.731947}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.8909702301025391, \"sum\": 0.8909702301025391, \"min\": 0.8909702301025391}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.349119186401367, \"sum\": 22.349119186401367, \"min\": 22.349119186401367}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375431.060571, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375430.423962}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.8909702301025391, \"sum\": 0.8909702301025391, \"min\": 0.8909702301025391}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.349119186401367, \"sum\": 22.349119186401367, \"min\": 22.349119186401367}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375431.060571, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375430.423962}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9140968322753906, \"sum\": 0.9140968322753906, \"min\": 0.9140968322753906}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.253990173339844, \"sum\": 22.253990173339844, \"min\": 22.253990173339844}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375431.704945, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375431.060715}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.8950233459472656, \"sum\": 0.8950233459472656, \"min\": 0.8950233459472656}, \"json.encoder.time\": {\"count\": 1, \"max\": 21.976947784423828, \"sum\": 21.976947784423828, \"min\": 21.976947784423828}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375432.339448, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375431.705106}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 2.2869110107421875, \"sum\": 2.2869110107421875, \"min\": 2.2869110107421875}, \"json.encoder.time\": {\"count\": 1, \"max\": 6.129026412963867, \"sum\": 6.129026412963867, \"min\": 6.129026412963867}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375432.462981, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375432.33961}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9140968322753906, \"sum\": 0.9140968322753906, \"min\": 0.9140968322753906}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.253990173339844, \"sum\": 22.253990173339844, \"min\": 22.253990173339844}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375431.704945, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375431.060715}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.8950233459472656, \"sum\": 0.8950233459472656, \"min\": 0.8950233459472656}, \"json.encoder.time\": {\"count\": 1, \"max\": 21.976947784423828, \"sum\": 21.976947784423828, \"min\": 21.976947784423828}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375432.339448, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375431.705106}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 2.2869110107421875, \"sum\": 2.2869110107421875, \"min\": 2.2869110107421875}, \"json.encoder.time\": {\"count\": 1, \"max\": 6.129026412963867, \"sum\": 6.129026412963867, \"min\": 6.129026412963867}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375432.462981, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375432.33961}\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[32m2021-02-15T07:50:27.554:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loading entry points\u001b[0m\n",
      "\u001b[35mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[35mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loading entry points\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/serve.py:195: DeprecationWarning: entrypoint algorithm.request_iterators is deprecated in favor of algorithm.io.data_handlers.serve\n",
      "  \"in favor of algorithm.io.data_handlers.serve\", DeprecationWarning)\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator text/csv\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/json\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/jsonlines\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/json\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] Number of server workers: 1\u001b[0m\n",
      "\u001b[34m[2021-02-15 07:50:27 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2021-02-15 07:50:27 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2021-02-15 07:50:27 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2021-02-15 07:50:27 +0000] [31] [INFO] Booting worker with pid: 31\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] loading model...\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] Deserialized model from /opt/ml/model\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 WARNING 140559675376832] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] nvidia-smi took: 0.0252740383148 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 WARNING 140559675376832] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/15/2021 07:50:27 INFO 140559675376832] ...model loaded.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"execution_parameters.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375427.548628, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375427.501916}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 4.781007766723633, \"sum\": 4.781007766723633, \"min\": 4.781007766723633}, \"json.encoder.time\": {\"count\": 1, \"max\": 45.690059661865234, \"sum\": 45.690059661865234, \"min\": 45.690059661865234}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375428.347159, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375427.548727}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.7698535919189453, \"sum\": 0.7698535919189453, \"min\": 0.7698535919189453}, \"json.encoder.time\": {\"count\": 1, \"max\": 28.426170349121094, \"sum\": 28.426170349121094, \"min\": 28.426170349121094}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375429.093456, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375428.347299}\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/serve.py:195: DeprecationWarning: entrypoint algorithm.request_iterators is deprecated in favor of algorithm.io.data_handlers.serve\n",
      "  \"in favor of algorithm.io.data_handlers.serve\", DeprecationWarning)\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator text/csv\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/json\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded request iterator application/jsonlines\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/json\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] Number of server workers: 1\u001b[0m\n",
      "\u001b[35m[2021-02-15 07:50:27 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[35m[2021-02-15 07:50:27 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[35m[2021-02-15 07:50:27 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2021-02-15 07:50:27 +0000] [31] [INFO] Booting worker with pid: 31\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] loading model...\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] Deserialized model from /opt/ml/model\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 WARNING 140559675376832] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] nvidia-smi took: 0.0252740383148 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 WARNING 140559675376832] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[02/15/2021 07:50:27 INFO 140559675376832] ...model loaded.\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"execution_parameters.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375427.548628, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375427.501916}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 4.781007766723633, \"sum\": 4.781007766723633, \"min\": 4.781007766723633}, \"json.encoder.time\": {\"count\": 1, \"max\": 45.690059661865234, \"sum\": 45.690059661865234, \"min\": 45.690059661865234}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375428.347159, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375427.548727}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.7698535919189453, \"sum\": 0.7698535919189453, \"min\": 0.7698535919189453}, \"json.encoder.time\": {\"count\": 1, \"max\": 28.426170349121094, \"sum\": 28.426170349121094, \"min\": 28.426170349121094}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375429.093456, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375428.347299}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9100437164306641, \"sum\": 0.9100437164306641, \"min\": 0.9100437164306641}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.186994552612305, \"sum\": 22.186994552612305, \"min\": 22.186994552612305}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375429.731803, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375429.093595}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9140968322753906, \"sum\": 0.9140968322753906, \"min\": 0.9140968322753906}, \"json.encoder.time\": {\"count\": 1, \"max\": 25.7568359375, \"sum\": 25.7568359375, \"min\": 25.7568359375}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375430.42381, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375429.731947}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9100437164306641, \"sum\": 0.9100437164306641, \"min\": 0.9100437164306641}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.186994552612305, \"sum\": 22.186994552612305, \"min\": 22.186994552612305}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375429.731803, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375429.093595}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9140968322753906, \"sum\": 0.9140968322753906, \"min\": 0.9140968322753906}, \"json.encoder.time\": {\"count\": 1, \"max\": 25.7568359375, \"sum\": 25.7568359375, \"min\": 25.7568359375}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375430.42381, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375429.731947}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.8909702301025391, \"sum\": 0.8909702301025391, \"min\": 0.8909702301025391}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.349119186401367, \"sum\": 22.349119186401367, \"min\": 22.349119186401367}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375431.060571, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375430.423962}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.8909702301025391, \"sum\": 0.8909702301025391, \"min\": 0.8909702301025391}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.349119186401367, \"sum\": 22.349119186401367, \"min\": 22.349119186401367}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375431.060571, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375430.423962}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9140968322753906, \"sum\": 0.9140968322753906, \"min\": 0.9140968322753906}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.253990173339844, \"sum\": 22.253990173339844, \"min\": 22.253990173339844}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375431.704945, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375431.060715}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.8950233459472656, \"sum\": 0.8950233459472656, \"min\": 0.8950233459472656}, \"json.encoder.time\": {\"count\": 1, \"max\": 21.976947784423828, \"sum\": 21.976947784423828, \"min\": 21.976947784423828}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375432.339448, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375431.705106}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 2.2869110107421875, \"sum\": 2.2869110107421875, \"min\": 2.2869110107421875}, \"json.encoder.time\": {\"count\": 1, \"max\": 6.129026412963867, \"sum\": 6.129026412963867, \"min\": 6.129026412963867}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375432.462981, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375432.33961}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.9140968322753906, \"sum\": 0.9140968322753906, \"min\": 0.9140968322753906}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.253990173339844, \"sum\": 22.253990173339844, \"min\": 22.253990173339844}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375431.704945, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375431.060715}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.8950233459472656, \"sum\": 0.8950233459472656, \"min\": 0.8950233459472656}, \"json.encoder.time\": {\"count\": 1, \"max\": 21.976947784423828, \"sum\": 21.976947784423828, \"min\": 21.976947784423828}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375432.339448, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375431.705106}\n",
      "\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 2.2869110107421875, \"sum\": 2.2869110107421875, \"min\": 2.2869110107421875}, \"json.encoder.time\": {\"count\": 1, \"max\": 6.129026412963867, \"sum\": 6.129026412963867, \"min\": 6.129026412963867}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1613375432.462981, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1613375432.33961}\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## option B: deploy with batch transform\n",
    "np.savetxt('trainvectors.csv',\n",
    "           vectors.todense(),\n",
    "           delimiter=',',\n",
    "           fmt='%i')\n",
    "batch_prefix = '20newsgroups/batch'\n",
    "\n",
    "train_s3 = sess.upload_data('trainvectors.csv', \n",
    "                            bucket=bucket, \n",
    "                            key_prefix='{}/train'.format(batch_prefix))\n",
    "\n",
    "print(train_s3)\n",
    "\n",
    "batch_output_path = 's3://{}/{}/test'.format(bucket, batch_prefix)\n",
    "\n",
    "ntm_transformer = ntm.transformer(instance_count=1,\n",
    "                                  instance_type ='ml.m4.xlarge',\n",
    "                                  output_path=batch_output_path\n",
    "                                 )\n",
    "ntm_transformer.transform(train_s3, content_type='text/csv', split_type='Line')\n",
    "ntm_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "prescribed-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-xl/20newsgroups/batch/test/trainvectors.csv.out to ./trainvectors.csv.out\n",
      "{\"topic_weights\":[0.0176811125,0.0177120008,0.0198236313,0.0155713204,0.0370621458,0.019373972,0.0329566523,0.0248791873,0.0209578648,0.0368159488,0.0210807733,0.0163670015,0.0150072491,0.305621922,0.0184352715,0.0254585743,0.0254659783,0.0621491335,0.0402800441,0.0228296835,0.0182546265,0.0164231956,0.0294983257,0.0167901646,0.0195879862,0.0253007896,0.0249296222,0.0119190179,0.022343887,0.0194228888]}\n",
      "{\"topic_weights\":[0.0217818506,0.0221054517,0.0209346376,0.0147795798,0.0288178343,0.0195078794,0.043546889,0.0219893213,0.0236818846,0.0440922901,0.0279528257,0.1586860567,0.0159528498,0.0142511055,0.0281842519,0.0314711705,0.0351098254,0.047913488,0.0610321723,0.0294166282,0.0194408093,0.0185179617,0.0251341276,0.0185227469,0.0201560762,0.0327243693,0.036396794,0.067391634,0.0287253447,0.0217822287]}\n",
      "{\"topic_weights\":[0.0296324547,0.0311008636,0.0316695571,0.0197660085,0.0358408168,0.0261726286,0.0321184136,0.0317075402,0.0319581963,0.0339237228,0.0306876972,0.0167588796,0.0249414518,0.0781305209,0.030086251,0.0325872116,0.0320243686,0.0205734652,0.0322706811,0.0299564935,0.0314931944,0.0219639614,0.0346305184,0.0300335493,0.0308332685,0.0325325653,0.0319346003,0.0939218551,0.0295673665,0.0311818626]}\n",
      "{\"topic_weights\":[0.0212667827,0.0208175611,0.0267179664,0.0117876818,0.0241995025,0.0313089862,0.0166034289,0.0266389046,0.0233524982,0.0180762336,0.0185065679,0.335681349,0.0492491648,0.0444337539,0.0208504386,0.0205798857,0.0196133256,0.0101170111,0.0152696073,0.0185337141,0.0244623963,0.025623871,0.0243779179,0.0292418133,0.0275250468,0.0178718418,0.016540708,0.0173860528,0.0201258957,0.0232400671]}\n",
      "{\"topic_weights\":[0.0181363299,0.0190086514,0.0187498685,0.0116239805,0.0178308841,0.0219462011,0.0138633335,0.0183845181,0.0193872228,0.0143665913,0.018374579,0.0727560818,0.0520496219,0.0133640971,0.0166306794,0.0181064587,0.0159898698,0.350543499,0.01133864,0.0181423537,0.0192656014,0.015329211,0.0187698435,0.02040256,0.0179286227,0.0168839488,0.0176377743,0.0784374848,0.0163646489,0.0183868594]}\n",
      "{\"topic_weights\":[0.0271383151,0.0253893938,0.030321138,0.0123540573,0.025053056,0.0343785994,0.0198564194,0.0283103958,0.026899159,0.0210933164,0.0203771349,0.020124672,0.1299384832,0.1653563529,0.0321012363,0.0217340495,0.0227277111,0.0071057151,0.0184114687,0.020432204,0.0341195017,0.0119310943,0.0253611468,0.0372859165,0.0340982415,0.0225701071,0.0177776106,0.0547747388,0.0228583422,0.0301203392]}\n",
      "{\"topic_weights\":[0.0334933661,0.0325042121,0.0323135443,0.0318436697,0.0351944603,0.0345902368,0.0320836678,0.0347573645,0.0354683548,0.032079611,0.0326013602,0.0212194882,0.0214854535,0.1003830656,0.0314928815,0.0325561389,0.0336843804,0.0280316453,0.0318676122,0.0306001548,0.0350289568,0.0163078029,0.0333829597,0.0347932391,0.0350232348,0.0333561525,0.0296690762,0.0169265922,0.0311388057,0.0361225531]}\n",
      "{\"topic_weights\":[0.0210089777,0.0205620993,0.0207747091,0.014732765,0.0256285369,0.0212219339,0.0271728411,0.0215647612,0.0208901186,0.0282553546,0.0236750133,0.0184803996,0.1513209343,0.0132313026,0.0211239103,0.0268350895,0.0259351563,0.0709224194,0.0304481853,0.0264243651,0.0201105382,0.1701096594,0.024237046,0.0178971384,0.0195448846,0.0248354338,0.0274826381,0.017234128,0.0266472548,0.0216924176]}\n",
      "{\"topic_weights\":[0.0152849555,0.0155894523,0.0173577797,0.0110782515,0.0188833177,0.018171953,0.0183438621,0.0169231594,0.0167459846,0.018113751,0.0181552544,0.2730645537,0.0241896063,0.0111042354,0.0164793245,0.0185404159,0.0178003572,0.0134882145,0.0188542642,0.0203168616,0.0149502642,0.013421433,0.0186187197,0.0158470906,0.0163396448,0.0179832689,0.0224470291,0.2496539354,0.0166886505,0.0155643737]}\n",
      "{\"topic_weights\":[0.030412158,0.0315877125,0.0295035727,0.1281812191,0.0274297632,0.0306919981,0.0257371888,0.0286242701,0.0295152217,0.0261618942,0.028659299,0.0274274033,0.0179579034,0.0463497229,0.0281315707,0.0270261895,0.029015068,0.0263465289,0.0258654784,0.0283771101,0.031158993,0.0608821586,0.0281357467,0.0324861258,0.029695699,0.0280768089,0.0258743297,0.0315962583,0.028599197,0.0304934625]}\n",
      "{\"topic_weights\":[0.0252784248,0.0229163598,0.0229555778,0.0176102743,0.0331694111,0.0219411757,0.0422400199,0.0240802635,0.0251610465,0.0454618298,0.0277302358,0.021496173,0.0979686901,0.0143460352,0.0327300616,0.0323108509,0.0357485674,0.0238084737,0.0582776889,0.0311255045,0.0220027715,0.0260308068,0.0286109615,0.0191201027,0.022279622,0.0356484838,0.0366867036,0.0951574668,0.032162413,0.0259440895]}\n",
      "{\"topic_weights\":[0.0205240343,0.0185324177,0.0213532355,0.1543470025,0.0229779594,0.0274760574,0.0226386674,0.0193943828,0.0209112186,0.0236098375,0.0229097027,0.0205265805,0.1828179508,0.0148698734,0.0170982778,0.0245680362,0.025397962,0.0105601763,0.0271938164,0.0298646763,0.0186265185,0.013201206,0.0218584016,0.0186781622,0.0197049007,0.0261341874,0.0266796183,0.0837688074,0.0222631507,0.021513287]}\n",
      "{\"topic_weights\":[0.0282732397,0.0274046399,0.02488661,0.0967398509,0.0280187875,0.0264828056,0.0329722762,0.0254592057,0."
     ]
    }
   ],
   "source": [
    "# once the transform is done, download the outputs back to local notebook instance for inspection\n",
    "!aws s3 cp --recursive $ntm_transformer.output_path ./\n",
    "!head -c 5000 trainvectors.csv.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "desperate-ocean",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-32cf53fbcb88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtsne_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't-SNE done! Time elapsed: {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=5000)\n",
    "tsne_results = tsne.fit_transform(predictions)\n",
    "\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "tsnedf = pd.DataFrame()\n",
    "tsnedf['tsne-2d-one'] = tsne_results[:,0]\n",
    "tsnedf['tsne-2d-two'] = tsne_results[:,1]\n",
    "tsnedf['Topic'] = topicnames\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "\n",
    "sns.lmplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue='Topic',\n",
    "    palette=sns.color_palette(\"hls\", NUM_TOPICS),\n",
    "    data=tsnedf,\n",
    "    legend=\"full\",\n",
    "    fit_reg=False\n",
    ")\n",
    "\n",
    "plt.axis('Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "seventh-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = newidx \n",
    "labeldict = dict(zip(newidx,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "special-depth",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-9e52dd949332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_features shape = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_labels shape = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "\n",
    "print('train_features shape = ', predictions.shape)\n",
    "print('train_labels shape = ', labels.shape)\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, predictions, labels)\n",
    "buf.seek(0)\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "key = 'knn/train'\n",
    "fname = os.path.join(prefix, key)\n",
    "print(fname)\n",
    "boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "duplicate-timber",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b28437bcefae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m hyperparams = {\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;34m'feature_dim'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNUM_NEIGHBORS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m'sample_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.c4.xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    knn.fit(fit_input)\n",
    "    return knn\n",
    "\n",
    "hyperparams = {\n",
    "    'feature_dim': predictions.shape[1],\n",
    "    'k': NUM_NEIGHBORS,\n",
    "    'sample_size': predictions.shape[0],\n",
    "    'predictor_type': 'classifier' ,\n",
    "    'index_metric':'COSINE'\n",
    "}\n",
    "output_path = 's3://' + bucket + '/' + prefix + '/knn/output'\n",
    "knn_estimator = trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "flying-might",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up the endpoint..\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'knn_estimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-aff81e2165ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'setting up the endpoint..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mknn_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor_from_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'knn_estimator' is not defined"
     ]
    }
   ],
   "source": [
    "def predictor_from_estimator(knn_estimator, estimator_name, instance_type, endpoint_name=None): \n",
    "    knn_predictor = knn_estimator.deploy(initial_instance_count=1, instance_type=instance_type,\n",
    "                                        endpoint_name=endpoint_name,\n",
    "                                        accept=\"application/jsonlines; verbose=true\")\n",
    "    knn_predictor.content_type = 'text/csv'\n",
    "    knn_predictor.serializer = csv_serializer\n",
    "    knn_predictor.deserializer = json_deserializer\n",
    "    \n",
    "    return knn_predictor\n",
    "\n",
    "import time\n",
    "\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "model_name = 'knn_%s'% instance_type\n",
    "endpoint_name = 'knn-ml-m4-xlarge-%s'% (str(time.time()).replace('.','-'))\n",
    "\n",
    "print('setting up the endpoint..')\n",
    "\n",
    "knn_predictor = predictor_from_estimator(knn_estimator, model_name, instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "metropolitan-despite",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint ntm-2021-02-15-07-55-45-667 of account 900019131056 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a8f95daa08e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtest_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_vectors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mntm_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtest_topics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint ntm-2021-02-15-07-55-45-667 of account 900019131056 not found."
     ]
    }
   ],
   "source": [
    "def preprocess_input(text):\n",
    "    text = strip_newsgroup_header(text)\n",
    "    text = strip_newsgroup_quoting(text)\n",
    "    text = strip_newsgroup_footer(text)\n",
    "    return text    \n",
    "    \n",
    "test_data_prep = []\n",
    "for i in range(len(newsgroups_test)):\n",
    "    test_data_prep.append(preprocess_input(newsgroups_test[i]))\n",
    "test_vectors = vectorizer.fit_transform(test_data_prep)\n",
    "\n",
    "test_vectors = np.array(test_vectors.todense())\n",
    "test_topics = []\n",
    "for vec in test_vectors:\n",
    "    test_result = ntm_predictor.predict(vec)\n",
    "    test_topics.append(test_result['predictions'][0]['topic_weights'])\n",
    "\n",
    "topic_predictions = []\n",
    "for topic in test_topics:\n",
    "    result = knn_predictor.predict(topic)\n",
    "    cur_predictions = np.array([int(result['labels'][i]) for i in range(len(result['labels']))])\n",
    "    topic_predictions.append(cur_predictions[::-1][:10])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "seasonal-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own k.\n",
    "def plot_topic_distribution(topic_num, k = 5):\n",
    "    \n",
    "    closest_topics = [predictions[labeldict[x]] for x in topic_predictions[topic_num][:k]]\n",
    "    closest_topics.append(np.array(test_topics[topic_num]))\n",
    "    closest_topics = np.array(closest_topics)\n",
    "    df = pd.DataFrame(closest_topics.T)\n",
    "    df.rename(columns ={k:\"Test Document Distribution\"}, inplace=True)\n",
    "    fs = 12\n",
    "    df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "    plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "    plt.xlabel('Topic ID', fontsize=fs+2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "gross-interest",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-4d9efe35dabd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_topic_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-a0c3e18127b8>\u001b[0m in \u001b[0;36mplot_topic_distribution\u001b[0;34m(topic_num, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_topic_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclosest_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabeldict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mclosest_topics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mclosest_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosest_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "plot_topic_distribution(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "undefined-mailman",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-762dfb2643c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_topic_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# plot_topic_distribution(5000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-a0c3e18127b8>\u001b[0m in \u001b[0;36mplot_topic_distribution\u001b[0;34m(topic_num, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_topic_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclosest_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabeldict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mclosest_topics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mclosest_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosest_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "plot_topic_distribution(25)\n",
    "# plot_topic_distribution(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-header",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
